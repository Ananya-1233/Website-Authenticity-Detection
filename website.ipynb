{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3UmxRth2kBN"
      },
      "source": [
        "**The goal of this notebook to develop a machine learning model to identify whether an url is authentic or not**\n",
        "\n",
        "Authentic and fake websites, though share innumerable similarities when seen through the naked eye, have several conflicting differences. These differences can only be identified through programming. Python libraries provide snippets for detection of these details without hassle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fjYeKW864Z2S"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "#Import all the necessary libraries in one cell\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from selenium import webdriver\n",
        "import requests\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from urllib.parse import urlparse\n",
        "import ipaddress\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import whois\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers\n",
        "import urllib\n",
        "import requests\n",
        "import urllib.request\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfGieCJhSKxN"
      },
      "outputs": [],
      "source": [
        "#Extracts the domain name of the url\n",
        "def getDomain(url):\n",
        "    domain = urlparse(url).netloc\n",
        "    if re.match(r\"^www.\",domain):\n",
        "        domain = domain.replace(\"www.\",\"\")\n",
        "    return domain\n",
        "\n",
        "#Checks whether the url has an IP address\n",
        "def havingIP(url):\n",
        "    try:\n",
        "        ipaddress.ip_address(url)\n",
        "        ip = 1\n",
        "    except:\n",
        "        ip = 0\n",
        "    return ip\n",
        "\n",
        "#Checks whether the url has '@' sign in it\n",
        "def haveAtSign(url):\n",
        "    if \"@\" in url:\n",
        "        at = 1\n",
        "    else:\n",
        "        at = 0\n",
        "    return at\n",
        "\n",
        "#Retrieves the url length\n",
        "def getLength(url):\n",
        "    if len(url) < 54:\n",
        "        length = 0\n",
        "    else:\n",
        "        length = 1\n",
        "    return length\n",
        "\n",
        "#Gets the url depth, i.e., how many html pages into the url is the link\n",
        "def getDepth(url):\n",
        "    s = urlparse(url).path.split('/')\n",
        "    depth = 0\n",
        "    for j in range(len(s)):\n",
        "        if len(s[j]) != 0:\n",
        "            depth = depth+1\n",
        "    return depth\n",
        "\n",
        "#Checks whether the url leads to other websites\n",
        "def redirection(url):\n",
        "    pos = url.rfind('//')\n",
        "    if pos > 6:\n",
        "        if pos > 7:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "#Whether the domain is tricked to display http/https\n",
        "def httpDomain(url):\n",
        "    domain = urlparse(url).netloc\n",
        "    if 'https' in domain:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
        "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
        "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
        "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
        "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
        "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
        "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
        "                      r\"tr\\.im|link\\.zip\\.net\"\n",
        "\n",
        "\n",
        "#Checking for Shortening Services in URL (Tiny_URL)\n",
        "def tinyURL(url):\n",
        "    match=re.search(shortening_services,url)\n",
        "    if match:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "#Check for '-' in the url\n",
        "def prefixSuffix(url):\n",
        "    if '-' in urlparse(url).netloc:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRBaNy2VSQuS"
      },
      "outputs": [],
      "source": [
        "# 12.Web traffic (Web_Traffic)\n",
        "def web_traffic(url):\n",
        "    try:\n",
        "        # Filling the whitespaces in the URL if any\n",
        "        url = urllib.parse.quote(url)\n",
        "        rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
        "        rank = int(rank)\n",
        "    except (TypeError, KeyError, ValueError, urllib.error.URLError):\n",
        "        return 1  # Returning 1 for any error\n",
        "    if rank < 100000:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "#The minimum age for a legitimate domain is set as 12 months\n",
        "def domainAge(domain_name):\n",
        "  creation_date = domain_name.creation_date\n",
        "  expiration_date = domain_name.expiration_date\n",
        "  if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
        "    try:\n",
        "      creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
        "      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
        "    except:\n",
        "      return 1\n",
        "  if ((expiration_date is None) or (creation_date is None)):\n",
        "      return 1\n",
        "  elif ((type(expiration_date) is list) or (type(creation_date) is list)):\n",
        "      return 1\n",
        "  else:\n",
        "    ageofdomain = abs((expiration_date - creation_date).days)\n",
        "    if ((ageofdomain/30) < 6):\n",
        "      age = 1\n",
        "    else:\n",
        "      age = 0\n",
        "  return age\n",
        "\n",
        "def domainEnd(domain_name):\n",
        "  expiration_date = domain_name.expiration_date\n",
        "  if isinstance(expiration_date,str):\n",
        "    try:\n",
        "      expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
        "    except:\n",
        "      return 1\n",
        "  if (expiration_date is None):\n",
        "      return 1\n",
        "  elif (type(expiration_date) is list):\n",
        "      return 1\n",
        "  else:\n",
        "    today = datetime.now()\n",
        "    end = abs((expiration_date - today).days)\n",
        "    if ((end/30) < 6):\n",
        "      end = 0\n",
        "    else:\n",
        "      end = 1\n",
        "  return end\n",
        "\n",
        "#Checks for the presence of inline documents\n",
        "def iframe(response):\n",
        "  if response == \"\":\n",
        "      return 1\n",
        "  else:\n",
        "      if re.findall(r\"[|]\", response.text):\n",
        "          return 0\n",
        "      else:\n",
        "          return 1\n",
        "\n",
        "#On mouseover, change should be observed in authentic websites\n",
        "def mouseOver(response):\n",
        "  if response == \"\" :\n",
        "    return 1\n",
        "  else:\n",
        "    if re.findall(\"\", response.text):\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "#right click is disabled in fake websites to avoid inspection\n",
        "def rightClick(response):\n",
        "  if response == \"\":\n",
        "    return 1\n",
        "  else:\n",
        "    if re.findall(r\"event.button ?== ?2\", response.text):\n",
        "      return 0\n",
        "    else:\n",
        "      return 1\n",
        "\n",
        "#Count how many times the url has been forwarded. If none, then fake\n",
        "def forwarding(response):\n",
        "  if response == \"\":\n",
        "    return 1\n",
        "  else:\n",
        "    if len(response.history) <= 2:\n",
        "      return 0\n",
        "    else:\n",
        "      return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLA1CfyuSySx"
      },
      "outputs": [],
      "source": [
        "def featureExtraction(url,label):\n",
        "\n",
        "  features = []\n",
        "  #Address bar based features (10)\n",
        "  features.append(getDomain(url))\n",
        "  features.append(havingIP(url))\n",
        "  features.append(haveAtSign(url))\n",
        "  features.append(getLength(url))\n",
        "  features.append(getDepth(url))\n",
        "  features.append(redirection(url))\n",
        "  features.append(httpDomain(url))\n",
        "  features.append(tinyURL(url))\n",
        "  features.append(prefixSuffix(url))\n",
        "\n",
        "  #Domain based features (4)\n",
        "  dns = 0\n",
        "  try:\n",
        "    domain_name = whois.whois(urlparse(url).netloc)\n",
        "  except:\n",
        "    dns = 1\n",
        "\n",
        "  features.append(dns)\n",
        "  features.append(web_traffic(url))\n",
        "  features.append(1 if dns == 1 else domainAge(domain_name))\n",
        "  features.append(1 if dns == 1 else domainEnd(domain_name))\n",
        "\n",
        "  # HTML & Javascript based features (4)\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "  except:\n",
        "    response = \"\"\n",
        "  features.append(iframe(response))\n",
        "  features.append(mouseOver(response))\n",
        "  features.append(rightClick(response))\n",
        "  features.append(forwarding(response))\n",
        "  features.append(label)\n",
        "\n",
        "  return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMgubbRLS0Od"
      },
      "outputs": [],
      "source": [
        "phish_features = []\n",
        "label = 1\n",
        "for i in range(0, 250):\n",
        "  url = phishurl['url'][i]\n",
        "  phish_features.append(featureExtraction(url,label))\n",
        "\n",
        "#Add all the features into the phishing dataset\n",
        "#converting the list to dataframe\n",
        "feature_names = ['Domain', 'Have_IP', 'Have_At', 'URL_Length', 'URL_Depth','Redirection',\n",
        "                      'https_Domain', 'TinyURL', 'Prefix/Suffix', 'DNS_Record', 'Web_Traffic',\n",
        "                      'Domain_Age', 'Domain_End', 'iFrame', 'Mouse_Over','Right_Click', 'Web_Forwards', 'Label']\n",
        "\n",
        "phishing = pd.DataFrame(phish_features, columns= feature_names)\n",
        "phishing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "NcwnqJUxn1jx",
        "outputId": "ffc92dd4-bbd5-41a9-e4b7-438930c20754"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "can't subtract offset-naive and offset-aware datetimes",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-dc5778c4352f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphishurl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mphish_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Add all the features into the phishing dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-80849d8cbd65>\u001b[0m in \u001b[0;36mfeatureExtraction\u001b[0;34m(url, label)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweb_traffic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdomainAge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdomainEnd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# HTML & Javascript based features (4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4c2700b7b2bc>\u001b[0m in \u001b[0;36mdomainEnd\u001b[0;34m(domain_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtoday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpiration_date\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtoday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't subtract offset-naive and offset-aware datetimes"
          ]
        }
      ],
      "source": [
        "phish_features = []\n",
        "label = 1\n",
        "for i in range(250, 500):\n",
        "  url = phishurl['url'][i]\n",
        "  phish_features.append(featureExtraction(url,label))\n",
        "\n",
        "#Add all the features into the phishing dataset\n",
        "#converting the list to dataframe\n",
        "feature_names = ['Domain', 'Have_IP', 'Have_At', 'URL_Length', 'URL_Depth','Redirection',\n",
        "                      'https_Domain', 'TinyURL', 'Prefix/Suffix', 'DNS_Record', 'Web_Traffic',\n",
        "                      'Domain_Age', 'Domain_End', 'iFrame', 'Mouse_Over','Right_Click', 'Web_Forwards', 'Label']\n",
        "\n",
        "\n",
        "phishing2 = pd.DataFrame(phish_features, columns= feature_names)\n",
        "phishing = pd.concatenate([phishing, phishing2], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0s_N2l5S-2i"
      },
      "outputs": [],
      "source": [
        "phishing['url'] = ''\n",
        "phishing['url'] = phishurl['url']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "94pp83i2oKqt",
        "outputId": "9293c4bc-23c3-4aec-94b2-ece0f17bcf21"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"phishing\",\n  \"rows\": 250,\n  \"fields\": [\n    {\n      \"column\": \"Domain\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 187,\n        \"samples\": [\n          \"att-108711.weeblysite.com\",\n          \"login-auth.update-ourtime.workers.dev\",\n          \"apnjuneargo.web.app\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Have_IP\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Have_At\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL_Length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL_Depth\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 12,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Redirection\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"https_Domain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TinyURL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Prefix/Suffix\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DNS_Record\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Web_Traffic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Domain_Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Domain_End\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"iFrame\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mouse_Over\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Right_Click\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Web_Forwards\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 250,\n        \"samples\": [\n          \"https://is.gd/KLZdiE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "phishing"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b559539a-970c-4d70-9b49-baa7c1ac466b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Domain</th>\n",
              "      <th>Have_IP</th>\n",
              "      <th>Have_At</th>\n",
              "      <th>URL_Length</th>\n",
              "      <th>URL_Depth</th>\n",
              "      <th>Redirection</th>\n",
              "      <th>https_Domain</th>\n",
              "      <th>TinyURL</th>\n",
              "      <th>Prefix/Suffix</th>\n",
              "      <th>DNS_Record</th>\n",
              "      <th>Web_Traffic</th>\n",
              "      <th>Domain_Age</th>\n",
              "      <th>Domain_End</th>\n",
              "      <th>iFrame</th>\n",
              "      <th>Mouse_Over</th>\n",
              "      <th>Right_Click</th>\n",
              "      <th>Web_Forwards</th>\n",
              "      <th>Label</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>newburycafe.co.za</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>http://newburycafe.co.za/Webmail/41/Webmail/we...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aol-109024.weeblysite.com</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>https://aol-109024.weeblysite.com/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pixels-verify.pages.dev</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>http://pixels-verify.pages.dev</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>multicoin-system.web.app</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>https://multicoin-system.web.app/</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>att-100183.weeblysite.com</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>https://att-100183.weeblysite.com/</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b559539a-970c-4d70-9b49-baa7c1ac466b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b559539a-970c-4d70-9b49-baa7c1ac466b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b559539a-970c-4d70-9b49-baa7c1ac466b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-96bce9ee-42b1-4d82-9eb8-6bdb3ae62006\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96bce9ee-42b1-4d82-9eb8-6bdb3ae62006')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-96bce9ee-42b1-4d82-9eb8-6bdb3ae62006 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                      Domain  Have_IP  Have_At  URL_Length  URL_Depth  \\\n",
              "0          newburycafe.co.za        0        0           1          4   \n",
              "1  aol-109024.weeblysite.com        0        0           0          0   \n",
              "2    pixels-verify.pages.dev        0        0           0          0   \n",
              "3   multicoin-system.web.app        0        0           0          0   \n",
              "4  att-100183.weeblysite.com        0        0           0          0   \n",
              "\n",
              "   Redirection  https_Domain  TinyURL  Prefix/Suffix  DNS_Record  Web_Traffic  \\\n",
              "0            0             0        0              0           0            1   \n",
              "1            0             0        0              1           0            1   \n",
              "2            0             0        0              1           0            1   \n",
              "3            0             0        0              1           0            1   \n",
              "4            0             0        0              1           0            1   \n",
              "\n",
              "   Domain_Age  Domain_End  iFrame  Mouse_Over  Right_Click  Web_Forwards  \\\n",
              "0           0           1       0           1            1             0   \n",
              "1           1           1       0           1            1             0   \n",
              "2           1           1       1           1            1             0   \n",
              "3           1           1       1           1            1             0   \n",
              "4           1           1       0           1            1             0   \n",
              "\n",
              "   Label                                                url  \n",
              "0      1  http://newburycafe.co.za/Webmail/41/Webmail/we...  \n",
              "1      1                 https://aol-109024.weeblysite.com/  \n",
              "2      1                     http://pixels-verify.pages.dev  \n",
              "3      1                  https://multicoin-system.web.app/  \n",
              "4      1                 https://att-100183.weeblysite.com/  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "phishing.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrniqLdS4Cug"
      },
      "outputs": [],
      "source": [
        "phishing.to_csv('phishing.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aevgq8AvWQed",
        "outputId": "3deaae30-d040-4c9d-c6b3-413a09c26cb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-30 09:06:12,642 - whois.whois - ERROR - Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n",
            "ERROR:whois.whois:Error trying to connect to socket: closing socket - [Errno -2] Name or service not known\n"
          ]
        }
      ],
      "source": [
        "#Select 20 samples from the dataset of legitimate urls\n",
        "\n",
        "leg = pd.read_csv('/content/1.Benign_list_big_final.csv')\n",
        "leg.columns = ['URLs']\n",
        "\n",
        "legiurl = leg.sample(n = 1000, random_state = 12).copy()\n",
        "legiurl = legiurl.reset_index(drop=True)\n",
        "legiurl.head()\n",
        "\n",
        "legi_features = []\n",
        "label = 0\n",
        "\n",
        "for i in range(0, 250):\n",
        "  url = legiurl['URLs'][i]\n",
        "  legi_features.append(featureExtraction(url,label))\n",
        "\n",
        "#Add all the features into the phishing dataset\n",
        "#converting the list to dataframe\n",
        "feature_names = ['Domain', 'Have_IP', 'Have_At', 'URL_Length', 'URL_Depth','Redirection',\n",
        "                      'https_Domain', 'TinyURL', 'Prefix/Suffix', 'DNS_Record', 'Web_Traffic',\n",
        "                      'Domain_Age', 'Domain_End', 'iFrame', 'Mouse_Over','Right_Click', 'Web_Forwards', 'Label']\n",
        "\n",
        "legitimate = pd.DataFrame(legi_features, columns= feature_names)\n",
        "legitimate['url'] = legiurl['URLs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ClQa9rfvA1U"
      },
      "outputs": [],
      "source": [
        "legi_features = []\n",
        "label = 0\n",
        "\n",
        "for i in range(250, 500):\n",
        "  url = legiurl['URLs'][i]\n",
        "  legi_features.append(featureExtraction(url,label))\n",
        "legitimate2 = pd.DataFrame(legi_features, columns= feature_names)\n",
        "legitimate = pd.concatenate([legitimate, legitimate2],axis=0)\n",
        "legitimate['url'] = legiurl['URLs']\n",
        "legitimate.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c8ONnsKvM-3"
      },
      "outputs": [],
      "source": [
        "legitimate.to_csv('legitimate.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGGPozsNZugz"
      },
      "outputs": [],
      "source": [
        "urldata = pd.concat([legitimate, phishing]).reset_index(drop=True)\n",
        "urldata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "CahWkH1HozHX",
        "outputId": "df8c6f50-02e8-400f-ea45-7a3b307db649"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'urldata' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-07d06956d5b8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Storing the data in CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0murldata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urldata.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'urldata' is not defined"
          ]
        }
      ],
      "source": [
        "# Storing the data in CSV file\n",
        "urldata.to_csv('urldata.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SXs2jzWmSkMG"
      },
      "outputs": [],
      "source": [
        "urldata = pd.read_csv('urldata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "9X2DOkCEI2lS",
        "outputId": "c8e21e57-fc3a-4318-f73e-ecfac2507e63"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Domain</th>\n",
              "      <th>Have_IP</th>\n",
              "      <th>Have_At</th>\n",
              "      <th>URL_Length</th>\n",
              "      <th>URL_Depth</th>\n",
              "      <th>Redirection</th>\n",
              "      <th>https_Domain</th>\n",
              "      <th>TinyURL</th>\n",
              "      <th>Prefix/Suffix</th>\n",
              "      <th>DNS_Record</th>\n",
              "      <th>Web_Traffic</th>\n",
              "      <th>Domain_Age</th>\n",
              "      <th>Domain_End</th>\n",
              "      <th>iFrame</th>\n",
              "      <th>Mouse_Over</th>\n",
              "      <th>Right_Click</th>\n",
              "      <th>Web_Forwards</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gansta-paradise.com</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>webmailadmin0.myfreesites.net</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>scanmail.trustwave.com</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>snapchat.accounts.crimsonmart.com</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>momoshop.com.tw</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              Domain  Have_IP  Have_At  URL_Length  URL_Depth  \\\n",
              "0                gansta-paradise.com        0        0           0          1   \n",
              "1      webmailadmin0.myfreesites.net        0        0           0          0   \n",
              "2             scanmail.trustwave.com        0        0           1          0   \n",
              "3  snapchat.accounts.crimsonmart.com        0        0           0          1   \n",
              "4                    momoshop.com.tw        0        0           1          2   \n",
              "\n",
              "   Redirection  https_Domain  TinyURL  Prefix/Suffix  DNS_Record  Web_Traffic  \\\n",
              "0            0             0        0              1           0            1   \n",
              "1            0             0        0              0           0            0   \n",
              "2            0             0        0              0           0            1   \n",
              "3            0             0        1              0           0            1   \n",
              "4            0             0        0              0           0            1   \n",
              "\n",
              "   Domain_Age  Domain_End  iFrame  Mouse_Over  Right_Click  Web_Forwards  \\\n",
              "0           0           1       0           0            1             0   \n",
              "1           0           1       0           0            1             0   \n",
              "2           0           1       0           0            1             0   \n",
              "3           1           1       0           0            1             0   \n",
              "4           0           1       1           1            1             1   \n",
              "\n",
              "   Label  \n",
              "0      1  \n",
              "1      1  \n",
              "2      1  \n",
              "3      1  \n",
              "4      0  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Shuffle data points to improve generalizability of the model.\n",
        "\n",
        "urldata = urldata.sample(frac=1).reset_index(drop = True)\n",
        "urldata.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sEkjF4bRdA7L"
      },
      "outputs": [],
      "source": [
        "#Drop the columns that have zero contribution\n",
        "\n",
        "df = urldata.drop(columns = ['Domain','Domain_End','Domain_Age','Right_Click','Mouse_Over', 'DNS_Record','TinyURL','https_Domain','Redirection','Have_IP','Web_Forwards'], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_buyiTgpn3dF",
        "outputId": "732f8c7c-78ab-4dbf-f1d3-db5bc764430f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 7 columns):\n",
            " #   Column         Non-Null Count  Dtype\n",
            "---  ------         --------------  -----\n",
            " 0   Have_At        10000 non-null  int64\n",
            " 1   URL_Length     10000 non-null  int64\n",
            " 2   URL_Depth      10000 non-null  int64\n",
            " 3   Prefix/Suffix  10000 non-null  int64\n",
            " 4   Web_Traffic    10000 non-null  int64\n",
            " 5   iFrame         10000 non-null  int64\n",
            " 6   Label          10000 non-null  int64\n",
            "dtypes: int64(7)\n",
            "memory usage: 547.0 KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bre24yA9WH8i"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Initialize the scaler and imputer\n",
        "scaler = StandardScaler()\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Impute missing values in int columns\n",
        "int_columns = df.select_dtypes(include=['int']).columns\n",
        "df[int_columns] = scaler.fit_transform(df[int_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fD-IsAzX3ZxN"
      },
      "outputs": [],
      "source": [
        "#Function to plot the loss and accuracy curves\n",
        "\n",
        "def loss_and_accuracy(model, history):\n",
        "\n",
        "  los, acc = model.evaluate(X_train, y_train)\n",
        "\n",
        "  loss = history.history['loss']\n",
        " # val_loss = history.history['val_loss']\n",
        "  accuracy = history.history['accuracy']\n",
        "  #val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(len(history.history['loss']))\n",
        "\n",
        "  plt.figure(figsize = (6,4))\n",
        "  plt.plot(epochs , loss , label = 'training_loss')\n",
        " # plt.plot(epochs , label = 'validation_loss')\n",
        "  plt.title(f'Loss Curve: {los}')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.figure(figsize = (6,4))\n",
        "  plt.plot(epochs , accuracy , label = 'training_accuracy')\n",
        " # plt.plot(epochs , label = 'validation_accuracy')\n",
        "  plt.title(f'Accuracy Curve: {acc}')\n",
        "  plt.ylim(0.0,1.0)\n",
        "  plt.xlabel('epochs')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEtT0q5gqK3t",
        "outputId": "b4edc296-5216-40dc-b50a-b75741d1d492"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8000, 5), (8000,))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Convert dataframe into arrays for model input\n",
        "\n",
        "X = df.iloc[:,1:-1]\n",
        "y = df['Label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Split data into training and testing subsets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 42)\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSSEFTFVgLrS",
        "outputId": "ecabf568-a920-4d2d-8cbe-28b43edd644f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Label\n",
              " 1.0    5000\n",
              "-1.0    5000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Label\n",
              "1.0    5000\n",
              "0.0    5000\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Label'] = df['Label'].replace(-1,0)\n",
        "df['Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "2EpgmgXuOnm9",
        "outputId": "a222ad0c-1046-4cf3-f905-423dd130369f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;DecisionTreeClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(max_depth=5)</pre></div> </div></div></div></div>"
            ],
            "text/plain": [
              "DecisionTreeClassifier(max_depth=5)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Use decision tree classifier to identify the weightage of each of the variables and drop the unnecessary ones.\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(max_depth = 5)\n",
        "tree.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "52YI8ByxOnjL",
        "outputId": "544a978d-c04e-47b2-90d6-8125211ecb52"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 900x700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#checking the feature improtance in using decision trees\n",
        "plt.figure(figsize=(9,7))\n",
        "n_features = X_train.shape[1]\n",
        "plt.barh(range(n_features), tree.feature_importances_, align='center')\n",
        "plt.yticks(np.arange(n_features), X_train.columns)\n",
        "plt.xlabel(\"Feature importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "vXZQUB9jquOI",
        "outputId": "724559c8-d1e1-4135-82f1-64e62c1b8a11"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=5)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(max_depth=5)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest = RandomForestClassifier(max_depth=5)\n",
        "\n",
        "#Fit the model\n",
        "forest.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "26CrokVwe4Id",
        "outputId": "202d9a83-f7fa-42e0-b8c3-76ba78b1679c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "SVC()"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "model = svm.SVC()\n",
        "\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcGwuxPLrC9c",
        "outputId": "ea35c45f-df6f-4878-b274-1e8a65a18bbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest: Accuracy on training Data: 0.806\n",
            "Random forest: Accuracy on test Data: 0.799\n",
            "[[3948   59]\n",
            " [1489 2504]]\n",
            "Random forest: Accuracy on training Data: 0.801\n",
            "Random forest: Accuracy on test Data: 0.796\n",
            "[[3914   93]\n",
            " [1495 2498]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "y_test_forest = forest.predict(X_test)\n",
        "y_train_forest = forest.predict(X_train)\n",
        "\n",
        "#computing the accuracy of the RFC model performance\n",
        "acc_train_forest = accuracy_score(y_train,y_train_forest)\n",
        "acc_test_forest = accuracy_score(y_test,y_test_forest)\n",
        "\n",
        "print(\"Random forest: Accuracy on training Data: {:.3f}\".format(acc_train_forest))\n",
        "print(\"Random forest: Accuracy on test Data: {:.3f}\".format(acc_test_forest))\n",
        "print(confusion_matrix(y_train, y_train_forest))\n",
        "\n",
        "y_test_model = model.predict(X_test)\n",
        "y_train_model = model.predict(X_train)\n",
        "\n",
        "#computing the accuracy of the SVM model performance\n",
        "acc_train_model = accuracy_score(y_train,y_train_model)\n",
        "acc_test_model = accuracy_score(y_test,y_test_model)\n",
        "\n",
        "print(\"Random forest: Accuracy on training Data: {:.3f}\".format(acc_train_model))\n",
        "print(\"Random forest: Accuracy on test Data: {:.3f}\".format(acc_test_model))\n",
        "print(confusion_matrix(y_train, y_train_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7mOcIYXYMZh",
        "outputId": "59884afb-1eab-4aeb-dbdd-3941f5b86f0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8075\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'multi:softmax',  # For multiclass classification\n",
        "    'num_class': 2,  # Number of classes in the target variable\n",
        "    'eta': 0.3,  # Learning rate\n",
        "    'max_depth': 10,  # Maximum depth of the tree\n",
        "    'subsample': 0.8,  # Subsample ratio of the training instances\n",
        "    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree\n",
        "    'eval_metric': 'merror'  # Evaluation metric (multiclass classification error)\n",
        "}\n",
        "\n",
        "# Convert data into DMatrix format for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Train the model\n",
        "num_round = 100  # Number of boosting rounds\n",
        "model = xgb.train(params, dtrain, num_round)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(dtest)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo03Ao38YMWr",
        "outputId": "469f9f73-489e-4020-eea6-4f27f97785ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7995\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define MLP classifier\n",
        "#mlp_clf = MLPClassifier(hidden_layer_sizes=([100, 100,100]), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "mlp_clf = MLPClassifier(alpha=0.001, hidden_layer_sizes=([100,100,100]))\n",
        "# Train the classifier\n",
        "mlp_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = mlp_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDk-g2eOZwB1",
        "outputId": "be18dbfe-c4e8-4e55-8a18-ce15a7f69bcb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Web_Traffic\n",
              "1.0    8457\n",
              "0.0    1543\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Web_Traffic'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voK4s45iW2IJ"
      },
      "outputs": [],
      "source": [
        "#User-defined function that can predict the URL's authenticity when the URL and method are given as arguments.\n",
        "\n",
        "def prediction_model(url,model):\n",
        "  have_at = haveAtSign(url)\n",
        "  traffic = web_traffic(url)\n",
        "  length = getLength(url)\n",
        "  depth = getDepth(url)\n",
        "  prefix = prefixSuffix(url)\n",
        "  response = requests.get(url)\n",
        "  ifme = iframe(response)\n",
        "  output = [0,1]\n",
        "  input = [have_at, length, depth, prefix, traffic, ifme]\n",
        "  input = np.asarray(input)\n",
        "  input = input.reshape(1,-1)\n",
        "  pred = model.predict(input[0:6])\n",
        "  return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlS-SjBR4I5H",
        "outputId": "c8378f19-43a4-4d5f-d2cf-a0f5627e0d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The URL is authentic\n"
          ]
        }
      ],
      "source": [
        "#Prediction on a sample URL\n",
        "pred = prediction_model('https://www.geeksforgeeks.org/convert-python-list-to-numpy-arrays/',forest)\n",
        "\n",
        "if pred == 0:\n",
        "  print('The URL is authentic')\n",
        "else:\n",
        "  print(\"URL is malicious\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_qUuX9R-jnD",
        "outputId": "034b199f-f2a4-4834-b857-af9740c1c5e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.78825\n",
            "Classification Report:\n",
            "[[3968   41]\n",
            " [1653 2338]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels for the test set\n",
        "y_pred = nb_classifier.predict(X_train)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_train, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Generate a classification report\n",
        "print(\"Classification Report:\")\n",
        "print(confusion_matrix(y_train, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHipYETzfe_f",
        "outputId": "c1781767-8764-4f86-b6f1-2ed03d16a3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "250/250 [==============================] - 9s 6ms/step - loss: 0.4948 - accuracy: 0.5030\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.4138 - accuracy: 0.5030\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.4080 - accuracy: 0.5030\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.4065 - accuracy: 0.5030\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.4043 - accuracy: 0.5030\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with two hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric, trained on 5 epochs.\n",
        "'''\n",
        "\n",
        "model_1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model_1.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_1 = model_1.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XiZjEk7UEe_",
        "outputId": "80781a3c-e981-4355-96e1-8e79d5e9c2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "250/250 [==============================] - 2s 3ms/step - loss: 0.4850 - accuracy: 0.7711\n",
            "Epoch 2/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4139 - accuracy: 0.8027\n",
            "Epoch 3/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4083 - accuracy: 0.8040\n",
            "Epoch 4/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4093 - accuracy: 0.8044\n",
            "Epoch 5/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4052 - accuracy: 0.8055\n",
            "Epoch 6/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4035 - accuracy: 0.8039\n",
            "Epoch 7/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4020 - accuracy: 0.8059\n",
            "Epoch 8/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3996 - accuracy: 0.8099\n",
            "Epoch 9/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4011 - accuracy: 0.8067\n",
            "Epoch 10/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3979 - accuracy: 0.8110\n",
            "Epoch 11/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3963 - accuracy: 0.8138\n",
            "Epoch 12/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3970 - accuracy: 0.8123\n",
            "Epoch 13/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3968 - accuracy: 0.8134\n",
            "Epoch 14/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3953 - accuracy: 0.8131\n",
            "Epoch 15/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3941 - accuracy: 0.8149\n",
            "Epoch 16/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3946 - accuracy: 0.8138\n",
            "Epoch 17/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3945 - accuracy: 0.8156\n",
            "Epoch 18/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3932 - accuracy: 0.8161\n",
            "Epoch 19/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3930 - accuracy: 0.8155\n",
            "Epoch 20/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3936 - accuracy: 0.8164\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with two hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric, learning rate of 0.001, trained on 20 epochs.\n",
        "'''\n",
        "\n",
        "model_2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_2.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_2 = model_2.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-88NKBEUmqh",
        "outputId": "ed4841cc-b31a-4820-c783-fa7eacbb0d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "250/250 [==============================] - 6s 5ms/step - loss: 0.4510 - accuracy: 0.7804\n",
            "Epoch 2/20\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.4200 - accuracy: 0.8027\n",
            "Epoch 3/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4147 - accuracy: 0.8030\n",
            "Epoch 4/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4075 - accuracy: 0.8075\n",
            "Epoch 5/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4049 - accuracy: 0.8104\n",
            "Epoch 6/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4033 - accuracy: 0.8100\n",
            "Epoch 7/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4014 - accuracy: 0.8094\n",
            "Epoch 8/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.4014 - accuracy: 0.8092\n",
            "Epoch 9/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3972 - accuracy: 0.8156\n",
            "Epoch 10/20\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3948 - accuracy: 0.8154\n",
            "Epoch 11/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3938 - accuracy: 0.8170\n",
            "Epoch 12/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3989 - accuracy: 0.8141\n",
            "Epoch 13/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4055 - accuracy: 0.8106\n",
            "Epoch 14/20\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.3996 - accuracy: 0.8139\n",
            "Epoch 15/20\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.3962 - accuracy: 0.8146\n",
            "Epoch 16/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3958 - accuracy: 0.8145\n",
            "Epoch 17/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3938 - accuracy: 0.8146\n",
            "Epoch 18/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3942 - accuracy: 0.8167\n",
            "Epoch 19/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3930 - accuracy: 0.8154\n",
            "Epoch 20/20\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3935 - accuracy: 0.8158\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with five hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.001, trained on 20 epochs.\n",
        "'''\n",
        "model_3 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_3.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_3 = model_3.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9wIRxYvXWxF",
        "outputId": "2914dc92-1781-43eb-bd09-1ea44696eb86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 3s 4ms/step - loss: 0.5118 - accuracy: 0.7701\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4150 - accuracy: 0.8015\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4075 - accuracy: 0.8036\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4041 - accuracy: 0.8056\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.4028 - accuracy: 0.8094\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3990 - accuracy: 0.8104\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3968 - accuracy: 0.8120\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3991 - accuracy: 0.8101\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.3966 - accuracy: 0.8117\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3951 - accuracy: 0.8124\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3961 - accuracy: 0.8131\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3965 - accuracy: 0.8108\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 0.3944 - accuracy: 0.8151\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3936 - accuracy: 0.8131\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.3938 - accuracy: 0.8155\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with five hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, trained on 15 epochs.\n",
        "'''\n",
        "model_4 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_4.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_4 = model_4.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx55a5C9ZA3o",
        "outputId": "a0c9c802-15e9-4659-aa46-f19df97c6233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 11s 27ms/step - loss: 0.7950 - accuracy: 0.7335\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 7s 28ms/step - loss: 0.6393 - accuracy: 0.7879\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 7s 29ms/step - loss: 0.6017 - accuracy: 0.7890\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 7s 26ms/step - loss: 0.5763 - accuracy: 0.7914\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 6s 25ms/step - loss: 0.5621 - accuracy: 0.7941\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 6s 26ms/step - loss: 0.5489 - accuracy: 0.7954\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 7s 27ms/step - loss: 0.5402 - accuracy: 0.7976\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 7s 26ms/step - loss: 0.5347 - accuracy: 0.7961\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 7s 26ms/step - loss: 0.5302 - accuracy: 0.7985\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 6s 26ms/step - loss: 0.5253 - accuracy: 0.8001\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 7s 27ms/step - loss: 0.5221 - accuracy: 0.7979\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 0.5186 - accuracy: 0.7997\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 6s 25ms/step - loss: 0.5168 - accuracy: 0.8010\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 7s 28ms/step - loss: 0.5129 - accuracy: 0.7987\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.5097 - accuracy: 0.7994\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with five hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, kernel regularizer for regularization, trained on 15 epochs.\n",
        "'''\n",
        "model_5 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_5.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_5 = model_5.fit(X_train,\n",
        "                        y_train,\n",
        "                        epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulaL5Pveh3pd"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "ANN model with eleven hidden layers, increasing and decreasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, trained on 15 epochs.\n",
        "'''\n",
        "model_6 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_6.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_6 = model_6.fit(X_train,\n",
        "                        y_train,\n",
        "                        validation_data = (X_test, y_test),\n",
        "                        epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR2KT83Ml6B9",
        "outputId": "762bd3ac-1b15-4fb1-b822-50a1b100d981"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 114s 21ms/step - loss: -247.3423 - accuracy: 0.2529\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: -6198.8516 - accuracy: 0.2264\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: -30654.6621 - accuracy: 0.1861\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: -86040.7344 - accuracy: 0.1700\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 5s 20ms/step - loss: -189522.8906 - accuracy: 0.1540\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: -362355.5000 - accuracy: 0.1565\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 4s 15ms/step - loss: -606770.6250 - accuracy: 0.1869\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 5s 22ms/step - loss: -964998.9375 - accuracy: 0.1695\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: -1424693.7500 - accuracy: 0.1649\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 5s 18ms/step - loss: -2023545.0000 - accuracy: 0.1625\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 5s 20ms/step - loss: -2796396.7500 - accuracy: 0.1750\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: -3685086.5000 - accuracy: 0.1628\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: -4810475.5000 - accuracy: 0.1591\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: -6053253.0000 - accuracy: 0.1506\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: -7570462.5000 - accuracy: 0.1647\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with five hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, batch normalization trained on 15 epochs.\n",
        "'''\n",
        "model_7 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_7.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_7 = model_7.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7jjI9NVpZlF7",
        "outputId": "98ed62f2-530c-429a-9594-6ff5ede0a3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 4s 9ms/step - loss: -0.3180 - accuracy: 0.2603 - val_loss: -1.6629 - val_accuracy: 0.2640\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: -3.7484 - accuracy: 0.2739 - val_loss: -7.1461 - val_accuracy: 0.2640\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -11.2463 - accuracy: 0.2772 - val_loss: -17.6195 - val_accuracy: 0.2730\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -23.1311 - accuracy: 0.2799 - val_loss: -32.5123 - val_accuracy: 0.2730\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -38.8407 - accuracy: 0.2797 - val_loss: -51.0065 - val_accuracy: 0.2715\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -57.9017 - accuracy: 0.2861 - val_loss: -72.7873 - val_accuracy: 0.2775\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -80.0304 - accuracy: 0.2870 - val_loss: -97.8536 - val_accuracy: 0.2800\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -104.8065 - accuracy: 0.2884 - val_loss: -125.5450 - val_accuracy: 0.2810\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -132.1218 - accuracy: 0.2899 - val_loss: -155.8402 - val_accuracy: 0.2890\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -161.7278 - accuracy: 0.2906 - val_loss: -188.3765 - val_accuracy: 0.2890\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -193.6336 - accuracy: 0.2905 - val_loss: -223.3241 - val_accuracy: 0.2845\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -227.6147 - accuracy: 0.2899 - val_loss: -260.3144 - val_accuracy: 0.2835\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -263.6501 - accuracy: 0.2891 - val_loss: -299.7476 - val_accuracy: 0.2865\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -301.7172 - accuracy: 0.2923 - val_loss: -341.0286 - val_accuracy: 0.2810\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -341.7562 - accuracy: 0.2904 - val_loss: -384.2933 - val_accuracy: 0.2845\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -383.5862 - accuracy: 0.2919 - val_loss: -429.4443 - val_accuracy: 0.2830\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -427.3195 - accuracy: 0.2904 - val_loss: -477.0214 - val_accuracy: 0.2830\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -473.0622 - accuracy: 0.2911 - val_loss: -526.4074 - val_accuracy: 0.2835\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -520.8317 - accuracy: 0.2914 - val_loss: -577.6580 - val_accuracy: 0.2830\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -570.4174 - accuracy: 0.2915 - val_loss: -631.2251 - val_accuracy: 0.2845\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -621.7632 - accuracy: 0.2920 - val_loss: -686.3514 - val_accuracy: 0.2835\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -674.8143 - accuracy: 0.2914 - val_loss: -743.4934 - val_accuracy: 0.2870\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -729.8557 - accuracy: 0.2928 - val_loss: -802.3517 - val_accuracy: 0.2840\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -786.6010 - accuracy: 0.2923 - val_loss: -863.2973 - val_accuracy: 0.2875\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -845.2422 - accuracy: 0.2934 - val_loss: -926.1965 - val_accuracy: 0.2840\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -905.7833 - accuracy: 0.2928 - val_loss: -991.3799 - val_accuracy: 0.2815\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -968.1210 - accuracy: 0.2934 - val_loss: -1058.0746 - val_accuracy: 0.2810\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: -1032.1060 - accuracy: 0.2921 - val_loss: -1126.3973 - val_accuracy: 0.2870\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -1098.0199 - accuracy: 0.2936 - val_loss: -1197.5353 - val_accuracy: 0.2840\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: -1165.6598 - accuracy: 0.2930 - val_loss: -1269.2755 - val_accuracy: 0.2875\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -1234.9803 - accuracy: 0.2912 - val_loss: -1343.7217 - val_accuracy: 0.2820\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -1305.7262 - accuracy: 0.2926 - val_loss: -1419.2075 - val_accuracy: 0.2850\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: -1378.4303 - accuracy: 0.2926 - val_loss: -1496.5715 - val_accuracy: 0.2840\n",
            "Epoch 34/50\n",
            "135/250 [===============>..............] - ETA: 0s - loss: -1470.8837 - accuracy: 0.2903"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b25619a7ae6c>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Input, Multiply, Softmax\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def attention_mechanism(inputs):\n",
        "    attention_probs = Dense(inputs.shape[-1], activation='softmax')(inputs)\n",
        "    attention_mul = Multiply()([inputs, attention_probs])\n",
        "    return attention_mul\n",
        "\n",
        "input_layer = Input(shape=(X.shape[1],))\n",
        "\n",
        "# Apply attention mechanism\n",
        "attention_layer = attention_mechanism(input_layer)\n",
        "\n",
        "# Add more layers as needed\n",
        "dense_layer = Dense(64, activation='relu')(attention_layer)\n",
        "output_layer = Dense(1, activation='sigmoid')(dense_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=50, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6z3e6zjZwUB",
        "outputId": "49b00914-a3db-4237-a6fc-ab247d804817"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8000, 4), (8000,))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data1 = df.drop(columns = ['Have_At'], axis = 1)\n",
        "\n",
        "X = data1.iloc[:,1:-1]\n",
        "y = data1['Label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Split data into training and testing subsets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 12)\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIefxBHeZ-VE",
        "outputId": "7258ace5-1736-45c0-a78b-26d560a14769"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 10s 25ms/step - loss: 0.6029 - accuracy: 0.6488\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.5932 - accuracy: 0.6470\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.5883 - accuracy: 0.6634\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.5859 - accuracy: 0.6635\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 6s 22ms/step - loss: 0.5856 - accuracy: 0.6579\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 6s 25ms/step - loss: 0.5814 - accuracy: 0.6619\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.5817 - accuracy: 0.6662\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.5801 - accuracy: 0.6664\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.5806 - accuracy: 0.6615\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.5779 - accuracy: 0.6681\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 6s 23ms/step - loss: 0.5822 - accuracy: 0.6654\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.5761 - accuracy: 0.6671\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.5792 - accuracy: 0.6693\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 6s 23ms/step - loss: 0.5772 - accuracy: 0.6702\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.5800 - accuracy: 0.6656\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with eleven hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, batch normalization trained on 15 epochs.\n",
        "'''\n",
        "\n",
        "model_8 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_8.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_8 = model_8.fit(X_train,\n",
        "                        y_train,\n",
        "                        epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNQBt21nbdXO",
        "outputId": "9db9170b-7edb-4592-f4e5-13be0506dc7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8000, 4), (8000,))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data2 = df.drop(columns = ['Web_Traffic'], axis = 1)\n",
        "X = data2.iloc[:,1:-1]\n",
        "y = data2['Label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Split data into training and testing subsets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 12)\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecoMpMmlbiLN",
        "outputId": "75940e5e-3c8a-4e1c-9e2b-183d219043c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 8s 20ms/step - loss: 0.4590 - accuracy: 0.7688\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.4363 - accuracy: 0.7854\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.4357 - accuracy: 0.7821\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.4332 - accuracy: 0.7854\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.4286 - accuracy: 0.7883\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.4310 - accuracy: 0.7854\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 6s 22ms/step - loss: 0.4291 - accuracy: 0.7855\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.4304 - accuracy: 0.7859\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.4299 - accuracy: 0.7878\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.4261 - accuracy: 0.7869\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.4262 - accuracy: 0.7880\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.4259 - accuracy: 0.7871\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.4261 - accuracy: 0.7879\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.4288 - accuracy: 0.7847\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 6s 23ms/step - loss: 0.4262 - accuracy: 0.7864\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with five hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, batch normalization trained on 15 epochs.\n",
        "'''\n",
        "model_9 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_9.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_9 = model_9.fit(X_train,\n",
        "                        y_train,\n",
        "                        epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S854-5jRbpj_",
        "outputId": "62aa03db-4343-4a48-d1ba-74c5dfe031a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((8000, 4), (8000,))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data3 = df.drop(columns = ['iFrame'], axis = 1)\n",
        "\n",
        "X = data3.iloc[:,1:-1]\n",
        "y = data3['Label']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Split data into training and testing subsets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size = 0.2,\n",
        "                                                    random_state = 12)\n",
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcw4lI3tgpyk",
        "outputId": "4f3ab47f-a9d5-43d5-f42d-7fbaad4b8423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "250/250 [==============================] - 12s 17ms/step - loss: 0.4560 - accuracy: 0.7701\n",
            "Epoch 2/15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.4383 - accuracy: 0.7840\n",
            "Epoch 3/15\n",
            "250/250 [==============================] - 6s 23ms/step - loss: 0.4365 - accuracy: 0.7819\n",
            "Epoch 4/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.4321 - accuracy: 0.7845\n",
            "Epoch 5/15\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.4325 - accuracy: 0.7879\n",
            "Epoch 6/15\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.4273 - accuracy: 0.7857\n",
            "Epoch 7/15\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.4275 - accuracy: 0.7839\n",
            "Epoch 8/15\n",
            "250/250 [==============================] - 5s 22ms/step - loss: 0.4254 - accuracy: 0.7871\n",
            "Epoch 9/15\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.4262 - accuracy: 0.7925\n",
            "Epoch 10/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.4270 - accuracy: 0.7855\n",
            "Epoch 11/15\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.4254 - accuracy: 0.7846\n",
            "Epoch 12/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.4242 - accuracy: 0.7866\n",
            "Epoch 13/15\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.4245 - accuracy: 0.7876\n",
            "Epoch 14/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.4235 - accuracy: 0.7860\n",
            "Epoch 15/15\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.4234 - accuracy: 0.7890\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with five hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, batch normalization trained on 15 epochs.\n",
        "'''\n",
        "model_10 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_10.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history_10 = model_10.fit(X_train,\n",
        "                        y_train,\n",
        "                        epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H9834EAgvvw",
        "outputId": "e80ad62d-33d0-4aa3-d43a-72b69c6ab040"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  Batch 2847/8000, Loss: 0.9309516549110413, Accuracy: 0.0\n",
            "  Batch 2848/8000, Loss: 1.2708759307861328, Accuracy: 0.0\n",
            "  Batch 2849/8000, Loss: 0.12561747431755066, Accuracy: 1.0\n",
            "  Batch 2850/8000, Loss: 0.12612396478652954, Accuracy: 1.0\n",
            "  Batch 2851/8000, Loss: 0.38682523369789124, Accuracy: 1.0\n",
            "  Batch 2852/8000, Loss: 0.1261993646621704, Accuracy: 1.0\n",
            "  Batch 2853/8000, Loss: 0.5625436305999756, Accuracy: 1.0\n",
            "  Batch 2854/8000, Loss: 0.3943256735801697, Accuracy: 1.0\n",
            "  Batch 2855/8000, Loss: 0.3731466233730316, Accuracy: 1.0\n",
            "  Batch 2856/8000, Loss: 0.1244683787226677, Accuracy: 1.0\n",
            "  Batch 2857/8000, Loss: 0.4959091544151306, Accuracy: 1.0\n",
            "  Batch 2858/8000, Loss: 0.37370818853378296, Accuracy: 1.0\n",
            "  Batch 2859/8000, Loss: 0.6936402916908264, Accuracy: 1.0\n",
            "  Batch 2860/8000, Loss: 0.12676940858364105, Accuracy: 1.0\n",
            "  Batch 2861/8000, Loss: 0.5378097891807556, Accuracy: 1.0\n",
            "  Batch 2862/8000, Loss: 0.5212622284889221, Accuracy: 1.0\n",
            "  Batch 2863/8000, Loss: 0.38560718297958374, Accuracy: 1.0\n",
            "  Batch 2864/8000, Loss: 0.12441650778055191, Accuracy: 1.0\n",
            "  Batch 2865/8000, Loss: 0.7416833639144897, Accuracy: 1.0\n",
            "  Batch 2866/8000, Loss: 0.6346021890640259, Accuracy: 1.0\n",
            "  Batch 2867/8000, Loss: 0.34931808710098267, Accuracy: 1.0\n",
            "  Batch 2868/8000, Loss: 0.6137982606887817, Accuracy: 1.0\n",
            "  Batch 2869/8000, Loss: 0.12583079934120178, Accuracy: 1.0\n",
            "  Batch 2870/8000, Loss: 0.33923232555389404, Accuracy: 1.0\n",
            "  Batch 2871/8000, Loss: 0.12855762243270874, Accuracy: 1.0\n",
            "  Batch 2872/8000, Loss: 0.39148110151290894, Accuracy: 1.0\n",
            "  Batch 2873/8000, Loss: 1.1012898683547974, Accuracy: 0.0\n",
            "  Batch 2874/8000, Loss: 0.3924952745437622, Accuracy: 1.0\n",
            "  Batch 2875/8000, Loss: 1.327500343322754, Accuracy: 0.0\n",
            "  Batch 2876/8000, Loss: 0.1258755475282669, Accuracy: 1.0\n",
            "  Batch 2877/8000, Loss: 0.37156933546066284, Accuracy: 1.0\n",
            "  Batch 2878/8000, Loss: 0.1286286860704422, Accuracy: 1.0\n",
            "  Batch 2879/8000, Loss: 0.12790802121162415, Accuracy: 1.0\n",
            "  Batch 2880/8000, Loss: 1.0412250757217407, Accuracy: 0.0\n",
            "  Batch 2881/8000, Loss: 0.290909081697464, Accuracy: 1.0\n",
            "  Batch 2882/8000, Loss: 0.4405291676521301, Accuracy: 1.0\n",
            "  Batch 2883/8000, Loss: 0.46611112356185913, Accuracy: 1.0\n",
            "  Batch 2884/8000, Loss: 0.2872973084449768, Accuracy: 1.0\n",
            "  Batch 2885/8000, Loss: 0.37421566247940063, Accuracy: 1.0\n",
            "  Batch 2886/8000, Loss: 0.4955812692642212, Accuracy: 1.0\n",
            "  Batch 2887/8000, Loss: 0.24691270291805267, Accuracy: 1.0\n",
            "  Batch 2888/8000, Loss: 1.2491486072540283, Accuracy: 0.0\n",
            "  Batch 2889/8000, Loss: 0.2870315909385681, Accuracy: 1.0\n",
            "  Batch 2890/8000, Loss: 0.34927600622177124, Accuracy: 1.0\n",
            "  Batch 2891/8000, Loss: 0.12557967007160187, Accuracy: 1.0\n",
            "  Batch 2892/8000, Loss: 0.3160707354545593, Accuracy: 1.0\n",
            "  Batch 2893/8000, Loss: 0.49936285614967346, Accuracy: 1.0\n",
            "  Batch 2894/8000, Loss: 2.0085277557373047, Accuracy: 0.0\n",
            "  Batch 2895/8000, Loss: 0.4503259062767029, Accuracy: 1.0\n",
            "  Batch 2896/8000, Loss: 0.466433048248291, Accuracy: 1.0\n",
            "  Batch 2897/8000, Loss: 0.26084446907043457, Accuracy: 1.0\n",
            "  Batch 2898/8000, Loss: 0.12419988214969635, Accuracy: 1.0\n",
            "  Batch 2899/8000, Loss: 1.4814645051956177, Accuracy: 0.0\n",
            "  Batch 2900/8000, Loss: 0.12557268142700195, Accuracy: 1.0\n",
            "  Batch 2901/8000, Loss: 0.4008280336856842, Accuracy: 1.0\n",
            "  Batch 2902/8000, Loss: 0.14181968569755554, Accuracy: 1.0\n",
            "  Batch 2903/8000, Loss: 0.24782979488372803, Accuracy: 1.0\n",
            "  Batch 2904/8000, Loss: 0.25808149576187134, Accuracy: 1.0\n",
            "  Batch 2905/8000, Loss: 0.3373231291770935, Accuracy: 1.0\n",
            "  Batch 2906/8000, Loss: 1.9345450401306152, Accuracy: 0.0\n",
            "  Batch 2907/8000, Loss: 0.12415742129087448, Accuracy: 1.0\n",
            "  Batch 2908/8000, Loss: 0.28171828389167786, Accuracy: 1.0\n",
            "  Batch 2909/8000, Loss: 0.25534090399742126, Accuracy: 1.0\n",
            "  Batch 2910/8000, Loss: 0.8751052618026733, Accuracy: 0.0\n",
            "  Batch 2911/8000, Loss: 1.9749016761779785, Accuracy: 0.0\n",
            "  Batch 2912/8000, Loss: 0.31393805146217346, Accuracy: 1.0\n",
            "  Batch 2913/8000, Loss: 0.1240704208612442, Accuracy: 1.0\n",
            "  Batch 2914/8000, Loss: 0.2850709855556488, Accuracy: 1.0\n",
            "  Batch 2915/8000, Loss: 0.2332373857498169, Accuracy: 1.0\n",
            "  Batch 2916/8000, Loss: 0.5363211631774902, Accuracy: 1.0\n",
            "  Batch 2917/8000, Loss: 0.4172050654888153, Accuracy: 1.0\n",
            "  Batch 2918/8000, Loss: 0.12845945358276367, Accuracy: 1.0\n",
            "  Batch 2919/8000, Loss: 0.4539512097835541, Accuracy: 1.0\n",
            "  Batch 2920/8000, Loss: 0.24632610380649567, Accuracy: 1.0\n",
            "  Batch 2921/8000, Loss: 2.0979831218719482, Accuracy: 0.0\n",
            "  Batch 2922/8000, Loss: 0.16535690426826477, Accuracy: 1.0\n",
            "  Batch 2923/8000, Loss: 0.28920263051986694, Accuracy: 1.0\n",
            "  Batch 2924/8000, Loss: 0.33519357442855835, Accuracy: 1.0\n",
            "  Batch 2925/8000, Loss: 0.1239958107471466, Accuracy: 1.0\n",
            "  Batch 2926/8000, Loss: 1.5511189699172974, Accuracy: 0.0\n",
            "  Batch 2927/8000, Loss: 0.2905532717704773, Accuracy: 1.0\n",
            "  Batch 2928/8000, Loss: 0.5478475689888, Accuracy: 1.0\n",
            "  Batch 2929/8000, Loss: 0.4197522699832916, Accuracy: 1.0\n",
            "  Batch 2930/8000, Loss: 0.277416855096817, Accuracy: 1.0\n",
            "  Batch 2931/8000, Loss: 0.5074614882469177, Accuracy: 1.0\n",
            "  Batch 2932/8000, Loss: 0.32381558418273926, Accuracy: 1.0\n",
            "  Batch 2933/8000, Loss: 0.12674134969711304, Accuracy: 1.0\n",
            "  Batch 2934/8000, Loss: 0.3833269476890564, Accuracy: 1.0\n",
            "  Batch 2935/8000, Loss: 0.314883291721344, Accuracy: 1.0\n",
            "  Batch 2936/8000, Loss: 0.2864566445350647, Accuracy: 1.0\n",
            "  Batch 2937/8000, Loss: 1.0171996355056763, Accuracy: 0.0\n",
            "  Batch 2938/8000, Loss: 0.2847073972225189, Accuracy: 1.0\n",
            "  Batch 2939/8000, Loss: 2.215892791748047, Accuracy: 0.0\n",
            "  Batch 2940/8000, Loss: 0.36627885699272156, Accuracy: 1.0\n",
            "  Batch 2941/8000, Loss: 0.31110307574272156, Accuracy: 1.0\n",
            "  Batch 2942/8000, Loss: 0.39292117953300476, Accuracy: 1.0\n",
            "  Batch 2943/8000, Loss: 0.27280884981155396, Accuracy: 1.0\n",
            "  Batch 2944/8000, Loss: 1.6747851371765137, Accuracy: 0.0\n",
            "  Batch 2945/8000, Loss: 0.4658108651638031, Accuracy: 1.0\n",
            "  Batch 2946/8000, Loss: 0.39901474118232727, Accuracy: 1.0\n",
            "  Batch 2947/8000, Loss: 1.1345255374908447, Accuracy: 0.0\n",
            "  Batch 2948/8000, Loss: 0.3085618019104004, Accuracy: 1.0\n",
            "  Batch 2949/8000, Loss: 0.12691274285316467, Accuracy: 1.0\n",
            "  Batch 2950/8000, Loss: 0.12627480924129486, Accuracy: 1.0\n",
            "  Batch 2951/8000, Loss: 0.2832626402378082, Accuracy: 1.0\n",
            "  Batch 2952/8000, Loss: 0.26132696866989136, Accuracy: 1.0\n",
            "  Batch 2953/8000, Loss: 2.0740132331848145, Accuracy: 0.0\n",
            "  Batch 2954/8000, Loss: 0.27574843168258667, Accuracy: 1.0\n",
            "  Batch 2955/8000, Loss: 1.77755606174469, Accuracy: 0.0\n",
            "  Batch 2956/8000, Loss: 0.30954164266586304, Accuracy: 1.0\n",
            "  Batch 2957/8000, Loss: 0.4960952401161194, Accuracy: 1.0\n",
            "  Batch 2958/8000, Loss: 0.47015559673309326, Accuracy: 1.0\n",
            "  Batch 2959/8000, Loss: 0.29664146900177, Accuracy: 1.0\n",
            "  Batch 2960/8000, Loss: 1.93364679813385, Accuracy: 0.0\n",
            "  Batch 2961/8000, Loss: 1.186591625213623, Accuracy: 0.0\n",
            "  Batch 2962/8000, Loss: 0.30283257365226746, Accuracy: 1.0\n",
            "  Batch 2963/8000, Loss: 0.5403473377227783, Accuracy: 1.0\n",
            "  Batch 2964/8000, Loss: 0.12464453279972076, Accuracy: 1.0\n",
            "  Batch 2965/8000, Loss: 0.6238631010055542, Accuracy: 1.0\n",
            "  Batch 2966/8000, Loss: 0.32245784997940063, Accuracy: 1.0\n",
            "  Batch 2967/8000, Loss: 0.6246061325073242, Accuracy: 1.0\n",
            "  Batch 2968/8000, Loss: 0.12360887974500656, Accuracy: 1.0\n",
            "  Batch 2969/8000, Loss: 0.5088701248168945, Accuracy: 1.0\n",
            "  Batch 2970/8000, Loss: 1.9515248537063599, Accuracy: 0.0\n",
            "  Batch 2971/8000, Loss: 0.42705750465393066, Accuracy: 1.0\n",
            "  Batch 2972/8000, Loss: 0.9409592151641846, Accuracy: 0.0\n",
            "  Batch 2973/8000, Loss: 0.5067875981330872, Accuracy: 1.0\n",
            "  Batch 2974/8000, Loss: 0.12355183064937592, Accuracy: 1.0\n",
            "  Batch 2975/8000, Loss: 0.1245691180229187, Accuracy: 1.0\n",
            "  Batch 2976/8000, Loss: 0.34197166562080383, Accuracy: 1.0\n",
            "  Batch 2977/8000, Loss: 0.35593181848526, Accuracy: 1.0\n",
            "  Batch 2978/8000, Loss: 0.12468099594116211, Accuracy: 1.0\n",
            "  Batch 2979/8000, Loss: 1.227687954902649, Accuracy: 0.0\n",
            "  Batch 2980/8000, Loss: 0.12526537477970123, Accuracy: 1.0\n",
            "  Batch 2981/8000, Loss: 0.12348202615976334, Accuracy: 1.0\n",
            "  Batch 2982/8000, Loss: 0.45452991127967834, Accuracy: 1.0\n",
            "  Batch 2983/8000, Loss: 0.3413582146167755, Accuracy: 1.0\n",
            "  Batch 2984/8000, Loss: 1.0985627174377441, Accuracy: 0.0\n",
            "  Batch 2985/8000, Loss: 0.5309168100357056, Accuracy: 1.0\n",
            "  Batch 2986/8000, Loss: 0.12638047337532043, Accuracy: 1.0\n",
            "  Batch 2987/8000, Loss: 0.2953512668609619, Accuracy: 1.0\n",
            "  Batch 2988/8000, Loss: 0.3346537947654724, Accuracy: 1.0\n",
            "  Batch 2989/8000, Loss: 1.1665711402893066, Accuracy: 0.0\n",
            "  Batch 2990/8000, Loss: 0.3641403615474701, Accuracy: 1.0\n",
            "  Batch 2991/8000, Loss: 1.5321717262268066, Accuracy: 0.0\n",
            "  Batch 2992/8000, Loss: 0.5207865238189697, Accuracy: 1.0\n",
            "  Batch 2993/8000, Loss: 0.32844382524490356, Accuracy: 1.0\n",
            "  Batch 2994/8000, Loss: 0.5502679347991943, Accuracy: 1.0\n",
            "  Batch 2995/8000, Loss: 0.1252913475036621, Accuracy: 1.0\n",
            "  Batch 2996/8000, Loss: 0.12671130895614624, Accuracy: 1.0\n",
            "  Batch 2997/8000, Loss: 0.1274157464504242, Accuracy: 1.0\n",
            "  Batch 2998/8000, Loss: 0.31601136922836304, Accuracy: 1.0\n",
            "  Batch 2999/8000, Loss: 0.34772545099258423, Accuracy: 1.0\n",
            "  Batch 3000/8000, Loss: 0.12394075840711594, Accuracy: 1.0\n",
            "  Batch 3001/8000, Loss: 0.3319132626056671, Accuracy: 1.0\n",
            "  Batch 3002/8000, Loss: 0.4155861437320709, Accuracy: 1.0\n",
            "  Batch 3003/8000, Loss: 0.12987057864665985, Accuracy: 1.0\n",
            "  Batch 3004/8000, Loss: 0.5541982650756836, Accuracy: 1.0\n",
            "  Batch 3005/8000, Loss: 0.3335460424423218, Accuracy: 1.0\n",
            "  Batch 3006/8000, Loss: 0.3308972120285034, Accuracy: 1.0\n",
            "  Batch 3007/8000, Loss: 0.6005797982215881, Accuracy: 1.0\n",
            "  Batch 3008/8000, Loss: 1.814969539642334, Accuracy: 0.0\n",
            "  Batch 3009/8000, Loss: 0.12331334501504898, Accuracy: 1.0\n",
            "  Batch 3010/8000, Loss: 1.2614660263061523, Accuracy: 0.0\n",
            "  Batch 3011/8000, Loss: 0.1399887204170227, Accuracy: 1.0\n",
            "  Batch 3012/8000, Loss: 0.35272330045700073, Accuracy: 1.0\n",
            "  Batch 3013/8000, Loss: 1.2310309410095215, Accuracy: 0.0\n",
            "  Batch 3014/8000, Loss: 1.290686011314392, Accuracy: 0.0\n",
            "  Batch 3015/8000, Loss: 0.5212060213088989, Accuracy: 1.0\n",
            "  Batch 3016/8000, Loss: 0.12510061264038086, Accuracy: 1.0\n",
            "  Batch 3017/8000, Loss: 0.3247109353542328, Accuracy: 1.0\n",
            "  Batch 3018/8000, Loss: 0.3182431757450104, Accuracy: 1.0\n",
            "  Batch 3019/8000, Loss: 0.3113398849964142, Accuracy: 1.0\n",
            "  Batch 3020/8000, Loss: 0.32209011912345886, Accuracy: 1.0\n",
            "  Batch 3021/8000, Loss: 1.0859003067016602, Accuracy: 0.0\n",
            "  Batch 3022/8000, Loss: 0.521628201007843, Accuracy: 1.0\n",
            "  Batch 3023/8000, Loss: 0.4955923557281494, Accuracy: 1.0\n",
            "  Batch 3024/8000, Loss: 0.3368353247642517, Accuracy: 1.0\n",
            "  Batch 3025/8000, Loss: 0.3147619962692261, Accuracy: 1.0\n",
            "  Batch 3026/8000, Loss: 0.33577269315719604, Accuracy: 1.0\n",
            "  Batch 3027/8000, Loss: 0.3191692531108856, Accuracy: 1.0\n",
            "  Batch 3028/8000, Loss: 0.5723757147789001, Accuracy: 1.0\n",
            "  Batch 3029/8000, Loss: 0.30445486307144165, Accuracy: 1.0\n",
            "  Batch 3030/8000, Loss: 1.6973954439163208, Accuracy: 0.0\n",
            "  Batch 3031/8000, Loss: 0.31306663155555725, Accuracy: 1.0\n",
            "  Batch 3032/8000, Loss: 0.12523715198040009, Accuracy: 1.0\n",
            "  Batch 3033/8000, Loss: 0.30926233530044556, Accuracy: 1.0\n",
            "  Batch 3034/8000, Loss: 0.12367124855518341, Accuracy: 1.0\n",
            "  Batch 3035/8000, Loss: 0.1231057196855545, Accuracy: 1.0\n",
            "  Batch 3036/8000, Loss: 0.3274292051792145, Accuracy: 1.0\n",
            "  Batch 3037/8000, Loss: 0.5214778780937195, Accuracy: 1.0\n",
            "  Batch 3038/8000, Loss: 0.3766937851905823, Accuracy: 1.0\n",
            "  Batch 3039/8000, Loss: 0.5132054090499878, Accuracy: 1.0\n",
            "  Batch 3040/8000, Loss: 0.12474000453948975, Accuracy: 1.0\n",
            "  Batch 3041/8000, Loss: 0.287943959236145, Accuracy: 1.0\n",
            "  Batch 3042/8000, Loss: 0.2970094680786133, Accuracy: 1.0\n",
            "  Batch 3043/8000, Loss: 0.27773386240005493, Accuracy: 1.0\n",
            "  Batch 3044/8000, Loss: 0.41136813163757324, Accuracy: 1.0\n",
            "  Batch 3045/8000, Loss: 0.1267327517271042, Accuracy: 1.0\n",
            "  Batch 3046/8000, Loss: 1.3730641603469849, Accuracy: 0.0\n",
            "  Batch 3047/8000, Loss: 0.12759077548980713, Accuracy: 1.0\n",
            "  Batch 3048/8000, Loss: 0.2525397539138794, Accuracy: 1.0\n",
            "  Batch 3049/8000, Loss: 0.12340910732746124, Accuracy: 1.0\n",
            "  Batch 3050/8000, Loss: 0.12579430639743805, Accuracy: 1.0\n",
            "  Batch 3051/8000, Loss: 2.0967631340026855, Accuracy: 0.0\n",
            "  Batch 3052/8000, Loss: 1.303106665611267, Accuracy: 0.0\n",
            "  Batch 3053/8000, Loss: 0.12540701031684875, Accuracy: 1.0\n",
            "  Batch 3054/8000, Loss: 0.3343997299671173, Accuracy: 1.0\n",
            "  Batch 3055/8000, Loss: 0.5495554804801941, Accuracy: 1.0\n",
            "  Batch 3056/8000, Loss: 0.29832422733306885, Accuracy: 1.0\n",
            "  Batch 3057/8000, Loss: 0.12356626987457275, Accuracy: 1.0\n",
            "  Batch 3058/8000, Loss: 0.6621451377868652, Accuracy: 1.0\n",
            "  Batch 3059/8000, Loss: 0.12905390560626984, Accuracy: 1.0\n",
            "  Batch 3060/8000, Loss: 1.4399653673171997, Accuracy: 0.0\n",
            "  Batch 3061/8000, Loss: 2.1376798152923584, Accuracy: 0.0\n",
            "  Batch 3062/8000, Loss: 0.3813007175922394, Accuracy: 1.0\n",
            "  Batch 3063/8000, Loss: 0.4113343060016632, Accuracy: 1.0\n",
            "  Batch 3064/8000, Loss: 0.3159686028957367, Accuracy: 1.0\n",
            "  Batch 3065/8000, Loss: 0.12325375527143478, Accuracy: 1.0\n",
            "  Batch 3066/8000, Loss: 0.29331260919570923, Accuracy: 1.0\n",
            "  Batch 3067/8000, Loss: 0.527818500995636, Accuracy: 1.0\n",
            "  Batch 3068/8000, Loss: 0.6767964363098145, Accuracy: 1.0\n",
            "  Batch 3069/8000, Loss: 0.2971697151660919, Accuracy: 1.0\n",
            "  Batch 3070/8000, Loss: 0.5113309025764465, Accuracy: 1.0\n",
            "  Batch 3071/8000, Loss: 0.12314794957637787, Accuracy: 1.0\n",
            "  Batch 3072/8000, Loss: 0.3275793194770813, Accuracy: 1.0\n",
            "  Batch 3073/8000, Loss: 0.12282021343708038, Accuracy: 1.0\n",
            "  Batch 3074/8000, Loss: 0.5560160279273987, Accuracy: 1.0\n",
            "  Batch 3075/8000, Loss: 0.3106957972049713, Accuracy: 1.0\n",
            "  Batch 3076/8000, Loss: 0.5688201189041138, Accuracy: 1.0\n",
            "  Batch 3077/8000, Loss: 0.122784823179245, Accuracy: 1.0\n",
            "  Batch 3078/8000, Loss: 0.2974598705768585, Accuracy: 1.0\n",
            "  Batch 3079/8000, Loss: 0.5197219848632812, Accuracy: 1.0\n",
            "  Batch 3080/8000, Loss: 0.2614942789077759, Accuracy: 1.0\n",
            "  Batch 3081/8000, Loss: 0.12275859713554382, Accuracy: 1.0\n",
            "  Batch 3082/8000, Loss: 0.33160755038261414, Accuracy: 1.0\n",
            "  Batch 3083/8000, Loss: 0.26906827092170715, Accuracy: 1.0\n",
            "  Batch 3084/8000, Loss: 0.12276722490787506, Accuracy: 1.0\n",
            "  Batch 3085/8000, Loss: 0.12488265335559845, Accuracy: 1.0\n",
            "  Batch 3086/8000, Loss: 0.12272217869758606, Accuracy: 1.0\n",
            "  Batch 3087/8000, Loss: 0.317369282245636, Accuracy: 1.0\n",
            "  Batch 3088/8000, Loss: 0.29936379194259644, Accuracy: 1.0\n",
            "  Batch 3089/8000, Loss: 0.27087870240211487, Accuracy: 1.0\n",
            "  Batch 3090/8000, Loss: 2.2284605503082275, Accuracy: 0.0\n",
            "  Batch 3091/8000, Loss: 0.3770688772201538, Accuracy: 1.0\n",
            "  Batch 3092/8000, Loss: 0.12343072891235352, Accuracy: 1.0\n",
            "  Batch 3093/8000, Loss: 0.23790252208709717, Accuracy: 1.0\n",
            "  Batch 3094/8000, Loss: 2.0819172859191895, Accuracy: 0.0\n",
            "  Batch 3095/8000, Loss: 0.12265288084745407, Accuracy: 1.0\n",
            "  Batch 3096/8000, Loss: 0.26166021823883057, Accuracy: 1.0\n",
            "  Batch 3097/8000, Loss: 0.3382209837436676, Accuracy: 1.0\n",
            "  Batch 3098/8000, Loss: 0.24423804879188538, Accuracy: 1.0\n",
            "  Batch 3099/8000, Loss: 0.12372054159641266, Accuracy: 1.0\n",
            "  Batch 3100/8000, Loss: 1.0090036392211914, Accuracy: 0.0\n",
            "  Batch 3101/8000, Loss: 2.3369882106781006, Accuracy: 0.0\n",
            "  Batch 3102/8000, Loss: 0.25336775183677673, Accuracy: 1.0\n",
            "  Batch 3103/8000, Loss: 0.2746880054473877, Accuracy: 1.0\n",
            "  Batch 3104/8000, Loss: 1.246311902999878, Accuracy: 0.0\n",
            "  Batch 3105/8000, Loss: 0.4249754548072815, Accuracy: 1.0\n",
            "  Batch 3106/8000, Loss: 0.12437736988067627, Accuracy: 1.0\n",
            "  Batch 3107/8000, Loss: 1.2131208181381226, Accuracy: 0.0\n",
            "  Batch 3108/8000, Loss: 0.2888273596763611, Accuracy: 1.0\n",
            "  Batch 3109/8000, Loss: 0.40514659881591797, Accuracy: 1.0\n",
            "  Batch 3110/8000, Loss: 0.2993074655532837, Accuracy: 1.0\n",
            "  Batch 3111/8000, Loss: 0.292407751083374, Accuracy: 1.0\n",
            "  Batch 3112/8000, Loss: 0.3643486797809601, Accuracy: 1.0\n",
            "  Batch 3113/8000, Loss: 1.98932945728302, Accuracy: 0.0\n",
            "  Batch 3114/8000, Loss: 2.0877127647399902, Accuracy: 0.0\n",
            "  Batch 3115/8000, Loss: 0.3245390057563782, Accuracy: 1.0\n",
            "  Batch 3116/8000, Loss: 0.41329240798950195, Accuracy: 1.0\n",
            "  Batch 3117/8000, Loss: 0.12247596681118011, Accuracy: 1.0\n",
            "  Batch 3118/8000, Loss: 1.0886694192886353, Accuracy: 0.0\n",
            "  Batch 3119/8000, Loss: 0.12523768842220306, Accuracy: 1.0\n",
            "  Batch 3120/8000, Loss: 0.1224668100476265, Accuracy: 1.0\n",
            "  Batch 3121/8000, Loss: 0.41130632162094116, Accuracy: 1.0\n",
            "  Batch 3122/8000, Loss: 0.6607008576393127, Accuracy: 1.0\n",
            "  Batch 3123/8000, Loss: 0.32839202880859375, Accuracy: 1.0\n",
            "  Batch 3124/8000, Loss: 0.3555043339729309, Accuracy: 1.0\n",
            "  Batch 3125/8000, Loss: 0.34546953439712524, Accuracy: 1.0\n",
            "  Batch 3126/8000, Loss: 1.3507144451141357, Accuracy: 0.0\n",
            "  Batch 3127/8000, Loss: 0.5039548873901367, Accuracy: 1.0\n",
            "  Batch 3128/8000, Loss: 0.12254707515239716, Accuracy: 1.0\n",
            "  Batch 3129/8000, Loss: 0.12249588966369629, Accuracy: 1.0\n",
            "  Batch 3130/8000, Loss: 0.312126487493515, Accuracy: 1.0\n",
            "  Batch 3131/8000, Loss: 0.12318114936351776, Accuracy: 1.0\n",
            "  Batch 3132/8000, Loss: 0.47569113969802856, Accuracy: 1.0\n",
            "  Batch 3133/8000, Loss: 0.529468297958374, Accuracy: 1.0\n",
            "  Batch 3134/8000, Loss: 0.12238573282957077, Accuracy: 1.0\n",
            "  Batch 3135/8000, Loss: 0.4749968349933624, Accuracy: 1.0\n",
            "  Batch 3136/8000, Loss: 0.12244462221860886, Accuracy: 1.0\n",
            "  Batch 3137/8000, Loss: 0.33452966809272766, Accuracy: 1.0\n",
            "  Batch 3138/8000, Loss: 0.3595001995563507, Accuracy: 1.0\n",
            "  Batch 3139/8000, Loss: 1.844823956489563, Accuracy: 0.0\n",
            "  Batch 3140/8000, Loss: 0.12345089018344879, Accuracy: 1.0\n",
            "  Batch 3141/8000, Loss: 0.6529037952423096, Accuracy: 1.0\n",
            "  Batch 3142/8000, Loss: 0.12242084741592407, Accuracy: 1.0\n",
            "  Batch 3143/8000, Loss: 0.45315441489219666, Accuracy: 1.0\n",
            "  Batch 3144/8000, Loss: 0.43541812896728516, Accuracy: 1.0\n",
            "  Batch 3145/8000, Loss: 0.5450150370597839, Accuracy: 1.0\n",
            "  Batch 3146/8000, Loss: 0.12254508584737778, Accuracy: 1.0\n",
            "  Batch 3147/8000, Loss: 0.4270898997783661, Accuracy: 1.0\n",
            "  Batch 3148/8000, Loss: 0.3806731104850769, Accuracy: 1.0\n",
            "  Batch 3149/8000, Loss: 0.34793245792388916, Accuracy: 1.0\n",
            "  Batch 3150/8000, Loss: 2.017792224884033, Accuracy: 0.0\n",
            "  Batch 3151/8000, Loss: 0.439170777797699, Accuracy: 1.0\n",
            "  Batch 3152/8000, Loss: 1.1869633197784424, Accuracy: 0.0\n",
            "  Batch 3153/8000, Loss: 1.4235527515411377, Accuracy: 0.0\n",
            "  Batch 3154/8000, Loss: 0.3002736568450928, Accuracy: 1.0\n",
            "  Batch 3155/8000, Loss: 0.33365538716316223, Accuracy: 1.0\n",
            "  Batch 3156/8000, Loss: 1.4293125867843628, Accuracy: 0.0\n",
            "  Batch 3157/8000, Loss: 1.9066109657287598, Accuracy: 0.0\n",
            "  Batch 3158/8000, Loss: 0.6461908221244812, Accuracy: 1.0\n",
            "  Batch 3159/8000, Loss: 1.9223477840423584, Accuracy: 0.0\n",
            "  Batch 3160/8000, Loss: 1.6547431945800781, Accuracy: 0.0\n",
            "  Batch 3161/8000, Loss: 1.3913257122039795, Accuracy: 0.0\n",
            "  Batch 3162/8000, Loss: 0.33393779397010803, Accuracy: 1.0\n",
            "  Batch 3163/8000, Loss: 0.6825927495956421, Accuracy: 1.0\n",
            "  Batch 3164/8000, Loss: 0.6780792474746704, Accuracy: 1.0\n",
            "  Batch 3165/8000, Loss: 0.12303738296031952, Accuracy: 1.0\n",
            "  Batch 3166/8000, Loss: 0.9018534421920776, Accuracy: 0.0\n",
            "  Batch 3167/8000, Loss: 0.38138774037361145, Accuracy: 1.0\n",
            "  Batch 3168/8000, Loss: 0.12218540906906128, Accuracy: 1.0\n",
            "  Batch 3169/8000, Loss: 1.464476227760315, Accuracy: 0.0\n",
            "  Batch 3170/8000, Loss: 0.12347924709320068, Accuracy: 1.0\n",
            "  Batch 3171/8000, Loss: 0.8474565744400024, Accuracy: 0.0\n",
            "  Batch 3172/8000, Loss: 0.4041818678379059, Accuracy: 1.0\n",
            "  Batch 3173/8000, Loss: 1.383759617805481, Accuracy: 0.0\n",
            "  Batch 3174/8000, Loss: 0.5831435322761536, Accuracy: 1.0\n",
            "  Batch 3175/8000, Loss: 0.12202677875757217, Accuracy: 1.0\n",
            "  Batch 3176/8000, Loss: 0.1219908595085144, Accuracy: 1.0\n",
            "  Batch 3177/8000, Loss: 0.12212276458740234, Accuracy: 1.0\n",
            "  Batch 3178/8000, Loss: 0.7036270499229431, Accuracy: 1.0\n",
            "  Batch 3179/8000, Loss: 0.4584474563598633, Accuracy: 1.0\n",
            "  Batch 3180/8000, Loss: 0.45140448212623596, Accuracy: 1.0\n",
            "  Batch 3181/8000, Loss: 1.4730113744735718, Accuracy: 0.0\n",
            "  Batch 3182/8000, Loss: 0.43593159317970276, Accuracy: 1.0\n",
            "  Batch 3183/8000, Loss: 0.5295764207839966, Accuracy: 1.0\n",
            "  Batch 3184/8000, Loss: 0.4784846007823944, Accuracy: 1.0\n",
            "  Batch 3185/8000, Loss: 0.6901143789291382, Accuracy: 1.0\n",
            "  Batch 3186/8000, Loss: 1.0552458763122559, Accuracy: 0.0\n",
            "  Batch 3187/8000, Loss: 0.5491698980331421, Accuracy: 1.0\n",
            "  Batch 3188/8000, Loss: 0.6785272359848022, Accuracy: 1.0\n",
            "  Batch 3189/8000, Loss: 0.12188504636287689, Accuracy: 1.0\n",
            "  Batch 3190/8000, Loss: 0.12217966467142105, Accuracy: 1.0\n",
            "  Batch 3191/8000, Loss: 0.5987439155578613, Accuracy: 1.0\n",
            "  Batch 3192/8000, Loss: 0.12219256162643433, Accuracy: 1.0\n",
            "  Batch 3193/8000, Loss: 0.12185385823249817, Accuracy: 1.0\n",
            "  Batch 3194/8000, Loss: 0.8315199613571167, Accuracy: 0.0\n",
            "  Batch 3195/8000, Loss: 0.42887523770332336, Accuracy: 1.0\n",
            "  Batch 3196/8000, Loss: 0.12190651893615723, Accuracy: 1.0\n",
            "  Batch 3197/8000, Loss: 0.5877066850662231, Accuracy: 1.0\n",
            "  Batch 3198/8000, Loss: 0.6474259495735168, Accuracy: 1.0\n",
            "  Batch 3199/8000, Loss: 0.963154137134552, Accuracy: 0.0\n",
            "  Batch 3200/8000, Loss: 0.6382491588592529, Accuracy: 1.0\n",
            "  Batch 3201/8000, Loss: 0.43495696783065796, Accuracy: 1.0\n",
            "  Batch 3202/8000, Loss: 0.41033825278282166, Accuracy: 1.0\n",
            "  Batch 3203/8000, Loss: 0.5800054669380188, Accuracy: 1.0\n",
            "  Batch 3204/8000, Loss: 0.12206347286701202, Accuracy: 1.0\n",
            "  Batch 3205/8000, Loss: 0.5763185620307922, Accuracy: 1.0\n",
            "  Batch 3206/8000, Loss: 1.4182322025299072, Accuracy: 0.0\n",
            "  Batch 3207/8000, Loss: 0.5175009369850159, Accuracy: 1.0\n",
            "  Batch 3208/8000, Loss: 0.12240049988031387, Accuracy: 1.0\n",
            "  Batch 3209/8000, Loss: 1.170069694519043, Accuracy: 0.0\n",
            "  Batch 3210/8000, Loss: 0.4156338572502136, Accuracy: 1.0\n",
            "  Batch 3211/8000, Loss: 0.12307862937450409, Accuracy: 1.0\n",
            "  Batch 3212/8000, Loss: 1.5952922105789185, Accuracy: 0.0\n",
            "  Batch 3213/8000, Loss: 0.5326434373855591, Accuracy: 1.0\n",
            "  Batch 3214/8000, Loss: 7.672211170196533, Accuracy: 0.0\n",
            "  Batch 3215/8000, Loss: 0.4440406858921051, Accuracy: 1.0\n",
            "  Batch 3216/8000, Loss: 0.5958507061004639, Accuracy: 1.0\n",
            "  Batch 3217/8000, Loss: 0.44663217663764954, Accuracy: 1.0\n",
            "  Batch 3218/8000, Loss: 0.5223233103752136, Accuracy: 1.0\n",
            "  Batch 3219/8000, Loss: 0.4483683705329895, Accuracy: 1.0\n",
            "  Batch 3220/8000, Loss: 0.6918134093284607, Accuracy: 1.0\n",
            "  Batch 3221/8000, Loss: 0.40781641006469727, Accuracy: 1.0\n",
            "  Batch 3222/8000, Loss: 0.505180835723877, Accuracy: 1.0\n",
            "  Batch 3223/8000, Loss: 1.5191372632980347, Accuracy: 0.0\n",
            "  Batch 3224/8000, Loss: 0.12650063633918762, Accuracy: 1.0\n",
            "  Batch 3225/8000, Loss: 1.6408562660217285, Accuracy: 0.0\n",
            "  Batch 3226/8000, Loss: 0.4722728133201599, Accuracy: 1.0\n",
            "  Batch 3227/8000, Loss: 0.12414171546697617, Accuracy: 1.0\n",
            "  Batch 3228/8000, Loss: 0.14969348907470703, Accuracy: 1.0\n",
            "  Batch 3229/8000, Loss: 1.2354265451431274, Accuracy: 0.0\n",
            "  Batch 3230/8000, Loss: 0.49081578850746155, Accuracy: 1.0\n",
            "  Batch 3231/8000, Loss: 0.1431085169315338, Accuracy: 1.0\n",
            "  Batch 3232/8000, Loss: 0.4767371416091919, Accuracy: 1.0\n",
            "  Batch 3233/8000, Loss: 1.5327520370483398, Accuracy: 0.0\n",
            "  Batch 3234/8000, Loss: 0.14045965671539307, Accuracy: 1.0\n",
            "  Batch 3235/8000, Loss: 0.12164910137653351, Accuracy: 1.0\n",
            "  Batch 3236/8000, Loss: 1.1609654426574707, Accuracy: 0.0\n",
            "  Batch 3237/8000, Loss: 1.1844958066940308, Accuracy: 0.0\n",
            "  Batch 3238/8000, Loss: 0.12168951332569122, Accuracy: 1.0\n",
            "  Batch 3239/8000, Loss: 0.40549954771995544, Accuracy: 1.0\n",
            "  Batch 3240/8000, Loss: 0.42236265540122986, Accuracy: 1.0\n",
            "  Batch 3241/8000, Loss: 0.1385454386472702, Accuracy: 1.0\n",
            "  Batch 3242/8000, Loss: 0.1308193802833557, Accuracy: 1.0\n",
            "  Batch 3243/8000, Loss: 0.41211259365081787, Accuracy: 1.0\n",
            "  Batch 3244/8000, Loss: 1.004921793937683, Accuracy: 0.0\n",
            "  Batch 3245/8000, Loss: 0.5289430618286133, Accuracy: 1.0\n",
            "  Batch 3246/8000, Loss: 0.4436608850955963, Accuracy: 1.0\n",
            "  Batch 3247/8000, Loss: 0.8535733222961426, Accuracy: 0.0\n",
            "  Batch 3248/8000, Loss: 0.12137309461832047, Accuracy: 1.0\n",
            "  Batch 3249/8000, Loss: 0.2236184924840927, Accuracy: 1.0\n",
            "  Batch 3250/8000, Loss: 0.6100500226020813, Accuracy: 1.0\n",
            "  Batch 3251/8000, Loss: 0.37437203526496887, Accuracy: 1.0\n",
            "  Batch 3252/8000, Loss: 0.12157277017831802, Accuracy: 1.0\n",
            "  Batch 3253/8000, Loss: 1.0398931503295898, Accuracy: 0.0\n",
            "  Batch 3254/8000, Loss: 0.4173967242240906, Accuracy: 1.0\n",
            "  Batch 3255/8000, Loss: 0.8546590209007263, Accuracy: 0.0\n",
            "  Batch 3256/8000, Loss: 1.1711058616638184, Accuracy: 0.0\n",
            "  Batch 3257/8000, Loss: 0.4057933986186981, Accuracy: 1.0\n",
            "  Batch 3258/8000, Loss: 0.12857291102409363, Accuracy: 1.0\n",
            "  Batch 3259/8000, Loss: 0.4016718864440918, Accuracy: 1.0\n",
            "  Batch 3260/8000, Loss: 0.4124099612236023, Accuracy: 1.0\n",
            "  Batch 3261/8000, Loss: 1.4364908933639526, Accuracy: 0.0\n",
            "  Batch 3262/8000, Loss: 0.7196145057678223, Accuracy: 1.0\n",
            "  Batch 3263/8000, Loss: 0.5348618626594543, Accuracy: 1.0\n",
            "  Batch 3264/8000, Loss: 0.1293073445558548, Accuracy: 1.0\n",
            "  Batch 3265/8000, Loss: 0.6610142588615417, Accuracy: 1.0\n",
            "  Batch 3266/8000, Loss: 0.131771057844162, Accuracy: 1.0\n",
            "  Batch 3267/8000, Loss: 0.4059194326400757, Accuracy: 1.0\n",
            "  Batch 3268/8000, Loss: 0.9513680338859558, Accuracy: 0.0\n",
            "  Batch 3269/8000, Loss: 0.1647643893957138, Accuracy: 1.0\n",
            "  Batch 3270/8000, Loss: 0.9999041557312012, Accuracy: 0.0\n",
            "  Batch 3271/8000, Loss: 1.680184006690979, Accuracy: 0.0\n",
            "  Batch 3272/8000, Loss: 0.12154874205589294, Accuracy: 1.0\n",
            "  Batch 3273/8000, Loss: 0.5953760147094727, Accuracy: 1.0\n",
            "  Batch 3274/8000, Loss: 0.581866443157196, Accuracy: 1.0\n",
            "  Batch 3275/8000, Loss: 1.179922342300415, Accuracy: 0.0\n",
            "  Batch 3276/8000, Loss: 0.15544678270816803, Accuracy: 1.0\n",
            "  Batch 3277/8000, Loss: 0.9065099954605103, Accuracy: 0.0\n",
            "  Batch 3278/8000, Loss: 0.40601903200149536, Accuracy: 1.0\n",
            "  Batch 3279/8000, Loss: 1.1118555068969727, Accuracy: 0.0\n",
            "  Batch 3280/8000, Loss: 1.3925330638885498, Accuracy: 0.0\n",
            "  Batch 3281/8000, Loss: 1.1087757349014282, Accuracy: 0.0\n",
            "  Batch 3282/8000, Loss: 0.12112414836883545, Accuracy: 1.0\n",
            "  Batch 3283/8000, Loss: 0.1267361044883728, Accuracy: 1.0\n",
            "  Batch 3284/8000, Loss: 0.1299419403076172, Accuracy: 1.0\n",
            "  Batch 3285/8000, Loss: 0.1267710030078888, Accuracy: 1.0\n",
            "  Batch 3286/8000, Loss: 0.5702720880508423, Accuracy: 1.0\n",
            "  Batch 3287/8000, Loss: 0.9249579906463623, Accuracy: 0.0\n",
            "  Batch 3288/8000, Loss: 0.4915274381637573, Accuracy: 1.0\n",
            "  Batch 3289/8000, Loss: 0.8949099183082581, Accuracy: 0.0\n",
            "  Batch 3290/8000, Loss: 0.4611652195453644, Accuracy: 1.0\n",
            "  Batch 3291/8000, Loss: 1.4401143789291382, Accuracy: 0.0\n",
            "  Batch 3292/8000, Loss: 0.5823419094085693, Accuracy: 1.0\n",
            "  Batch 3293/8000, Loss: 0.7700648903846741, Accuracy: 1.0\n",
            "  Batch 3294/8000, Loss: 0.45562899112701416, Accuracy: 1.0\n",
            "  Batch 3295/8000, Loss: 0.9962835907936096, Accuracy: 0.0\n",
            "  Batch 3296/8000, Loss: 0.12104794383049011, Accuracy: 1.0\n",
            "  Batch 3297/8000, Loss: 0.824725329875946, Accuracy: 0.0\n",
            "  Batch 3298/8000, Loss: 0.24047154188156128, Accuracy: 1.0\n",
            "  Batch 3299/8000, Loss: 0.43993955850601196, Accuracy: 1.0\n",
            "  Batch 3300/8000, Loss: 0.9098529815673828, Accuracy: 0.0\n",
            "  Batch 3301/8000, Loss: 0.29542332887649536, Accuracy: 1.0\n",
            "  Batch 3302/8000, Loss: 0.8550711274147034, Accuracy: 0.0\n",
            "  Batch 3303/8000, Loss: 1.1645996570587158, Accuracy: 0.0\n",
            "  Batch 3304/8000, Loss: 0.5352399945259094, Accuracy: 1.0\n",
            "  Batch 3305/8000, Loss: 0.4390907287597656, Accuracy: 1.0\n",
            "  Batch 3306/8000, Loss: 0.1715879589319229, Accuracy: 1.0\n",
            "  Batch 3307/8000, Loss: 0.9034271836280823, Accuracy: 0.0\n",
            "  Batch 3308/8000, Loss: 0.8173702359199524, Accuracy: 0.0\n",
            "  Batch 3309/8000, Loss: 0.6271904706954956, Accuracy: 1.0\n",
            "  Batch 3310/8000, Loss: 0.8233542442321777, Accuracy: 0.0\n",
            "  Batch 3311/8000, Loss: 0.4986036717891693, Accuracy: 1.0\n",
            "  Batch 3312/8000, Loss: 1.0661225318908691, Accuracy: 0.0\n",
            "  Batch 3313/8000, Loss: 0.46602874994277954, Accuracy: 1.0\n",
            "  Batch 3314/8000, Loss: 0.5408210754394531, Accuracy: 1.0\n",
            "  Batch 3315/8000, Loss: 0.9269870519638062, Accuracy: 0.0\n",
            "  Batch 3316/8000, Loss: 0.16033796966075897, Accuracy: 1.0\n",
            "  Batch 3317/8000, Loss: 0.7425565719604492, Accuracy: 1.0\n",
            "  Batch 3318/8000, Loss: 0.9287688136100769, Accuracy: 0.0\n",
            "  Batch 3319/8000, Loss: 0.5286564230918884, Accuracy: 1.0\n",
            "  Batch 3320/8000, Loss: 0.14483167231082916, Accuracy: 1.0\n",
            "  Batch 3321/8000, Loss: 1.1688505411148071, Accuracy: 0.0\n",
            "  Batch 3322/8000, Loss: 1.2047566175460815, Accuracy: 0.0\n",
            "  Batch 3323/8000, Loss: 0.5235350728034973, Accuracy: 1.0\n",
            "  Batch 3324/8000, Loss: 0.798113226890564, Accuracy: 1.0\n",
            "  Batch 3325/8000, Loss: 0.4625183641910553, Accuracy: 1.0\n",
            "  Batch 3326/8000, Loss: 0.12336480617523193, Accuracy: 1.0\n",
            "  Batch 3327/8000, Loss: 0.13557803630828857, Accuracy: 1.0\n",
            "  Batch 3328/8000, Loss: 0.9480977058410645, Accuracy: 0.0\n",
            "  Batch 3329/8000, Loss: 0.12632255256175995, Accuracy: 1.0\n",
            "  Batch 3330/8000, Loss: 0.12080690264701843, Accuracy: 1.0\n",
            "  Batch 3331/8000, Loss: 0.12077751755714417, Accuracy: 1.0\n",
            "  Batch 3332/8000, Loss: 0.1207931861281395, Accuracy: 1.0\n",
            "  Batch 3333/8000, Loss: 0.1256893277168274, Accuracy: 1.0\n",
            "  Batch 3334/8000, Loss: 0.5095199346542358, Accuracy: 1.0\n",
            "  Batch 3335/8000, Loss: 1.1698448657989502, Accuracy: 0.0\n",
            "  Batch 3336/8000, Loss: 0.7531166076660156, Accuracy: 1.0\n",
            "  Batch 3337/8000, Loss: 0.13460630178451538, Accuracy: 1.0\n",
            "  Batch 3338/8000, Loss: 0.12370017170906067, Accuracy: 1.0\n",
            "  Batch 3339/8000, Loss: 0.4833252429962158, Accuracy: 1.0\n",
            "  Batch 3340/8000, Loss: 0.12635886669158936, Accuracy: 1.0\n",
            "  Batch 3341/8000, Loss: 0.8535538911819458, Accuracy: 0.0\n",
            "  Batch 3342/8000, Loss: 1.3490900993347168, Accuracy: 0.0\n",
            "  Batch 3343/8000, Loss: 0.820857584476471, Accuracy: 0.0\n",
            "  Batch 3344/8000, Loss: 1.3247807025909424, Accuracy: 0.0\n",
            "  Batch 3345/8000, Loss: 0.15368519723415375, Accuracy: 1.0\n",
            "  Batch 3346/8000, Loss: 0.12066641449928284, Accuracy: 1.0\n",
            "  Batch 3347/8000, Loss: 0.1366422027349472, Accuracy: 1.0\n",
            "  Batch 3348/8000, Loss: 0.5221773982048035, Accuracy: 1.0\n",
            "  Batch 3349/8000, Loss: 0.12068590521812439, Accuracy: 1.0\n",
            "  Batch 3350/8000, Loss: 0.48078539967536926, Accuracy: 1.0\n",
            "  Batch 3351/8000, Loss: 0.1347697675228119, Accuracy: 1.0\n",
            "  Batch 3352/8000, Loss: 0.4875119924545288, Accuracy: 1.0\n",
            "  Batch 3353/8000, Loss: 0.1227966696023941, Accuracy: 1.0\n",
            "  Batch 3354/8000, Loss: 0.46367770433425903, Accuracy: 1.0\n",
            "  Batch 3355/8000, Loss: 0.12473709881305695, Accuracy: 1.0\n",
            "  Batch 3356/8000, Loss: 0.4829736351966858, Accuracy: 1.0\n",
            "  Batch 3357/8000, Loss: 0.5036170482635498, Accuracy: 1.0\n",
            "  Batch 3358/8000, Loss: 0.8383877277374268, Accuracy: 0.0\n",
            "  Batch 3359/8000, Loss: 0.12058818340301514, Accuracy: 1.0\n",
            "  Batch 3360/8000, Loss: 1.3214207887649536, Accuracy: 0.0\n",
            "  Batch 3361/8000, Loss: 0.48531481623649597, Accuracy: 1.0\n",
            "  Batch 3362/8000, Loss: 0.5541980266571045, Accuracy: 1.0\n",
            "  Batch 3363/8000, Loss: 0.12444514036178589, Accuracy: 1.0\n",
            "  Batch 3364/8000, Loss: 0.513853907585144, Accuracy: 1.0\n",
            "  Batch 3365/8000, Loss: 0.12808799743652344, Accuracy: 1.0\n",
            "  Batch 3366/8000, Loss: 0.14200234413146973, Accuracy: 1.0\n",
            "  Batch 3367/8000, Loss: 0.6840561628341675, Accuracy: 1.0\n",
            "  Batch 3368/8000, Loss: 0.8689599633216858, Accuracy: 0.0\n",
            "  Batch 3369/8000, Loss: 0.754814863204956, Accuracy: 1.0\n",
            "  Batch 3370/8000, Loss: 0.1227712631225586, Accuracy: 1.0\n",
            "  Batch 3371/8000, Loss: 0.6976004838943481, Accuracy: 1.0\n",
            "  Batch 3372/8000, Loss: 0.12055051326751709, Accuracy: 1.0\n",
            "  Batch 3373/8000, Loss: 0.1205042153596878, Accuracy: 1.0\n",
            "  Batch 3374/8000, Loss: 0.5637195110321045, Accuracy: 1.0\n",
            "  Batch 3375/8000, Loss: 0.403816819190979, Accuracy: 1.0\n",
            "  Batch 3376/8000, Loss: 0.1363047957420349, Accuracy: 1.0\n",
            "  Batch 3377/8000, Loss: 0.4393751621246338, Accuracy: 1.0\n",
            "  Batch 3378/8000, Loss: 1.4389780759811401, Accuracy: 0.0\n",
            "  Batch 3379/8000, Loss: 0.506514310836792, Accuracy: 1.0\n",
            "  Batch 3380/8000, Loss: 0.12295709550380707, Accuracy: 1.0\n",
            "  Batch 3381/8000, Loss: 0.6283627152442932, Accuracy: 1.0\n",
            "  Batch 3382/8000, Loss: 0.12232846021652222, Accuracy: 1.0\n",
            "  Batch 3383/8000, Loss: 0.9490483403205872, Accuracy: 0.0\n",
            "  Batch 3384/8000, Loss: 0.7120761275291443, Accuracy: 1.0\n",
            "  Batch 3385/8000, Loss: 0.36857831478118896, Accuracy: 1.0\n",
            "  Batch 3386/8000, Loss: 0.36062169075012207, Accuracy: 1.0\n",
            "  Batch 3387/8000, Loss: 0.14067316055297852, Accuracy: 1.0\n",
            "  Batch 3388/8000, Loss: 0.7515835165977478, Accuracy: 1.0\n",
            "  Batch 3389/8000, Loss: 4.563300609588623, Accuracy: 0.0\n",
            "  Batch 3390/8000, Loss: 0.6876983046531677, Accuracy: 1.0\n",
            "  Batch 3391/8000, Loss: 0.12036890536546707, Accuracy: 1.0\n",
            "  Batch 3392/8000, Loss: 0.12456858158111572, Accuracy: 1.0\n",
            "  Batch 3393/8000, Loss: 0.12776727974414825, Accuracy: 1.0\n",
            "  Batch 3394/8000, Loss: 0.8741894960403442, Accuracy: 0.0\n",
            "  Batch 3395/8000, Loss: 0.33183056116104126, Accuracy: 1.0\n",
            "  Batch 3396/8000, Loss: 0.12805043160915375, Accuracy: 1.0\n",
            "  Batch 3397/8000, Loss: 0.3550792634487152, Accuracy: 1.0\n",
            "  Batch 3398/8000, Loss: 0.4673847556114197, Accuracy: 1.0\n",
            "  Batch 3399/8000, Loss: 0.13067804276943207, Accuracy: 1.0\n",
            "  Batch 3400/8000, Loss: 0.3557243347167969, Accuracy: 1.0\n",
            "  Batch 3401/8000, Loss: 0.35326457023620605, Accuracy: 1.0\n",
            "  Batch 3402/8000, Loss: 0.12073056399822235, Accuracy: 1.0\n",
            "  Batch 3403/8000, Loss: 0.3564886152744293, Accuracy: 1.0\n",
            "  Batch 3404/8000, Loss: 0.12893171608448029, Accuracy: 1.0\n",
            "  Batch 3405/8000, Loss: 0.5787970423698425, Accuracy: 1.0\n",
            "  Batch 3406/8000, Loss: 0.1541554033756256, Accuracy: 1.0\n",
            "  Batch 3407/8000, Loss: 1.2739975452423096, Accuracy: 0.0\n",
            "  Batch 3408/8000, Loss: 0.12041883915662766, Accuracy: 1.0\n",
            "  Batch 3409/8000, Loss: 0.42264118790626526, Accuracy: 1.0\n",
            "  Batch 3410/8000, Loss: 0.3771306872367859, Accuracy: 1.0\n",
            "  Batch 3411/8000, Loss: 0.31910356879234314, Accuracy: 1.0\n",
            "  Batch 3412/8000, Loss: 0.4939057528972626, Accuracy: 1.0\n",
            "  Batch 3413/8000, Loss: 0.3810061812400818, Accuracy: 1.0\n",
            "  Batch 3414/8000, Loss: 0.33937376737594604, Accuracy: 1.0\n",
            "  Batch 3415/8000, Loss: 0.37963396310806274, Accuracy: 1.0\n",
            "  Batch 3416/8000, Loss: 0.23578859865665436, Accuracy: 1.0\n",
            "  Batch 3417/8000, Loss: 1.3503719568252563, Accuracy: 0.0\n",
            "  Batch 3418/8000, Loss: 0.1202772781252861, Accuracy: 1.0\n",
            "  Batch 3419/8000, Loss: 1.0355048179626465, Accuracy: 0.0\n",
            "  Batch 3420/8000, Loss: 0.42019903659820557, Accuracy: 1.0\n",
            "  Batch 3421/8000, Loss: 0.26494932174682617, Accuracy: 1.0\n",
            "  Batch 3422/8000, Loss: 1.241694450378418, Accuracy: 0.0\n",
            "  Batch 3423/8000, Loss: 1.0518858432769775, Accuracy: 0.0\n",
            "  Batch 3424/8000, Loss: 0.2850821316242218, Accuracy: 1.0\n",
            "  Batch 3425/8000, Loss: 1.2371175289154053, Accuracy: 0.0\n",
            "  Batch 3426/8000, Loss: 0.31990331411361694, Accuracy: 1.0\n",
            "  Batch 3427/8000, Loss: 0.3128351867198944, Accuracy: 1.0\n",
            "  Batch 3428/8000, Loss: 0.8866375088691711, Accuracy: 0.0\n",
            "  Batch 3429/8000, Loss: 0.3449072241783142, Accuracy: 1.0\n",
            "  Batch 3430/8000, Loss: 1.8806248903274536, Accuracy: 0.0\n",
            "  Batch 3431/8000, Loss: 0.32179635763168335, Accuracy: 1.0\n",
            "  Batch 3432/8000, Loss: 0.8873334527015686, Accuracy: 0.0\n",
            "  Batch 3433/8000, Loss: 0.1427399218082428, Accuracy: 1.0\n",
            "  Batch 3434/8000, Loss: 0.3150253891944885, Accuracy: 1.0\n",
            "  Batch 3435/8000, Loss: 0.91895592212677, Accuracy: 0.0\n",
            "  Batch 3436/8000, Loss: 0.12714706361293793, Accuracy: 1.0\n",
            "  Batch 3437/8000, Loss: 0.12572598457336426, Accuracy: 1.0\n",
            "  Batch 3438/8000, Loss: 0.12683826684951782, Accuracy: 1.0\n",
            "  Batch 3439/8000, Loss: 0.13269537687301636, Accuracy: 1.0\n",
            "  Batch 3440/8000, Loss: 0.9370720982551575, Accuracy: 0.0\n",
            "  Batch 3441/8000, Loss: 0.7974798679351807, Accuracy: 1.0\n",
            "  Batch 3442/8000, Loss: 1.4191739559173584, Accuracy: 0.0\n",
            "  Batch 3443/8000, Loss: 0.4501165449619293, Accuracy: 1.0\n",
            "  Batch 3444/8000, Loss: 0.3993525803089142, Accuracy: 1.0\n",
            "  Batch 3445/8000, Loss: 0.12286533415317535, Accuracy: 1.0\n",
            "  Batch 3446/8000, Loss: 0.7703897356987, Accuracy: 1.0\n",
            "  Batch 3447/8000, Loss: 0.1201496422290802, Accuracy: 1.0\n",
            "  Batch 3448/8000, Loss: 0.36201587319374084, Accuracy: 1.0\n",
            "  Batch 3449/8000, Loss: 0.6189973950386047, Accuracy: 1.0\n",
            "  Batch 3450/8000, Loss: 0.12016531825065613, Accuracy: 1.0\n",
            "  Batch 3451/8000, Loss: 0.386412113904953, Accuracy: 1.0\n",
            "  Batch 3452/8000, Loss: 1.7434313297271729, Accuracy: 0.0\n",
            "  Batch 3453/8000, Loss: 0.4944695234298706, Accuracy: 1.0\n",
            "  Batch 3454/8000, Loss: 0.4018077850341797, Accuracy: 1.0\n",
            "  Batch 3455/8000, Loss: 0.12002679705619812, Accuracy: 1.0\n",
            "  Batch 3456/8000, Loss: 0.8611726760864258, Accuracy: 0.0\n",
            "  Batch 3457/8000, Loss: 0.36117345094680786, Accuracy: 1.0\n",
            "  Batch 3458/8000, Loss: 0.38636693358421326, Accuracy: 1.0\n",
            "  Batch 3459/8000, Loss: 1.5377262830734253, Accuracy: 0.0\n",
            "  Batch 3460/8000, Loss: 0.7623313069343567, Accuracy: 1.0\n",
            "  Batch 3461/8000, Loss: 0.8354629278182983, Accuracy: 0.0\n",
            "  Batch 3462/8000, Loss: 1.580899715423584, Accuracy: 0.0\n",
            "  Batch 3463/8000, Loss: 0.9271917939186096, Accuracy: 0.0\n",
            "  Batch 3464/8000, Loss: 0.7465967535972595, Accuracy: 1.0\n",
            "  Batch 3465/8000, Loss: 0.38751086592674255, Accuracy: 1.0\n",
            "  Batch 3466/8000, Loss: 0.4826519787311554, Accuracy: 1.0\n",
            "  Batch 3467/8000, Loss: 0.12008380144834518, Accuracy: 1.0\n",
            "  Batch 3468/8000, Loss: 0.753515362739563, Accuracy: 1.0\n",
            "  Batch 3469/8000, Loss: 0.9125489592552185, Accuracy: 0.0\n",
            "  Batch 3470/8000, Loss: 0.12517322599887848, Accuracy: 1.0\n",
            "  Batch 3471/8000, Loss: 0.48634326457977295, Accuracy: 1.0\n",
            "  Batch 3472/8000, Loss: 0.20246125757694244, Accuracy: 1.0\n",
            "  Batch 3473/8000, Loss: 0.12318632006645203, Accuracy: 1.0\n",
            "  Batch 3474/8000, Loss: 0.4154531955718994, Accuracy: 1.0\n",
            "  Batch 3475/8000, Loss: 0.36676719784736633, Accuracy: 1.0\n",
            "  Batch 3476/8000, Loss: 0.3684054911136627, Accuracy: 1.0\n",
            "  Batch 3477/8000, Loss: 0.1316787600517273, Accuracy: 1.0\n",
            "  Batch 3478/8000, Loss: 0.5515783429145813, Accuracy: 1.0\n",
            "  Batch 3479/8000, Loss: 0.4912514388561249, Accuracy: 1.0\n",
            "  Batch 3480/8000, Loss: 1.7729747295379639, Accuracy: 0.0\n",
            "  Batch 3481/8000, Loss: 0.3578920066356659, Accuracy: 1.0\n",
            "  Batch 3482/8000, Loss: 0.3372035026550293, Accuracy: 1.0\n",
            "  Batch 3483/8000, Loss: 0.5438919067382812, Accuracy: 1.0\n",
            "  Batch 3484/8000, Loss: 1.6220085620880127, Accuracy: 0.0\n",
            "  Batch 3485/8000, Loss: 0.12450119853019714, Accuracy: 1.0\n",
            "  Batch 3486/8000, Loss: 0.3494424819946289, Accuracy: 1.0\n",
            "  Batch 3487/8000, Loss: 0.12265858054161072, Accuracy: 1.0\n",
            "  Batch 3488/8000, Loss: 0.12230555713176727, Accuracy: 1.0\n",
            "  Batch 3489/8000, Loss: 0.3442748785018921, Accuracy: 1.0\n",
            "  Batch 3490/8000, Loss: 0.9591030478477478, Accuracy: 0.0\n",
            "  Batch 3491/8000, Loss: 0.13322050869464874, Accuracy: 1.0\n",
            "  Batch 3492/8000, Loss: 0.11979685723781586, Accuracy: 1.0\n",
            "  Batch 3493/8000, Loss: 0.34970220923423767, Accuracy: 1.0\n",
            "  Batch 3494/8000, Loss: 0.12689736485481262, Accuracy: 1.0\n",
            "  Batch 3495/8000, Loss: 1.2331198453903198, Accuracy: 0.0\n",
            "  Batch 3496/8000, Loss: 0.5295116305351257, Accuracy: 1.0\n",
            "  Batch 3497/8000, Loss: 0.12291093170642853, Accuracy: 1.0\n",
            "  Batch 3498/8000, Loss: 0.7611603736877441, Accuracy: 1.0\n",
            "  Batch 3499/8000, Loss: 0.18665271997451782, Accuracy: 1.0\n",
            "  Batch 3500/8000, Loss: 0.8862434029579163, Accuracy: 0.0\n",
            "  Batch 3501/8000, Loss: 0.6390933394432068, Accuracy: 1.0\n",
            "  Batch 3502/8000, Loss: 1.617282509803772, Accuracy: 0.0\n",
            "  Batch 3503/8000, Loss: 0.37229931354522705, Accuracy: 1.0\n",
            "  Batch 3504/8000, Loss: 0.7387507557868958, Accuracy: 1.0\n",
            "  Batch 3505/8000, Loss: 0.7796880602836609, Accuracy: 1.0\n",
            "  Batch 3506/8000, Loss: 0.7121840715408325, Accuracy: 1.0\n",
            "  Batch 3507/8000, Loss: 0.654719352722168, Accuracy: 1.0\n",
            "  Batch 3508/8000, Loss: 0.37831830978393555, Accuracy: 1.0\n",
            "  Batch 3509/8000, Loss: 0.9097062349319458, Accuracy: 0.0\n",
            "  Batch 3510/8000, Loss: 0.35877445340156555, Accuracy: 1.0\n",
            "  Batch 3511/8000, Loss: 1.6316908597946167, Accuracy: 0.0\n",
            "  Batch 3512/8000, Loss: 1.5586868524551392, Accuracy: 0.0\n",
            "  Batch 3513/8000, Loss: 0.12141551077365875, Accuracy: 1.0\n",
            "  Batch 3514/8000, Loss: 0.11957225203514099, Accuracy: 1.0\n",
            "  Batch 3515/8000, Loss: 0.46543657779693604, Accuracy: 1.0\n",
            "  Batch 3516/8000, Loss: 0.11959242075681686, Accuracy: 1.0\n",
            "  Batch 3517/8000, Loss: 0.1243082582950592, Accuracy: 1.0\n",
            "  Batch 3518/8000, Loss: 0.8506565690040588, Accuracy: 0.0\n",
            "  Batch 3519/8000, Loss: 0.12275449186563492, Accuracy: 1.0\n",
            "  Batch 3520/8000, Loss: 0.34431546926498413, Accuracy: 1.0\n",
            "  Batch 3521/8000, Loss: 1.6929274797439575, Accuracy: 0.0\n",
            "  Batch 3522/8000, Loss: 0.5010913610458374, Accuracy: 1.0\n",
            "  Batch 3523/8000, Loss: 0.3887902498245239, Accuracy: 1.0\n",
            "  Batch 3524/8000, Loss: 1.5998746156692505, Accuracy: 0.0\n",
            "  Batch 3525/8000, Loss: 0.12066422402858734, Accuracy: 1.0\n",
            "  Batch 3526/8000, Loss: 0.3970014750957489, Accuracy: 1.0\n",
            "  Batch 3527/8000, Loss: 0.507995069026947, Accuracy: 1.0\n",
            "  Batch 3528/8000, Loss: 0.843384325504303, Accuracy: 0.0\n",
            "  Batch 3529/8000, Loss: 1.410575270652771, Accuracy: 0.0\n",
            "  Batch 3530/8000, Loss: 0.43174463510513306, Accuracy: 1.0\n",
            "  Batch 3531/8000, Loss: 0.4365698993206024, Accuracy: 1.0\n",
            "  Batch 3532/8000, Loss: 0.643248438835144, Accuracy: 1.0\n",
            "  Batch 3533/8000, Loss: 0.4082365334033966, Accuracy: 1.0\n",
            "  Batch 3534/8000, Loss: 0.12231620401144028, Accuracy: 1.0\n",
            "  Batch 3535/8000, Loss: 0.13346466422080994, Accuracy: 1.0\n",
            "  Batch 3536/8000, Loss: 0.4056721031665802, Accuracy: 1.0\n",
            "  Batch 3537/8000, Loss: 0.12143287062644958, Accuracy: 1.0\n",
            "  Batch 3538/8000, Loss: 0.4123060405254364, Accuracy: 1.0\n",
            "  Batch 3539/8000, Loss: 1.5039153099060059, Accuracy: 0.0\n",
            "  Batch 3540/8000, Loss: 0.12049928307533264, Accuracy: 1.0\n",
            "  Batch 3541/8000, Loss: 0.11942019313573837, Accuracy: 1.0\n",
            "  Batch 3542/8000, Loss: 0.8086562752723694, Accuracy: 1.0\n",
            "  Batch 3543/8000, Loss: 0.4578690230846405, Accuracy: 1.0\n",
            "  Batch 3544/8000, Loss: 0.1217145100235939, Accuracy: 1.0\n",
            "  Batch 3545/8000, Loss: 0.8095765113830566, Accuracy: 1.0\n",
            "  Batch 3546/8000, Loss: 0.4481867253780365, Accuracy: 1.0\n",
            "  Batch 3547/8000, Loss: 1.632994532585144, Accuracy: 0.0\n",
            "  Batch 3548/8000, Loss: 0.4246707558631897, Accuracy: 1.0\n",
            "  Batch 3549/8000, Loss: 0.5252724289894104, Accuracy: 1.0\n",
            "  Batch 3550/8000, Loss: 0.5086084008216858, Accuracy: 1.0\n",
            "  Batch 3551/8000, Loss: 1.1561188697814941, Accuracy: 0.0\n",
            "  Batch 3552/8000, Loss: 1.6749294996261597, Accuracy: 0.0\n",
            "  Batch 3553/8000, Loss: 0.11925055831670761, Accuracy: 1.0\n",
            "  Batch 3554/8000, Loss: 0.12123006582260132, Accuracy: 1.0\n",
            "  Batch 3555/8000, Loss: 0.38470667600631714, Accuracy: 1.0\n",
            "  Batch 3556/8000, Loss: 0.6852251291275024, Accuracy: 1.0\n",
            "  Batch 3557/8000, Loss: 0.40702271461486816, Accuracy: 1.0\n",
            "  Batch 3558/8000, Loss: 0.8039308190345764, Accuracy: 1.0\n",
            "  Batch 3559/8000, Loss: 0.11921772360801697, Accuracy: 1.0\n",
            "  Batch 3560/8000, Loss: 0.8081375956535339, Accuracy: 1.0\n",
            "  Batch 3561/8000, Loss: 1.5983630418777466, Accuracy: 0.0\n",
            "  Batch 3562/8000, Loss: 0.8720993399620056, Accuracy: 0.0\n",
            "  Batch 3563/8000, Loss: 0.9993018507957458, Accuracy: 0.0\n",
            "  Batch 3564/8000, Loss: 0.4886866807937622, Accuracy: 1.0\n",
            "  Batch 3565/8000, Loss: 0.4666061997413635, Accuracy: 1.0\n",
            "  Batch 3566/8000, Loss: 0.34140580892562866, Accuracy: 1.0\n",
            "  Batch 3567/8000, Loss: 0.3546399474143982, Accuracy: 1.0\n",
            "  Batch 3568/8000, Loss: 0.3863334059715271, Accuracy: 1.0\n",
            "  Batch 3569/8000, Loss: 0.4598875641822815, Accuracy: 1.0\n",
            "  Batch 3570/8000, Loss: 0.12234735488891602, Accuracy: 1.0\n",
            "  Batch 3571/8000, Loss: 0.3489066958427429, Accuracy: 1.0\n",
            "  Batch 3572/8000, Loss: 0.9067527651786804, Accuracy: 0.0\n",
            "  Batch 3573/8000, Loss: 0.6309832334518433, Accuracy: 1.0\n",
            "  Batch 3574/8000, Loss: 0.42426517605781555, Accuracy: 1.0\n",
            "  Batch 3575/8000, Loss: 0.1191103458404541, Accuracy: 1.0\n",
            "  Batch 3576/8000, Loss: 0.8132410049438477, Accuracy: 0.0\n",
            "  Batch 3577/8000, Loss: 0.1216392070055008, Accuracy: 1.0\n",
            "  Batch 3578/8000, Loss: 0.1250859797000885, Accuracy: 1.0\n",
            "  Batch 3579/8000, Loss: 0.20412173867225647, Accuracy: 1.0\n",
            "  Batch 3580/8000, Loss: 0.5447612404823303, Accuracy: 1.0\n",
            "  Batch 3581/8000, Loss: 1.7751179933547974, Accuracy: 0.0\n",
            "  Batch 3582/8000, Loss: 0.12471388280391693, Accuracy: 1.0\n",
            "  Batch 3583/8000, Loss: 1.122738242149353, Accuracy: 0.0\n",
            "  Batch 3584/8000, Loss: 0.7257782816886902, Accuracy: 1.0\n",
            "  Batch 3585/8000, Loss: 0.11901916563510895, Accuracy: 1.0\n",
            "  Batch 3586/8000, Loss: 0.32259196043014526, Accuracy: 1.0\n",
            "  Batch 3587/8000, Loss: 0.3593621850013733, Accuracy: 1.0\n",
            "  Batch 3588/8000, Loss: 0.14357173442840576, Accuracy: 1.0\n",
            "  Batch 3589/8000, Loss: 0.12251070141792297, Accuracy: 1.0\n",
            "  Batch 3590/8000, Loss: 0.3361174762248993, Accuracy: 1.0\n",
            "  Batch 3591/8000, Loss: 1.5955142974853516, Accuracy: 0.0\n",
            "  Batch 3592/8000, Loss: 0.1214258000254631, Accuracy: 1.0\n",
            "  Batch 3593/8000, Loss: 0.3225319981575012, Accuracy: 1.0\n",
            "  Batch 3594/8000, Loss: 1.1301418542861938, Accuracy: 0.0\n",
            "  Batch 3595/8000, Loss: 0.3558240830898285, Accuracy: 1.0\n",
            "  Batch 3596/8000, Loss: 1.703033685684204, Accuracy: 0.0\n",
            "  Batch 3597/8000, Loss: 0.3635283410549164, Accuracy: 1.0\n",
            "  Batch 3598/8000, Loss: 0.34208765625953674, Accuracy: 1.0\n",
            "  Batch 3599/8000, Loss: 0.5847605466842651, Accuracy: 1.0\n",
            "  Batch 3600/8000, Loss: 1.0395945310592651, Accuracy: 0.0\n",
            "  Batch 3601/8000, Loss: 0.11903096735477448, Accuracy: 1.0\n",
            "  Batch 3602/8000, Loss: 0.4128415286540985, Accuracy: 1.0\n",
            "  Batch 3603/8000, Loss: 1.1648762226104736, Accuracy: 0.0\n",
            "  Batch 3604/8000, Loss: 0.12038899958133698, Accuracy: 1.0\n",
            "  Batch 3605/8000, Loss: 0.6831376552581787, Accuracy: 1.0\n",
            "  Batch 3606/8000, Loss: 0.3677182197570801, Accuracy: 1.0\n",
            "  Batch 3607/8000, Loss: 0.4857146143913269, Accuracy: 1.0\n",
            "  Batch 3608/8000, Loss: 0.4777332544326782, Accuracy: 1.0\n",
            "  Batch 3609/8000, Loss: 0.6024587154388428, Accuracy: 1.0\n",
            "  Batch 3610/8000, Loss: 0.38622456789016724, Accuracy: 1.0\n",
            "  Batch 3611/8000, Loss: 0.4791717231273651, Accuracy: 1.0\n",
            "  Batch 3612/8000, Loss: 0.7018790245056152, Accuracy: 1.0\n",
            "  Batch 3613/8000, Loss: 0.7271486520767212, Accuracy: 1.0\n",
            "  Batch 3614/8000, Loss: 0.7431914210319519, Accuracy: 1.0\n",
            "  Batch 3615/8000, Loss: 0.4496944546699524, Accuracy: 1.0\n",
            "  Batch 3616/8000, Loss: 0.11889892816543579, Accuracy: 1.0\n",
            "  Batch 3617/8000, Loss: 0.12156547605991364, Accuracy: 1.0\n",
            "  Batch 3618/8000, Loss: 1.7422122955322266, Accuracy: 0.0\n",
            "  Batch 3619/8000, Loss: 0.14834286272525787, Accuracy: 1.0\n",
            "  Batch 3620/8000, Loss: 0.32835865020751953, Accuracy: 1.0\n",
            "  Batch 3621/8000, Loss: 1.584232211112976, Accuracy: 0.0\n",
            "  Batch 3622/8000, Loss: 0.12289413809776306, Accuracy: 1.0\n",
            "  Batch 3623/8000, Loss: 1.6397508382797241, Accuracy: 0.0\n",
            "  Batch 3624/8000, Loss: 0.7954039573669434, Accuracy: 1.0\n",
            "  Batch 3625/8000, Loss: 0.44287043809890747, Accuracy: 1.0\n",
            "  Batch 3626/8000, Loss: 1.0426902770996094, Accuracy: 0.0\n",
            "  Batch 3627/8000, Loss: 0.854786217212677, Accuracy: 0.0\n",
            "  Batch 3628/8000, Loss: 0.8627852201461792, Accuracy: 0.0\n",
            "  Batch 3629/8000, Loss: 0.4504536986351013, Accuracy: 1.0\n",
            "  Batch 3630/8000, Loss: 0.1186780259013176, Accuracy: 1.0\n",
            "  Batch 3631/8000, Loss: 0.4272995591163635, Accuracy: 1.0\n",
            "  Batch 3632/8000, Loss: 1.048191785812378, Accuracy: 0.0\n",
            "  Batch 3633/8000, Loss: 0.3631691336631775, Accuracy: 1.0\n",
            "  Batch 3634/8000, Loss: 0.7058783173561096, Accuracy: 1.0\n",
            "  Batch 3635/8000, Loss: 1.296591877937317, Accuracy: 0.0\n",
            "  Batch 3636/8000, Loss: 1.4870158433914185, Accuracy: 0.0\n",
            "  Batch 3637/8000, Loss: 1.5182596445083618, Accuracy: 0.0\n",
            "  Batch 3638/8000, Loss: 1.5054820775985718, Accuracy: 0.0\n",
            "  Batch 3639/8000, Loss: 1.5468862056732178, Accuracy: 0.0\n",
            "  Batch 3640/8000, Loss: 0.5027774572372437, Accuracy: 1.0\n",
            "  Batch 3641/8000, Loss: 0.41550084948539734, Accuracy: 1.0\n",
            "  Batch 3642/8000, Loss: 0.7578400373458862, Accuracy: 1.0\n",
            "  Batch 3643/8000, Loss: 0.4371824860572815, Accuracy: 1.0\n",
            "  Batch 3644/8000, Loss: 0.48855656385421753, Accuracy: 1.0\n",
            "  Batch 3645/8000, Loss: 0.12034197151660919, Accuracy: 1.0\n",
            "  Batch 3646/8000, Loss: 0.7433454394340515, Accuracy: 1.0\n",
            "  Batch 3647/8000, Loss: 0.45581522583961487, Accuracy: 1.0\n",
            "  Batch 3648/8000, Loss: 0.5548269748687744, Accuracy: 1.0\n",
            "  Batch 3649/8000, Loss: 0.1321275532245636, Accuracy: 1.0\n",
            "  Batch 3650/8000, Loss: 0.4544052183628082, Accuracy: 1.0\n",
            "  Batch 3651/8000, Loss: 0.13737072050571442, Accuracy: 1.0\n",
            "  Batch 3652/8000, Loss: 0.47816774249076843, Accuracy: 1.0\n",
            "  Batch 3653/8000, Loss: 0.4859285056591034, Accuracy: 1.0\n",
            "  Batch 3654/8000, Loss: 1.4599193334579468, Accuracy: 0.0\n",
            "  Batch 3655/8000, Loss: 0.11848444491624832, Accuracy: 1.0\n",
            "  Batch 3656/8000, Loss: 0.809999406337738, Accuracy: 1.0\n",
            "  Batch 3657/8000, Loss: 0.4198340177536011, Accuracy: 1.0\n",
            "  Batch 3658/8000, Loss: 0.46220237016677856, Accuracy: 1.0\n",
            "  Batch 3659/8000, Loss: 0.12165412306785583, Accuracy: 1.0\n",
            "  Batch 3660/8000, Loss: 0.5064464807510376, Accuracy: 1.0\n",
            "  Batch 3661/8000, Loss: 0.12146604061126709, Accuracy: 1.0\n",
            "  Batch 3662/8000, Loss: 1.2380174398422241, Accuracy: 0.0\n",
            "  Batch 3663/8000, Loss: 0.14840342104434967, Accuracy: 1.0\n",
            "  Batch 3664/8000, Loss: 0.12488964200019836, Accuracy: 1.0\n",
            "  Batch 3665/8000, Loss: 0.5427499413490295, Accuracy: 1.0\n",
            "  Batch 3666/8000, Loss: 0.13152846693992615, Accuracy: 1.0\n",
            "  Batch 3667/8000, Loss: 0.4494186341762543, Accuracy: 1.0\n",
            "  Batch 3668/8000, Loss: 0.42504528164863586, Accuracy: 1.0\n",
            "  Batch 3669/8000, Loss: 0.5133936405181885, Accuracy: 1.0\n",
            "  Batch 3670/8000, Loss: 0.7471968531608582, Accuracy: 1.0\n",
            "  Batch 3671/8000, Loss: 0.3891458213329315, Accuracy: 1.0\n",
            "  Batch 3672/8000, Loss: 0.4931033253669739, Accuracy: 1.0\n",
            "  Batch 3673/8000, Loss: 0.9307038187980652, Accuracy: 0.0\n",
            "  Batch 3674/8000, Loss: 0.6216975450515747, Accuracy: 1.0\n",
            "  Batch 3675/8000, Loss: 1.6027809381484985, Accuracy: 0.0\n",
            "  Batch 3676/8000, Loss: 0.3713648319244385, Accuracy: 1.0\n",
            "  Batch 3677/8000, Loss: 0.12104303389787674, Accuracy: 1.0\n",
            "  Batch 3678/8000, Loss: 0.12040336430072784, Accuracy: 1.0\n",
            "  Batch 3679/8000, Loss: 0.44507792592048645, Accuracy: 1.0\n",
            "  Batch 3680/8000, Loss: 0.36943602561950684, Accuracy: 1.0\n",
            "  Batch 3681/8000, Loss: 0.41034626960754395, Accuracy: 1.0\n",
            "  Batch 3682/8000, Loss: 0.15008042752742767, Accuracy: 1.0\n",
            "  Batch 3683/8000, Loss: 0.12181045114994049, Accuracy: 1.0\n",
            "  Batch 3684/8000, Loss: 0.4482019543647766, Accuracy: 1.0\n",
            "  Batch 3685/8000, Loss: 1.0329703092575073, Accuracy: 0.0\n",
            "  Batch 3686/8000, Loss: 0.12834616005420685, Accuracy: 1.0\n",
            "  Batch 3687/8000, Loss: 0.11844196915626526, Accuracy: 1.0\n",
            "  Batch 3688/8000, Loss: 0.3847005367279053, Accuracy: 1.0\n",
            "  Batch 3689/8000, Loss: 0.36704957485198975, Accuracy: 1.0\n",
            "  Batch 3690/8000, Loss: 1.5261183977127075, Accuracy: 0.0\n",
            "  Batch 3691/8000, Loss: 0.5556480884552002, Accuracy: 1.0\n",
            "  Batch 3692/8000, Loss: 0.9005926251411438, Accuracy: 0.0\n",
            "  Batch 3693/8000, Loss: 0.36716964840888977, Accuracy: 1.0\n",
            "  Batch 3694/8000, Loss: 0.12240690737962723, Accuracy: 1.0\n",
            "  Batch 3695/8000, Loss: 0.11824185401201248, Accuracy: 1.0\n",
            "  Batch 3696/8000, Loss: 0.36508694291114807, Accuracy: 1.0\n",
            "  Batch 3697/8000, Loss: 0.3539499342441559, Accuracy: 1.0\n",
            "  Batch 3698/8000, Loss: 0.13444742560386658, Accuracy: 1.0\n",
            "  Batch 3699/8000, Loss: 0.3352895975112915, Accuracy: 1.0\n",
            "  Batch 3700/8000, Loss: 0.6460053324699402, Accuracy: 1.0\n",
            "  Batch 3701/8000, Loss: 0.12090820074081421, Accuracy: 1.0\n",
            "  Batch 3702/8000, Loss: 0.11820708215236664, Accuracy: 1.0\n",
            "  Batch 3703/8000, Loss: 0.36250585317611694, Accuracy: 1.0\n",
            "  Batch 3704/8000, Loss: 0.12098084390163422, Accuracy: 1.0\n",
            "  Batch 3705/8000, Loss: 1.6333087682724, Accuracy: 0.0\n",
            "  Batch 3706/8000, Loss: 0.679863691329956, Accuracy: 1.0\n",
            "  Batch 3707/8000, Loss: 0.379874587059021, Accuracy: 1.0\n",
            "  Batch 3708/8000, Loss: 1.5298614501953125, Accuracy: 0.0\n",
            "  Batch 3709/8000, Loss: 0.3972315788269043, Accuracy: 1.0\n",
            "  Batch 3710/8000, Loss: 0.4183637499809265, Accuracy: 1.0\n",
            "  Batch 3711/8000, Loss: 0.38688188791275024, Accuracy: 1.0\n",
            "  Batch 3712/8000, Loss: 0.13628704845905304, Accuracy: 1.0\n",
            "  Batch 3713/8000, Loss: 0.3486957252025604, Accuracy: 1.0\n",
            "  Batch 3714/8000, Loss: 1.4867441654205322, Accuracy: 0.0\n",
            "  Batch 3715/8000, Loss: 0.13118238747119904, Accuracy: 1.0\n",
            "  Batch 3716/8000, Loss: 0.11892498284578323, Accuracy: 1.0\n",
            "  Batch 3717/8000, Loss: 0.11813721060752869, Accuracy: 1.0\n",
            "  Batch 3718/8000, Loss: 0.11831455677747726, Accuracy: 1.0\n",
            "  Batch 3719/8000, Loss: 0.12430958449840546, Accuracy: 1.0\n",
            "  Batch 3720/8000, Loss: 0.5823962688446045, Accuracy: 1.0\n",
            "  Batch 3721/8000, Loss: 0.41767174005508423, Accuracy: 1.0\n",
            "  Batch 3722/8000, Loss: 1.7224675416946411, Accuracy: 0.0\n",
            "  Batch 3723/8000, Loss: 0.3962695002555847, Accuracy: 1.0\n",
            "  Batch 3724/8000, Loss: 0.951347291469574, Accuracy: 0.0\n",
            "  Batch 3725/8000, Loss: 0.41965553164482117, Accuracy: 1.0\n",
            "  Batch 3726/8000, Loss: 0.3580327033996582, Accuracy: 1.0\n",
            "  Batch 3727/8000, Loss: 0.12033730745315552, Accuracy: 1.0\n",
            "  Batch 3728/8000, Loss: 0.14184920489788055, Accuracy: 1.0\n",
            "  Batch 3729/8000, Loss: 0.39368656277656555, Accuracy: 1.0\n",
            "  Batch 3730/8000, Loss: 0.13913586735725403, Accuracy: 1.0\n",
            "  Batch 3731/8000, Loss: 1.6018345355987549, Accuracy: 0.0\n",
            "  Batch 3732/8000, Loss: 0.1180696189403534, Accuracy: 1.0\n",
            "  Batch 3733/8000, Loss: 1.5168520212173462, Accuracy: 0.0\n",
            "  Batch 3734/8000, Loss: 1.4799590110778809, Accuracy: 0.0\n",
            "  Batch 3735/8000, Loss: 0.12279883027076721, Accuracy: 1.0\n",
            "  Batch 3736/8000, Loss: 0.3882884681224823, Accuracy: 1.0\n",
            "  Batch 3737/8000, Loss: 1.6039252281188965, Accuracy: 0.0\n",
            "  Batch 3738/8000, Loss: 0.3755151331424713, Accuracy: 1.0\n",
            "  Batch 3739/8000, Loss: 0.43927842378616333, Accuracy: 1.0\n",
            "  Batch 3740/8000, Loss: 0.37926650047302246, Accuracy: 1.0\n",
            "  Batch 3741/8000, Loss: 0.12088081985712051, Accuracy: 1.0\n",
            "  Batch 3742/8000, Loss: 0.48629072308540344, Accuracy: 1.0\n",
            "  Batch 3743/8000, Loss: 0.4557373821735382, Accuracy: 1.0\n",
            "  Batch 3744/8000, Loss: 0.6472293138504028, Accuracy: 1.0\n",
            "  Batch 3745/8000, Loss: 0.561802327632904, Accuracy: 1.0\n",
            "  Batch 3746/8000, Loss: 0.1973123848438263, Accuracy: 1.0\n",
            "  Batch 3747/8000, Loss: 1.0421940088272095, Accuracy: 0.0\n",
            "  Batch 3748/8000, Loss: 1.5615206956863403, Accuracy: 0.0\n",
            "  Batch 3749/8000, Loss: 0.40781083703041077, Accuracy: 1.0\n",
            "  Batch 3750/8000, Loss: 0.1252044439315796, Accuracy: 1.0\n",
            "  Batch 3751/8000, Loss: 1.0002918243408203, Accuracy: 0.0\n",
            "  Batch 3752/8000, Loss: 0.6649644374847412, Accuracy: 1.0\n",
            "  Batch 3753/8000, Loss: 1.540366291999817, Accuracy: 0.0\n",
            "  Batch 3754/8000, Loss: 1.4360997676849365, Accuracy: 0.0\n",
            "  Batch 3755/8000, Loss: 0.46136873960494995, Accuracy: 1.0\n",
            "  Batch 3756/8000, Loss: 0.594413697719574, Accuracy: 1.0\n",
            "  Batch 3757/8000, Loss: 0.7403309941291809, Accuracy: 1.0\n",
            "  Batch 3758/8000, Loss: 0.11775490641593933, Accuracy: 1.0\n",
            "  Batch 3759/8000, Loss: 0.1269441694021225, Accuracy: 1.0\n",
            "  Batch 3760/8000, Loss: 1.4998975992202759, Accuracy: 0.0\n",
            "  Batch 3761/8000, Loss: 0.42352214455604553, Accuracy: 1.0\n",
            "  Batch 3762/8000, Loss: 0.11876635253429413, Accuracy: 1.0\n",
            "  Batch 3763/8000, Loss: 0.4130045175552368, Accuracy: 1.0\n",
            "  Batch 3764/8000, Loss: 0.4883483648300171, Accuracy: 1.0\n",
            "  Batch 3765/8000, Loss: 0.6063729524612427, Accuracy: 1.0\n",
            "  Batch 3766/8000, Loss: 0.46939101815223694, Accuracy: 1.0\n",
            "  Batch 3767/8000, Loss: 0.1189524233341217, Accuracy: 1.0\n",
            "  Batch 3768/8000, Loss: 0.41846081614494324, Accuracy: 1.0\n",
            "  Batch 3769/8000, Loss: 0.11958275735378265, Accuracy: 1.0\n",
            "  Batch 3770/8000, Loss: 1.0316182374954224, Accuracy: 0.0\n",
            "  Batch 3771/8000, Loss: 0.5402875542640686, Accuracy: 1.0\n",
            "  Batch 3772/8000, Loss: 0.6480756998062134, Accuracy: 1.0\n",
            "  Batch 3773/8000, Loss: 1.2282803058624268, Accuracy: 0.0\n",
            "  Batch 3774/8000, Loss: 0.11770141124725342, Accuracy: 1.0\n",
            "  Batch 3775/8000, Loss: 1.127802848815918, Accuracy: 0.0\n",
            "  Batch 3776/8000, Loss: 1.3646372556686401, Accuracy: 0.0\n",
            "  Batch 3777/8000, Loss: 0.4960738718509674, Accuracy: 1.0\n",
            "  Batch 3778/8000, Loss: 0.4900508522987366, Accuracy: 1.0\n",
            "  Batch 3779/8000, Loss: 0.46537691354751587, Accuracy: 1.0\n",
            "  Batch 3780/8000, Loss: 3.321863889694214, Accuracy: 0.0\n",
            "  Batch 3781/8000, Loss: 0.40251773595809937, Accuracy: 1.0\n",
            "  Batch 3782/8000, Loss: 0.9211869239807129, Accuracy: 0.0\n",
            "  Batch 3783/8000, Loss: 0.4339524209499359, Accuracy: 1.0\n",
            "  Batch 3784/8000, Loss: 0.999963641166687, Accuracy: 0.0\n",
            "  Batch 3785/8000, Loss: 1.1706206798553467, Accuracy: 0.0\n",
            "  Batch 3786/8000, Loss: 0.3969155550003052, Accuracy: 1.0\n",
            "  Batch 3787/8000, Loss: 0.3948425352573395, Accuracy: 1.0\n",
            "  Batch 3788/8000, Loss: 0.1286303699016571, Accuracy: 1.0\n",
            "  Batch 3789/8000, Loss: 0.41676339507102966, Accuracy: 1.0\n",
            "  Batch 3790/8000, Loss: 0.4918200373649597, Accuracy: 1.0\n",
            "  Batch 3791/8000, Loss: 0.11756487190723419, Accuracy: 1.0\n",
            "  Batch 3792/8000, Loss: 0.12649011611938477, Accuracy: 1.0\n",
            "  Batch 3793/8000, Loss: 0.4267192482948303, Accuracy: 1.0\n",
            "  Batch 3794/8000, Loss: 0.4631192684173584, Accuracy: 1.0\n",
            "  Batch 3795/8000, Loss: 0.39807310700416565, Accuracy: 1.0\n",
            "  Batch 3796/8000, Loss: 0.4230787456035614, Accuracy: 1.0\n",
            "  Batch 3797/8000, Loss: 0.6210656762123108, Accuracy: 1.0\n",
            "  Batch 3798/8000, Loss: 0.9988550543785095, Accuracy: 0.0\n",
            "  Batch 3799/8000, Loss: 0.6736279129981995, Accuracy: 1.0\n",
            "  Batch 3800/8000, Loss: 0.40462958812713623, Accuracy: 1.0\n",
            "  Batch 3801/8000, Loss: 1.6342930793762207, Accuracy: 0.0\n",
            "  Batch 3802/8000, Loss: 1.190885305404663, Accuracy: 0.0\n",
            "  Batch 3803/8000, Loss: 0.3926365375518799, Accuracy: 1.0\n",
            "  Batch 3804/8000, Loss: 0.3622194230556488, Accuracy: 1.0\n",
            "  Batch 3805/8000, Loss: 0.6701099276542664, Accuracy: 1.0\n",
            "  Batch 3806/8000, Loss: 0.4096611738204956, Accuracy: 1.0\n",
            "  Batch 3807/8000, Loss: 0.5476146936416626, Accuracy: 1.0\n",
            "  Batch 3808/8000, Loss: 0.41270166635513306, Accuracy: 1.0\n",
            "  Batch 3809/8000, Loss: 0.4096178710460663, Accuracy: 1.0\n",
            "  Batch 3810/8000, Loss: 0.6685916781425476, Accuracy: 1.0\n",
            "  Batch 3811/8000, Loss: 1.6246927976608276, Accuracy: 0.0\n",
            "  Batch 3812/8000, Loss: 0.3366757929325104, Accuracy: 1.0\n",
            "  Batch 3813/8000, Loss: 0.5592302083969116, Accuracy: 1.0\n",
            "  Batch 3814/8000, Loss: 0.3807593882083893, Accuracy: 1.0\n",
            "  Batch 3815/8000, Loss: 0.343291699886322, Accuracy: 1.0\n",
            "  Batch 3816/8000, Loss: 0.3236468732357025, Accuracy: 1.0\n",
            "  Batch 3817/8000, Loss: 0.13362616300582886, Accuracy: 1.0\n",
            "  Batch 3818/8000, Loss: 0.14593440294265747, Accuracy: 1.0\n",
            "  Batch 3819/8000, Loss: 0.3342626690864563, Accuracy: 1.0\n",
            "  Batch 3820/8000, Loss: 1.7154749631881714, Accuracy: 0.0\n",
            "  Batch 3821/8000, Loss: 0.32793310284614563, Accuracy: 1.0\n",
            "  Batch 3822/8000, Loss: 0.4002045691013336, Accuracy: 1.0\n",
            "  Batch 3823/8000, Loss: 1.3103299140930176, Accuracy: 0.0\n",
            "  Batch 3824/8000, Loss: 0.4957740902900696, Accuracy: 1.0\n",
            "  Batch 3825/8000, Loss: 0.415835440158844, Accuracy: 1.0\n",
            "  Batch 3826/8000, Loss: 0.3208102583885193, Accuracy: 1.0\n",
            "  Batch 3827/8000, Loss: 0.3522143065929413, Accuracy: 1.0\n",
            "  Batch 3828/8000, Loss: 0.3575568199157715, Accuracy: 1.0\n",
            "  Batch 3829/8000, Loss: 1.4924399852752686, Accuracy: 0.0\n",
            "  Batch 3830/8000, Loss: 1.4785516262054443, Accuracy: 0.0\n",
            "  Batch 3831/8000, Loss: 1.5473798513412476, Accuracy: 0.0\n",
            "  Batch 3832/8000, Loss: 0.3106367588043213, Accuracy: 1.0\n",
            "  Batch 3833/8000, Loss: 0.31446218490600586, Accuracy: 1.0\n",
            "  Batch 3834/8000, Loss: 1.5557570457458496, Accuracy: 0.0\n",
            "  Batch 3835/8000, Loss: 0.11739496886730194, Accuracy: 1.0\n",
            "  Batch 3836/8000, Loss: 0.4697808027267456, Accuracy: 1.0\n",
            "  Batch 3837/8000, Loss: 1.56382155418396, Accuracy: 0.0\n",
            "  Batch 3838/8000, Loss: 0.3571782410144806, Accuracy: 1.0\n",
            "  Batch 3839/8000, Loss: 0.47789251804351807, Accuracy: 1.0\n",
            "  Batch 3840/8000, Loss: 0.39244556427001953, Accuracy: 1.0\n",
            "  Batch 3841/8000, Loss: 0.45787614583969116, Accuracy: 1.0\n",
            "  Batch 3842/8000, Loss: 0.4313139319419861, Accuracy: 1.0\n",
            "  Batch 3843/8000, Loss: 1.6112821102142334, Accuracy: 0.0\n",
            "  Batch 3844/8000, Loss: 0.38046425580978394, Accuracy: 1.0\n",
            "  Batch 3845/8000, Loss: 1.1355596780776978, Accuracy: 0.0\n",
            "  Batch 3846/8000, Loss: 0.11801895499229431, Accuracy: 1.0\n",
            "  Batch 3847/8000, Loss: 0.47026827931404114, Accuracy: 1.0\n",
            "  Batch 3848/8000, Loss: 0.38499903678894043, Accuracy: 1.0\n",
            "  Batch 3849/8000, Loss: 0.12999829649925232, Accuracy: 1.0\n",
            "  Batch 3850/8000, Loss: 1.5874723196029663, Accuracy: 0.0\n",
            "  Batch 3851/8000, Loss: 0.6066010594367981, Accuracy: 1.0\n",
            "  Batch 3852/8000, Loss: 0.14522714912891388, Accuracy: 1.0\n",
            "  Batch 3853/8000, Loss: 1.0815534591674805, Accuracy: 0.0\n",
            "  Batch 3854/8000, Loss: 0.37590909004211426, Accuracy: 1.0\n",
            "  Batch 3855/8000, Loss: 0.36502620577812195, Accuracy: 1.0\n",
            "  Batch 3856/8000, Loss: 0.684913694858551, Accuracy: 1.0\n",
            "  Batch 3857/8000, Loss: 0.12877880036830902, Accuracy: 1.0\n",
            "  Batch 3858/8000, Loss: 0.6672041416168213, Accuracy: 1.0\n",
            "  Batch 3859/8000, Loss: 0.44301873445510864, Accuracy: 1.0\n",
            "  Batch 3860/8000, Loss: 0.5940493941307068, Accuracy: 1.0\n",
            "  Batch 3861/8000, Loss: 0.399225115776062, Accuracy: 1.0\n",
            "  Batch 3862/8000, Loss: 0.6084765791893005, Accuracy: 1.0\n",
            "  Batch 3863/8000, Loss: 0.11702413856983185, Accuracy: 1.0\n",
            "  Batch 3864/8000, Loss: 0.11761783808469772, Accuracy: 1.0\n",
            "  Batch 3865/8000, Loss: 0.15715612471103668, Accuracy: 1.0\n",
            "  Batch 3866/8000, Loss: 0.13257735967636108, Accuracy: 1.0\n",
            "  Batch 3867/8000, Loss: 0.6708976030349731, Accuracy: 1.0\n",
            "  Batch 3868/8000, Loss: 0.6460628509521484, Accuracy: 1.0\n",
            "  Batch 3869/8000, Loss: 0.5964975953102112, Accuracy: 1.0\n",
            "  Batch 3870/8000, Loss: 0.41148871183395386, Accuracy: 1.0\n",
            "  Batch 3871/8000, Loss: 0.12595301866531372, Accuracy: 1.0\n",
            "  Batch 3872/8000, Loss: 0.40749841928482056, Accuracy: 1.0\n",
            "  Batch 3873/8000, Loss: 0.12501591444015503, Accuracy: 1.0\n",
            "  Batch 3874/8000, Loss: 0.9941164255142212, Accuracy: 0.0\n",
            "  Batch 3875/8000, Loss: 0.3495950400829315, Accuracy: 1.0\n",
            "  Batch 3876/8000, Loss: 0.35438454151153564, Accuracy: 1.0\n",
            "  Batch 3877/8000, Loss: 0.11695091426372528, Accuracy: 1.0\n",
            "  Batch 3878/8000, Loss: 1.0473129749298096, Accuracy: 0.0\n",
            "  Batch 3879/8000, Loss: 0.26477017998695374, Accuracy: 1.0\n",
            "  Batch 3880/8000, Loss: 0.40457314252853394, Accuracy: 1.0\n",
            "  Batch 3881/8000, Loss: 0.42879220843315125, Accuracy: 1.0\n",
            "  Batch 3882/8000, Loss: 0.11704429984092712, Accuracy: 1.0\n",
            "  Batch 3883/8000, Loss: 0.12243804335594177, Accuracy: 1.0\n",
            "  Batch 3884/8000, Loss: 1.0806914567947388, Accuracy: 0.0\n",
            "  Batch 3885/8000, Loss: 0.12254048883914948, Accuracy: 1.0\n",
            "  Batch 3886/8000, Loss: 0.1630299985408783, Accuracy: 1.0\n",
            "  Batch 3887/8000, Loss: 1.895288109779358, Accuracy: 0.0\n",
            "  Batch 3888/8000, Loss: 0.3333948850631714, Accuracy: 1.0\n",
            "  Batch 3889/8000, Loss: 0.33739739656448364, Accuracy: 1.0\n",
            "  Batch 3890/8000, Loss: 0.3118230104446411, Accuracy: 1.0\n",
            "  Batch 3891/8000, Loss: 1.780959963798523, Accuracy: 0.0\n",
            "  Batch 3892/8000, Loss: 0.13288283348083496, Accuracy: 1.0\n",
            "  Batch 3893/8000, Loss: 0.31842073798179626, Accuracy: 1.0\n",
            "  Batch 3894/8000, Loss: 0.12424376606941223, Accuracy: 1.0\n",
            "  Batch 3895/8000, Loss: 0.3137081563472748, Accuracy: 1.0\n",
            "  Batch 3896/8000, Loss: 0.28938886523246765, Accuracy: 1.0\n",
            "  Batch 3897/8000, Loss: 0.41793352365493774, Accuracy: 1.0\n",
            "  Batch 3898/8000, Loss: 0.3932555317878723, Accuracy: 1.0\n",
            "  Batch 3899/8000, Loss: 0.1420629322528839, Accuracy: 1.0\n",
            "  Batch 3900/8000, Loss: 0.1283653825521469, Accuracy: 1.0\n",
            "  Batch 3901/8000, Loss: 0.3419398367404938, Accuracy: 1.0\n",
            "  Batch 3902/8000, Loss: 0.28145214915275574, Accuracy: 1.0\n",
            "  Batch 3903/8000, Loss: 0.33485883474349976, Accuracy: 1.0\n",
            "  Batch 3904/8000, Loss: 0.11682441085577011, Accuracy: 1.0\n",
            "  Batch 3905/8000, Loss: 0.11679476499557495, Accuracy: 1.0\n",
            "  Batch 3906/8000, Loss: 1.796634316444397, Accuracy: 0.0\n",
            "  Batch 3907/8000, Loss: 0.31898343563079834, Accuracy: 1.0\n",
            "  Batch 3908/8000, Loss: 1.8332514762878418, Accuracy: 0.0\n",
            "  Batch 3909/8000, Loss: 0.32519781589508057, Accuracy: 1.0\n",
            "  Batch 3910/8000, Loss: 0.1501295566558838, Accuracy: 1.0\n",
            "  Batch 3911/8000, Loss: 0.29216891527175903, Accuracy: 1.0\n",
            "  Batch 3912/8000, Loss: 0.13412001729011536, Accuracy: 1.0\n",
            "  Batch 3913/8000, Loss: 0.11727163940668106, Accuracy: 1.0\n",
            "  Batch 3914/8000, Loss: 0.39791494607925415, Accuracy: 1.0\n",
            "  Batch 3915/8000, Loss: 0.2777368724346161, Accuracy: 1.0\n",
            "  Batch 3916/8000, Loss: 0.3091726005077362, Accuracy: 1.0\n",
            "  Batch 3917/8000, Loss: 0.6162630319595337, Accuracy: 1.0\n",
            "  Batch 3918/8000, Loss: 0.1287045180797577, Accuracy: 1.0\n",
            "  Batch 3919/8000, Loss: 1.7353520393371582, Accuracy: 0.0\n",
            "  Batch 3920/8000, Loss: 0.14065513014793396, Accuracy: 1.0\n",
            "  Batch 3921/8000, Loss: 0.31609272956848145, Accuracy: 1.0\n",
            "  Batch 3922/8000, Loss: 0.33460304141044617, Accuracy: 1.0\n",
            "  Batch 3923/8000, Loss: 0.3188360929489136, Accuracy: 1.0\n",
            "  Batch 3924/8000, Loss: 1.5248347520828247, Accuracy: 0.0\n",
            "  Batch 3925/8000, Loss: 0.13854201138019562, Accuracy: 1.0\n",
            "  Batch 3926/8000, Loss: 0.2737186551094055, Accuracy: 1.0\n",
            "  Batch 3927/8000, Loss: 0.34757936000823975, Accuracy: 1.0\n",
            "  Batch 3928/8000, Loss: 0.3020459711551666, Accuracy: 1.0\n",
            "  Batch 3929/8000, Loss: 0.33058616518974304, Accuracy: 1.0\n",
            "  Batch 3930/8000, Loss: 0.48926812410354614, Accuracy: 1.0\n",
            "  Batch 3931/8000, Loss: 0.4261738061904907, Accuracy: 1.0\n",
            "  Batch 3932/8000, Loss: 1.5887879133224487, Accuracy: 0.0\n",
            "  Batch 3933/8000, Loss: 0.404522180557251, Accuracy: 1.0\n",
            "  Batch 3934/8000, Loss: 0.36704662442207336, Accuracy: 1.0\n",
            "  Batch 3935/8000, Loss: 0.36533695459365845, Accuracy: 1.0\n",
            "  Batch 3936/8000, Loss: 0.3287128806114197, Accuracy: 1.0\n",
            "  Batch 3937/8000, Loss: 0.3074157238006592, Accuracy: 1.0\n",
            "  Batch 3938/8000, Loss: 0.3659195899963379, Accuracy: 1.0\n",
            "  Batch 3939/8000, Loss: 0.4745120704174042, Accuracy: 1.0\n",
            "  Batch 3940/8000, Loss: 0.46567291021347046, Accuracy: 1.0\n",
            "  Batch 3941/8000, Loss: 1.7588269710540771, Accuracy: 0.0\n",
            "  Batch 3942/8000, Loss: 1.1919167041778564, Accuracy: 0.0\n",
            "  Batch 3943/8000, Loss: 1.8277517557144165, Accuracy: 0.0\n",
            "  Batch 3944/8000, Loss: 0.38214439153671265, Accuracy: 1.0\n",
            "  Batch 3945/8000, Loss: 0.2239999771118164, Accuracy: 1.0\n",
            "  Batch 3946/8000, Loss: 0.4632001519203186, Accuracy: 1.0\n",
            "  Batch 3947/8000, Loss: 0.2958986461162567, Accuracy: 1.0\n",
            "  Batch 3948/8000, Loss: 1.2881940603256226, Accuracy: 0.0\n",
            "  Batch 3949/8000, Loss: 0.11962004005908966, Accuracy: 1.0\n",
            "  Batch 3950/8000, Loss: 0.2474653273820877, Accuracy: 1.0\n",
            "  Batch 3951/8000, Loss: 1.6949431896209717, Accuracy: 0.0\n",
            "  Batch 3952/8000, Loss: 0.3787168264389038, Accuracy: 1.0\n",
            "  Batch 3953/8000, Loss: 0.45408347249031067, Accuracy: 1.0\n",
            "  Batch 3954/8000, Loss: 1.1357110738754272, Accuracy: 0.0\n",
            "  Batch 3955/8000, Loss: 1.3008577823638916, Accuracy: 0.0\n",
            "  Batch 3956/8000, Loss: 0.11780882626771927, Accuracy: 1.0\n",
            "  Batch 3957/8000, Loss: 0.46073010563850403, Accuracy: 1.0\n",
            "  Batch 3958/8000, Loss: 1.6192036867141724, Accuracy: 0.0\n",
            "  Batch 3959/8000, Loss: 0.540160059928894, Accuracy: 1.0\n",
            "  Batch 3960/8000, Loss: 0.4177522659301758, Accuracy: 1.0\n",
            "  Batch 3961/8000, Loss: 0.3993808329105377, Accuracy: 1.0\n",
            "  Batch 3962/8000, Loss: 0.11656289547681808, Accuracy: 1.0\n",
            "  Batch 3963/8000, Loss: 0.6563163995742798, Accuracy: 1.0\n",
            "  Batch 3964/8000, Loss: 0.41186097264289856, Accuracy: 1.0\n",
            "  Batch 3965/8000, Loss: 0.40392813086509705, Accuracy: 1.0\n",
            "  Batch 3966/8000, Loss: 0.12433025240898132, Accuracy: 1.0\n",
            "  Batch 3967/8000, Loss: 0.11651149392127991, Accuracy: 1.0\n",
            "  Batch 3968/8000, Loss: 0.42020368576049805, Accuracy: 1.0\n",
            "  Batch 3969/8000, Loss: 0.81622314453125, Accuracy: 0.0\n",
            "  Batch 3970/8000, Loss: 1.3316813707351685, Accuracy: 0.0\n",
            "  Batch 3971/8000, Loss: 0.12009963393211365, Accuracy: 1.0\n",
            "  Batch 3972/8000, Loss: 0.42652446031570435, Accuracy: 1.0\n",
            "  Batch 3973/8000, Loss: 0.4616321623325348, Accuracy: 1.0\n",
            "  Batch 3974/8000, Loss: 0.4046415090560913, Accuracy: 1.0\n",
            "  Batch 3975/8000, Loss: 0.12391577661037445, Accuracy: 1.0\n",
            "  Batch 3976/8000, Loss: 1.2425426244735718, Accuracy: 0.0\n",
            "  Batch 3977/8000, Loss: 0.12236583977937698, Accuracy: 1.0\n",
            "  Batch 3978/8000, Loss: 0.39491286873817444, Accuracy: 1.0\n",
            "  Batch 3979/8000, Loss: 0.41053342819213867, Accuracy: 1.0\n",
            "  Batch 3980/8000, Loss: 0.12271758913993835, Accuracy: 1.0\n",
            "  Batch 3981/8000, Loss: 0.4750717282295227, Accuracy: 1.0\n",
            "  Batch 3982/8000, Loss: 0.11641277372837067, Accuracy: 1.0\n",
            "  Batch 3983/8000, Loss: 0.3630363941192627, Accuracy: 1.0\n",
            "  Batch 3984/8000, Loss: 0.11696716398000717, Accuracy: 1.0\n",
            "  Batch 3985/8000, Loss: 0.4885966181755066, Accuracy: 1.0\n",
            "  Batch 3986/8000, Loss: 0.1279684454202652, Accuracy: 1.0\n",
            "  Batch 3987/8000, Loss: 0.3949104845523834, Accuracy: 1.0\n",
            "  Batch 3988/8000, Loss: 0.961866557598114, Accuracy: 0.0\n",
            "  Batch 3989/8000, Loss: 0.7605332136154175, Accuracy: 1.0\n",
            "  Batch 3990/8000, Loss: 0.8714749217033386, Accuracy: 0.0\n",
            "  Batch 3991/8000, Loss: 0.3600099980831146, Accuracy: 1.0\n",
            "  Batch 3992/8000, Loss: 0.4019818902015686, Accuracy: 1.0\n",
            "  Batch 3993/8000, Loss: 0.3806975483894348, Accuracy: 1.0\n",
            "  Batch 3994/8000, Loss: 0.11825394630432129, Accuracy: 1.0\n",
            "  Batch 3995/8000, Loss: 0.3916703164577484, Accuracy: 1.0\n",
            "  Batch 3996/8000, Loss: 0.11686128377914429, Accuracy: 1.0\n",
            "  Batch 3997/8000, Loss: 0.11702105402946472, Accuracy: 1.0\n",
            "  Batch 3998/8000, Loss: 0.5101931691169739, Accuracy: 1.0\n",
            "  Batch 3999/8000, Loss: 0.5802575349807739, Accuracy: 1.0\n",
            "  Batch 4000/8000, Loss: 0.38053232431411743, Accuracy: 1.0\n",
            "  Batch 4001/8000, Loss: 1.6905162334442139, Accuracy: 0.0\n",
            "  Batch 4002/8000, Loss: 1.5333834886550903, Accuracy: 0.0\n",
            "  Batch 4003/8000, Loss: 0.1171923577785492, Accuracy: 1.0\n",
            "  Batch 4004/8000, Loss: 1.3245735168457031, Accuracy: 0.0\n",
            "  Batch 4005/8000, Loss: 0.11640787869691849, Accuracy: 1.0\n",
            "  Batch 4006/8000, Loss: 0.1168307363986969, Accuracy: 1.0\n",
            "  Batch 4007/8000, Loss: 0.11626705527305603, Accuracy: 1.0\n",
            "  Batch 4008/8000, Loss: 0.4196833074092865, Accuracy: 1.0\n",
            "  Batch 4009/8000, Loss: 1.7707600593566895, Accuracy: 0.0\n",
            "  Batch 4010/8000, Loss: 1.5445514917373657, Accuracy: 0.0\n",
            "  Batch 4011/8000, Loss: 0.3644678294658661, Accuracy: 1.0\n",
            "  Batch 4012/8000, Loss: 0.4210171103477478, Accuracy: 1.0\n",
            "  Batch 4013/8000, Loss: 0.7585732340812683, Accuracy: 1.0\n",
            "  Batch 4014/8000, Loss: 3.0810272693634033, Accuracy: 0.0\n",
            "  Batch 4015/8000, Loss: 0.45103389024734497, Accuracy: 1.0\n",
            "  Batch 4016/8000, Loss: 0.3660390079021454, Accuracy: 1.0\n",
            "  Batch 4017/8000, Loss: 0.6517313122749329, Accuracy: 1.0\n",
            "  Batch 4018/8000, Loss: 0.3697240948677063, Accuracy: 1.0\n",
            "  Batch 4019/8000, Loss: 0.3788227140903473, Accuracy: 1.0\n",
            "  Batch 4020/8000, Loss: 0.3729819357395172, Accuracy: 1.0\n",
            "  Batch 4021/8000, Loss: 0.6604255437850952, Accuracy: 1.0\n",
            "  Batch 4022/8000, Loss: 0.4252055883407593, Accuracy: 1.0\n",
            "  Batch 4023/8000, Loss: 0.13283875584602356, Accuracy: 1.0\n",
            "  Batch 4024/8000, Loss: 0.5501914620399475, Accuracy: 1.0\n",
            "  Batch 4025/8000, Loss: 0.15465639531612396, Accuracy: 1.0\n",
            "  Batch 4026/8000, Loss: 0.5571384429931641, Accuracy: 1.0\n",
            "  Batch 4027/8000, Loss: 1.7045271396636963, Accuracy: 0.0\n",
            "  Batch 4028/8000, Loss: 0.5642495155334473, Accuracy: 1.0\n",
            "  Batch 4029/8000, Loss: 0.3670063316822052, Accuracy: 1.0\n",
            "  Batch 4030/8000, Loss: 0.336406946182251, Accuracy: 1.0\n",
            "  Batch 4031/8000, Loss: 0.3569496273994446, Accuracy: 1.0\n",
            "  Batch 4032/8000, Loss: 1.2556922435760498, Accuracy: 0.0\n",
            "  Batch 4033/8000, Loss: 0.35471034049987793, Accuracy: 1.0\n",
            "  Batch 4034/8000, Loss: 0.4951638877391815, Accuracy: 1.0\n",
            "  Batch 4035/8000, Loss: 0.3372322618961334, Accuracy: 1.0\n",
            "  Batch 4036/8000, Loss: 0.3006845712661743, Accuracy: 1.0\n",
            "  Batch 4037/8000, Loss: 1.0130759477615356, Accuracy: 0.0\n",
            "  Batch 4038/8000, Loss: 0.3469598889350891, Accuracy: 1.0\n",
            "  Batch 4039/8000, Loss: 1.1212693452835083, Accuracy: 0.0\n",
            "  Batch 4040/8000, Loss: 1.538177251815796, Accuracy: 0.0\n",
            "  Batch 4041/8000, Loss: 1.43057119846344, Accuracy: 0.0\n",
            "  Batch 4042/8000, Loss: 0.30493420362472534, Accuracy: 1.0\n",
            "  Batch 4043/8000, Loss: 0.3238626718521118, Accuracy: 1.0\n",
            "  Batch 4044/8000, Loss: 0.5042635798454285, Accuracy: 1.0\n",
            "  Batch 4045/8000, Loss: 0.45206218957901, Accuracy: 1.0\n",
            "  Batch 4046/8000, Loss: 1.4771087169647217, Accuracy: 0.0\n",
            "  Batch 4047/8000, Loss: 1.0896015167236328, Accuracy: 0.0\n",
            "  Batch 4048/8000, Loss: 0.6257981657981873, Accuracy: 1.0\n",
            "  Batch 4049/8000, Loss: 1.3997219800949097, Accuracy: 0.0\n",
            "  Batch 4050/8000, Loss: 0.11606618762016296, Accuracy: 1.0\n",
            "  Batch 4051/8000, Loss: 0.11615390330553055, Accuracy: 1.0\n",
            "  Batch 4052/8000, Loss: 0.11920372396707535, Accuracy: 1.0\n",
            "  Batch 4053/8000, Loss: 0.3158912658691406, Accuracy: 1.0\n",
            "  Batch 4054/8000, Loss: 0.443442165851593, Accuracy: 1.0\n",
            "  Batch 4055/8000, Loss: 0.44175174832344055, Accuracy: 1.0\n",
            "  Batch 4056/8000, Loss: 0.35965248942375183, Accuracy: 1.0\n",
            "  Batch 4057/8000, Loss: 1.5054264068603516, Accuracy: 0.0\n",
            "  Batch 4058/8000, Loss: 0.5716435313224792, Accuracy: 1.0\n",
            "  Batch 4059/8000, Loss: 0.3351842761039734, Accuracy: 1.0\n",
            "  Batch 4060/8000, Loss: 1.724569320678711, Accuracy: 0.0\n",
            "  Batch 4061/8000, Loss: 0.3750777840614319, Accuracy: 1.0\n",
            "  Batch 4062/8000, Loss: 0.35504889488220215, Accuracy: 1.0\n",
            "  Batch 4063/8000, Loss: 0.3534729480743408, Accuracy: 1.0\n",
            "  Batch 4064/8000, Loss: 0.4270363748073578, Accuracy: 1.0\n",
            "  Batch 4065/8000, Loss: 0.6288605332374573, Accuracy: 1.0\n",
            "  Batch 4066/8000, Loss: 0.21193698048591614, Accuracy: 1.0\n",
            "  Batch 4067/8000, Loss: 1.6016353368759155, Accuracy: 0.0\n",
            "  Batch 4068/8000, Loss: 0.3874910771846771, Accuracy: 1.0\n",
            "  Batch 4069/8000, Loss: 0.7679814100265503, Accuracy: 1.0\n",
            "  Batch 4070/8000, Loss: 0.5221720933914185, Accuracy: 1.0\n",
            "  Batch 4071/8000, Loss: 0.35449814796447754, Accuracy: 1.0\n",
            "  Batch 4072/8000, Loss: 0.1159389466047287, Accuracy: 1.0\n",
            "  Batch 4073/8000, Loss: 0.3264707624912262, Accuracy: 1.0\n",
            "  Batch 4074/8000, Loss: 0.3460148870944977, Accuracy: 1.0\n",
            "  Batch 4075/8000, Loss: 0.3984077274799347, Accuracy: 1.0\n",
            "  Batch 4076/8000, Loss: 1.4972823858261108, Accuracy: 0.0\n",
            "  Batch 4077/8000, Loss: 0.9941530823707581, Accuracy: 0.0\n",
            "  Batch 4078/8000, Loss: 1.3818399906158447, Accuracy: 0.0\n",
            "  Batch 4079/8000, Loss: 1.7263370752334595, Accuracy: 0.0\n",
            "  Batch 4080/8000, Loss: 0.3329482674598694, Accuracy: 1.0\n",
            "  Batch 4081/8000, Loss: 0.19521674513816833, Accuracy: 1.0\n",
            "  Batch 4082/8000, Loss: 0.6344144344329834, Accuracy: 1.0\n",
            "  Batch 4083/8000, Loss: 0.6226654052734375, Accuracy: 1.0\n",
            "  Batch 4084/8000, Loss: 0.12523655593395233, Accuracy: 1.0\n",
            "  Batch 4085/8000, Loss: 0.9648282527923584, Accuracy: 0.0\n",
            "  Batch 4086/8000, Loss: 0.33262184262275696, Accuracy: 1.0\n",
            "  Batch 4087/8000, Loss: 0.46984678506851196, Accuracy: 1.0\n",
            "  Batch 4088/8000, Loss: 0.43094581365585327, Accuracy: 1.0\n",
            "  Batch 4089/8000, Loss: 0.1253126859664917, Accuracy: 1.0\n",
            "  Batch 4090/8000, Loss: 0.13087478280067444, Accuracy: 1.0\n",
            "  Batch 4091/8000, Loss: 1.0453802347183228, Accuracy: 0.0\n",
            "  Batch 4092/8000, Loss: 0.349483847618103, Accuracy: 1.0\n",
            "  Batch 4093/8000, Loss: 0.6424892544746399, Accuracy: 1.0\n",
            "  Batch 4094/8000, Loss: 1.559902548789978, Accuracy: 0.0\n",
            "  Batch 4095/8000, Loss: 0.6356807351112366, Accuracy: 1.0\n",
            "  Batch 4096/8000, Loss: 1.2946290969848633, Accuracy: 0.0\n",
            "  Batch 4097/8000, Loss: 0.5298168659210205, Accuracy: 1.0\n",
            "  Batch 4098/8000, Loss: 1.109574556350708, Accuracy: 0.0\n",
            "  Batch 4099/8000, Loss: 1.64622962474823, Accuracy: 0.0\n",
            "  Batch 4100/8000, Loss: 0.6049901843070984, Accuracy: 1.0\n",
            "  Batch 4101/8000, Loss: 1.015192985534668, Accuracy: 0.0\n",
            "  Batch 4102/8000, Loss: 0.39897704124450684, Accuracy: 1.0\n",
            "  Batch 4103/8000, Loss: 0.6441333889961243, Accuracy: 1.0\n",
            "  Batch 4104/8000, Loss: 0.8634979724884033, Accuracy: 0.0\n",
            "  Batch 4105/8000, Loss: 1.515904426574707, Accuracy: 0.0\n",
            "  Batch 4106/8000, Loss: 0.5625669956207275, Accuracy: 1.0\n",
            "  Batch 4107/8000, Loss: 0.6625419855117798, Accuracy: 1.0\n",
            "  Batch 4108/8000, Loss: 1.2931039333343506, Accuracy: 0.0\n",
            "  Batch 4109/8000, Loss: 0.5228903889656067, Accuracy: 1.0\n",
            "  Batch 4110/8000, Loss: 0.418885201215744, Accuracy: 1.0\n",
            "  Batch 4111/8000, Loss: 0.54997718334198, Accuracy: 1.0\n",
            "  Batch 4112/8000, Loss: 1.4891096353530884, Accuracy: 0.0\n",
            "  Batch 4113/8000, Loss: 0.7017228603363037, Accuracy: 1.0\n",
            "  Batch 4114/8000, Loss: 0.11922793090343475, Accuracy: 1.0\n",
            "  Batch 4115/8000, Loss: 0.9210359454154968, Accuracy: 0.0\n",
            "  Batch 4116/8000, Loss: 1.310237169265747, Accuracy: 0.0\n",
            "  Batch 4117/8000, Loss: 0.11742480099201202, Accuracy: 1.0\n",
            "  Batch 4118/8000, Loss: 0.6014531254768372, Accuracy: 1.0\n",
            "  Batch 4119/8000, Loss: 1.1249781847000122, Accuracy: 0.0\n",
            "  Batch 4120/8000, Loss: 0.6841388940811157, Accuracy: 1.0\n",
            "  Batch 4121/8000, Loss: 0.46421530842781067, Accuracy: 1.0\n",
            "  Batch 4122/8000, Loss: 0.7237962484359741, Accuracy: 1.0\n",
            "  Batch 4123/8000, Loss: 0.4921991229057312, Accuracy: 1.0\n",
            "  Batch 4124/8000, Loss: 0.4661828279495239, Accuracy: 1.0\n",
            "  Batch 4125/8000, Loss: 0.11791878193616867, Accuracy: 1.0\n",
            "  Batch 4126/8000, Loss: 0.3137871325016022, Accuracy: 1.0\n",
            "  Batch 4127/8000, Loss: 0.1190471202135086, Accuracy: 1.0\n",
            "  Batch 4128/8000, Loss: 0.47643494606018066, Accuracy: 1.0\n",
            "  Batch 4129/8000, Loss: 0.44038334488868713, Accuracy: 1.0\n",
            "  Batch 4130/8000, Loss: 1.3429458141326904, Accuracy: 0.0\n",
            "  Batch 4131/8000, Loss: 0.6412527561187744, Accuracy: 1.0\n",
            "  Batch 4132/8000, Loss: 0.459587037563324, Accuracy: 1.0\n",
            "  Batch 4133/8000, Loss: 0.7149716019630432, Accuracy: 1.0\n",
            "  Batch 4134/8000, Loss: 0.8999190926551819, Accuracy: 0.0\n",
            "  Batch 4135/8000, Loss: 0.8346075415611267, Accuracy: 0.0\n",
            "  Batch 4136/8000, Loss: 1.3092894554138184, Accuracy: 0.0\n",
            "  Batch 4137/8000, Loss: 0.4661905765533447, Accuracy: 1.0\n",
            "  Batch 4138/8000, Loss: 0.4819910228252411, Accuracy: 1.0\n",
            "  Batch 4139/8000, Loss: 0.5826781392097473, Accuracy: 1.0\n",
            "  Batch 4140/8000, Loss: 0.4739261567592621, Accuracy: 1.0\n",
            "  Batch 4141/8000, Loss: 0.8470839262008667, Accuracy: 0.0\n",
            "  Batch 4142/8000, Loss: 1.0124036073684692, Accuracy: 0.0\n",
            "  Batch 4143/8000, Loss: 0.4730631709098816, Accuracy: 1.0\n",
            "  Batch 4144/8000, Loss: 1.3706293106079102, Accuracy: 0.0\n",
            "  Batch 4145/8000, Loss: 0.5412434339523315, Accuracy: 1.0\n",
            "  Batch 4146/8000, Loss: 0.6795951128005981, Accuracy: 1.0\n",
            "  Batch 4147/8000, Loss: 0.11738190054893494, Accuracy: 1.0\n",
            "  Batch 4148/8000, Loss: 0.1175624281167984, Accuracy: 1.0\n",
            "  Batch 4149/8000, Loss: 0.6366231441497803, Accuracy: 1.0\n",
            "  Batch 4150/8000, Loss: 0.4720323979854584, Accuracy: 1.0\n",
            "  Batch 4151/8000, Loss: 0.6447545289993286, Accuracy: 1.0\n",
            "  Batch 4152/8000, Loss: 0.5688393712043762, Accuracy: 1.0\n",
            "  Batch 4153/8000, Loss: 0.45456719398498535, Accuracy: 1.0\n",
            "  Batch 4154/8000, Loss: 0.431949257850647, Accuracy: 1.0\n",
            "  Batch 4155/8000, Loss: 0.7685462832450867, Accuracy: 1.0\n",
            "  Batch 4156/8000, Loss: 1.014706015586853, Accuracy: 0.0\n",
            "  Batch 4157/8000, Loss: 0.5348882675170898, Accuracy: 1.0\n",
            "  Batch 4158/8000, Loss: 0.44757238030433655, Accuracy: 1.0\n",
            "  Batch 4159/8000, Loss: 0.49812135100364685, Accuracy: 1.0\n",
            "  Batch 4160/8000, Loss: 0.48367542028427124, Accuracy: 1.0\n",
            "  Batch 4161/8000, Loss: 0.4432710111141205, Accuracy: 1.0\n",
            "  Batch 4162/8000, Loss: 0.792560875415802, Accuracy: 1.0\n",
            "  Batch 4163/8000, Loss: 0.948828399181366, Accuracy: 0.0\n",
            "  Batch 4164/8000, Loss: 1.4344109296798706, Accuracy: 0.0\n",
            "  Batch 4165/8000, Loss: 0.40735340118408203, Accuracy: 1.0\n",
            "  Batch 4166/8000, Loss: 0.11588427424430847, Accuracy: 1.0\n",
            "  Batch 4167/8000, Loss: 0.40574145317077637, Accuracy: 1.0\n",
            "  Batch 4168/8000, Loss: 0.46988585591316223, Accuracy: 1.0\n",
            "  Batch 4169/8000, Loss: 0.4598248302936554, Accuracy: 1.0\n",
            "  Batch 4170/8000, Loss: 0.12120461463928223, Accuracy: 1.0\n",
            "  Batch 4171/8000, Loss: 0.9863331913948059, Accuracy: 0.0\n",
            "  Batch 4172/8000, Loss: 0.11753658205270767, Accuracy: 1.0\n",
            "  Batch 4173/8000, Loss: 1.53938627243042, Accuracy: 0.0\n",
            "  Batch 4174/8000, Loss: 0.38255828619003296, Accuracy: 1.0\n",
            "  Batch 4175/8000, Loss: 0.5732757449150085, Accuracy: 1.0\n",
            "  Batch 4176/8000, Loss: 1.1742923259735107, Accuracy: 0.0\n",
            "  Batch 4177/8000, Loss: 0.4473450779914856, Accuracy: 1.0\n",
            "  Batch 4178/8000, Loss: 0.3662874400615692, Accuracy: 1.0\n",
            "  Batch 4179/8000, Loss: 1.0602673292160034, Accuracy: 0.0\n",
            "  Batch 4180/8000, Loss: 0.11757645010948181, Accuracy: 1.0\n",
            "  Batch 4181/8000, Loss: 0.41359299421310425, Accuracy: 1.0\n",
            "  Batch 4182/8000, Loss: 0.11690650880336761, Accuracy: 1.0\n",
            "  Batch 4183/8000, Loss: 0.949977695941925, Accuracy: 0.0\n",
            "  Batch 4184/8000, Loss: 0.5980870127677917, Accuracy: 1.0\n",
            "  Batch 4185/8000, Loss: 1.5908119678497314, Accuracy: 0.0\n",
            "  Batch 4186/8000, Loss: 0.11767521500587463, Accuracy: 1.0\n",
            "  Batch 4187/8000, Loss: 2.06298565864563, Accuracy: 0.0\n",
            "  Batch 4188/8000, Loss: 0.11510150879621506, Accuracy: 1.0\n",
            "  Batch 4189/8000, Loss: 0.11667238175868988, Accuracy: 1.0\n",
            "  Batch 4190/8000, Loss: 0.4010336995124817, Accuracy: 1.0\n",
            "  Batch 4191/8000, Loss: 0.4112955927848816, Accuracy: 1.0\n",
            "  Batch 4192/8000, Loss: 0.5353590250015259, Accuracy: 1.0\n",
            "  Batch 4193/8000, Loss: 0.11502569913864136, Accuracy: 1.0\n",
            "  Batch 4194/8000, Loss: 1.078239917755127, Accuracy: 0.0\n",
            "  Batch 4195/8000, Loss: 0.4002166986465454, Accuracy: 1.0\n",
            "  Batch 4196/8000, Loss: 0.3972078859806061, Accuracy: 1.0\n",
            "  Batch 4197/8000, Loss: 1.5503475666046143, Accuracy: 0.0\n",
            "  Batch 4198/8000, Loss: 0.3840674161911011, Accuracy: 1.0\n",
            "  Batch 4199/8000, Loss: 0.38325947523117065, Accuracy: 1.0\n",
            "  Batch 4200/8000, Loss: 0.26329320669174194, Accuracy: 1.0\n",
            "  Batch 4201/8000, Loss: 0.3693806827068329, Accuracy: 1.0\n",
            "  Batch 4202/8000, Loss: 0.41666239500045776, Accuracy: 1.0\n",
            "  Batch 4203/8000, Loss: 0.38073113560676575, Accuracy: 1.0\n",
            "  Batch 4204/8000, Loss: 1.2648383378982544, Accuracy: 0.0\n",
            "  Batch 4205/8000, Loss: 0.628252387046814, Accuracy: 1.0\n",
            "  Batch 4206/8000, Loss: 0.3788394331932068, Accuracy: 1.0\n",
            "  Batch 4207/8000, Loss: 0.4354665279388428, Accuracy: 1.0\n",
            "  Batch 4208/8000, Loss: 1.0012410879135132, Accuracy: 0.0\n",
            "  Batch 4209/8000, Loss: 0.1181219145655632, Accuracy: 1.0\n",
            "  Batch 4210/8000, Loss: 1.055722713470459, Accuracy: 0.0\n",
            "  Batch 4211/8000, Loss: 0.46268221735954285, Accuracy: 1.0\n",
            "  Batch 4212/8000, Loss: 0.12091577798128128, Accuracy: 1.0\n",
            "  Batch 4213/8000, Loss: 0.5697393417358398, Accuracy: 1.0\n",
            "  Batch 4214/8000, Loss: 0.12851263582706451, Accuracy: 1.0\n",
            "  Batch 4215/8000, Loss: 0.6056607961654663, Accuracy: 1.0\n",
            "  Batch 4216/8000, Loss: 0.1225476935505867, Accuracy: 1.0\n",
            "  Batch 4217/8000, Loss: 0.40113142132759094, Accuracy: 1.0\n",
            "  Batch 4218/8000, Loss: 0.6205553412437439, Accuracy: 1.0\n",
            "  Batch 4219/8000, Loss: 1.1936312913894653, Accuracy: 0.0\n",
            "  Batch 4220/8000, Loss: 1.6275559663772583, Accuracy: 0.0\n",
            "  Batch 4221/8000, Loss: 0.11494467407464981, Accuracy: 1.0\n",
            "  Batch 4222/8000, Loss: 1.1905874013900757, Accuracy: 0.0\n",
            "  Batch 4223/8000, Loss: 0.8119857907295227, Accuracy: 0.0\n",
            "  Batch 4224/8000, Loss: 0.3779400885105133, Accuracy: 1.0\n",
            "  Batch 4225/8000, Loss: 0.9986092448234558, Accuracy: 0.0\n",
            "  Batch 4226/8000, Loss: 0.3004649877548218, Accuracy: 1.0\n",
            "  Batch 4227/8000, Loss: 0.5717124342918396, Accuracy: 1.0\n",
            "  Batch 4228/8000, Loss: 0.37951990962028503, Accuracy: 1.0\n",
            "  Batch 4229/8000, Loss: 0.4414738416671753, Accuracy: 1.0\n",
            "  Batch 4230/8000, Loss: 0.37211382389068604, Accuracy: 1.0\n",
            "  Batch 4231/8000, Loss: 0.4433785080909729, Accuracy: 1.0\n",
            "  Batch 4232/8000, Loss: 1.0605928897857666, Accuracy: 0.0\n",
            "  Batch 4233/8000, Loss: 0.4666171669960022, Accuracy: 1.0\n",
            "  Batch 4234/8000, Loss: 0.11481770873069763, Accuracy: 1.0\n",
            "  Batch 4235/8000, Loss: 1.3441375494003296, Accuracy: 0.0\n",
            "  Batch 4236/8000, Loss: 0.9973058104515076, Accuracy: 0.0\n",
            "  Batch 4237/8000, Loss: 1.2833436727523804, Accuracy: 0.0\n",
            "  Batch 4238/8000, Loss: 0.11862500011920929, Accuracy: 1.0\n",
            "  Batch 4239/8000, Loss: 0.3868482708930969, Accuracy: 1.0\n",
            "  Batch 4240/8000, Loss: 0.4122622609138489, Accuracy: 1.0\n",
            "  Batch 4241/8000, Loss: 0.11593273282051086, Accuracy: 1.0\n",
            "  Batch 4242/8000, Loss: 0.8355861902236938, Accuracy: 0.0\n",
            "  Batch 4243/8000, Loss: 0.11809936165809631, Accuracy: 1.0\n",
            "  Batch 4244/8000, Loss: 0.409084677696228, Accuracy: 1.0\n",
            "  Batch 4245/8000, Loss: 0.5093992352485657, Accuracy: 1.0\n",
            "  Batch 4246/8000, Loss: 0.4331606328487396, Accuracy: 1.0\n",
            "  Batch 4247/8000, Loss: 0.5363543629646301, Accuracy: 1.0\n",
            "  Batch 4248/8000, Loss: 0.11473003029823303, Accuracy: 1.0\n",
            "  Batch 4249/8000, Loss: 0.8122323751449585, Accuracy: 0.0\n",
            "  Batch 4250/8000, Loss: 0.465998113155365, Accuracy: 1.0\n",
            "  Batch 4251/8000, Loss: 0.3986276090145111, Accuracy: 1.0\n",
            "  Batch 4252/8000, Loss: 0.38820597529411316, Accuracy: 1.0\n",
            "  Batch 4253/8000, Loss: 1.5914902687072754, Accuracy: 0.0\n",
            "  Batch 4254/8000, Loss: 0.42636922001838684, Accuracy: 1.0\n",
            "  Batch 4255/8000, Loss: 0.5328152179718018, Accuracy: 1.0\n",
            "  Batch 4256/8000, Loss: 0.35411572456359863, Accuracy: 1.0\n",
            "  Batch 4257/8000, Loss: 0.8955067992210388, Accuracy: 0.0\n",
            "  Batch 4258/8000, Loss: 0.7652621865272522, Accuracy: 1.0\n",
            "  Batch 4259/8000, Loss: 0.8084774613380432, Accuracy: 0.0\n",
            "  Batch 4260/8000, Loss: 0.5765509605407715, Accuracy: 1.0\n",
            "  Batch 4261/8000, Loss: 0.11700029671192169, Accuracy: 1.0\n",
            "  Batch 4262/8000, Loss: 0.11459020525217056, Accuracy: 1.0\n",
            "  Batch 4263/8000, Loss: 0.3581618666648865, Accuracy: 1.0\n",
            "  Batch 4264/8000, Loss: 0.4780671000480652, Accuracy: 1.0\n",
            "  Batch 4265/8000, Loss: 0.6833646893501282, Accuracy: 1.0\n",
            "  Batch 4266/8000, Loss: 0.3871440291404724, Accuracy: 1.0\n",
            "  Batch 4267/8000, Loss: 0.11462558060884476, Accuracy: 1.0\n",
            "  Batch 4268/8000, Loss: 0.11811652779579163, Accuracy: 1.0\n",
            "  Batch 4269/8000, Loss: 0.9983262419700623, Accuracy: 0.0\n",
            "  Batch 4270/8000, Loss: 0.3624754548072815, Accuracy: 1.0\n",
            "  Batch 4271/8000, Loss: 0.1164930984377861, Accuracy: 1.0\n",
            "  Batch 4272/8000, Loss: 0.5267612338066101, Accuracy: 1.0\n",
            "  Batch 4273/8000, Loss: 1.4720247983932495, Accuracy: 0.0\n",
            "  Batch 4274/8000, Loss: 0.11561089754104614, Accuracy: 1.0\n",
            "  Batch 4275/8000, Loss: 0.11459164321422577, Accuracy: 1.0\n",
            "  Batch 4276/8000, Loss: 0.11592672765254974, Accuracy: 1.0\n",
            "  Batch 4277/8000, Loss: 1.1964136362075806, Accuracy: 0.0\n",
            "  Batch 4278/8000, Loss: 0.11450102925300598, Accuracy: 1.0\n",
            "  Batch 4279/8000, Loss: 0.11771343648433685, Accuracy: 1.0\n",
            "  Batch 4280/8000, Loss: 0.9750404953956604, Accuracy: 0.0\n",
            "  Batch 4281/8000, Loss: 0.11524086445569992, Accuracy: 1.0\n",
            "  Batch 4282/8000, Loss: 0.11448083817958832, Accuracy: 1.0\n",
            "  Batch 4283/8000, Loss: 0.5911626219749451, Accuracy: 1.0\n",
            "  Batch 4284/8000, Loss: 1.4918745756149292, Accuracy: 0.0\n",
            "  Batch 4285/8000, Loss: 0.11617307364940643, Accuracy: 1.0\n",
            "  Batch 4286/8000, Loss: 0.11447102576494217, Accuracy: 1.0\n",
            "  Batch 4287/8000, Loss: 0.5848327279090881, Accuracy: 1.0\n",
            "  Batch 4288/8000, Loss: 0.4188414216041565, Accuracy: 1.0\n",
            "  Batch 4289/8000, Loss: 0.11952445656061172, Accuracy: 1.0\n",
            "  Batch 4290/8000, Loss: 0.4170283079147339, Accuracy: 1.0\n",
            "  Batch 4291/8000, Loss: 0.5399309396743774, Accuracy: 1.0\n",
            "  Batch 4292/8000, Loss: 0.6052538752555847, Accuracy: 1.0\n",
            "  Batch 4293/8000, Loss: 0.11955225467681885, Accuracy: 1.0\n",
            "  Batch 4294/8000, Loss: 0.8338484168052673, Accuracy: 0.0\n",
            "  Batch 4295/8000, Loss: 0.11440728604793549, Accuracy: 1.0\n",
            "  Batch 4296/8000, Loss: 1.5134333372116089, Accuracy: 0.0\n",
            "  Batch 4297/8000, Loss: 0.11500749737024307, Accuracy: 1.0\n",
            "  Batch 4298/8000, Loss: 0.11438477784395218, Accuracy: 1.0\n",
            "  Batch 4299/8000, Loss: 0.12396025657653809, Accuracy: 1.0\n",
            "  Batch 4300/8000, Loss: 1.1596765518188477, Accuracy: 0.0\n",
            "  Batch 4301/8000, Loss: 0.4841872453689575, Accuracy: 1.0\n",
            "  Batch 4302/8000, Loss: 1.0156242847442627, Accuracy: 0.0\n",
            "  Batch 4303/8000, Loss: 0.4127100706100464, Accuracy: 1.0\n",
            "  Batch 4304/8000, Loss: 0.3943232297897339, Accuracy: 1.0\n",
            "  Batch 4305/8000, Loss: 0.11497363448143005, Accuracy: 1.0\n",
            "  Batch 4306/8000, Loss: 1.6138744354248047, Accuracy: 0.0\n",
            "  Batch 4307/8000, Loss: 1.6460747718811035, Accuracy: 0.0\n",
            "  Batch 4308/8000, Loss: 0.4653417468070984, Accuracy: 1.0\n",
            "  Batch 4309/8000, Loss: 0.3873650133609772, Accuracy: 1.0\n",
            "  Batch 4310/8000, Loss: 0.9368389248847961, Accuracy: 0.0\n",
            "  Batch 4311/8000, Loss: 0.7709570527076721, Accuracy: 1.0\n",
            "  Batch 4312/8000, Loss: 0.4926690459251404, Accuracy: 1.0\n",
            "  Batch 4313/8000, Loss: 0.3581485450267792, Accuracy: 1.0\n",
            "  Batch 4314/8000, Loss: 0.1147976964712143, Accuracy: 1.0\n",
            "  Batch 4315/8000, Loss: 1.55008864402771, Accuracy: 0.0\n",
            "  Batch 4316/8000, Loss: 0.1172352284193039, Accuracy: 1.0\n",
            "  Batch 4317/8000, Loss: 0.1438494622707367, Accuracy: 1.0\n",
            "  Batch 4318/8000, Loss: 0.11729282140731812, Accuracy: 1.0\n",
            "  Batch 4319/8000, Loss: 0.4143458604812622, Accuracy: 1.0\n",
            "  Batch 4320/8000, Loss: 0.4393157958984375, Accuracy: 1.0\n",
            "  Batch 4321/8000, Loss: 0.47155818343162537, Accuracy: 1.0\n",
            "  Batch 4322/8000, Loss: 0.11852394044399261, Accuracy: 1.0\n",
            "  Batch 4323/8000, Loss: 0.35798022150993347, Accuracy: 1.0\n",
            "  Batch 4324/8000, Loss: 0.3501446843147278, Accuracy: 1.0\n",
            "  Batch 4325/8000, Loss: 0.34222105145454407, Accuracy: 1.0\n",
            "  Batch 4326/8000, Loss: 0.5715429782867432, Accuracy: 1.0\n",
            "  Batch 4327/8000, Loss: 0.35124051570892334, Accuracy: 1.0\n",
            "  Batch 4328/8000, Loss: 0.11795715987682343, Accuracy: 1.0\n",
            "  Batch 4329/8000, Loss: 0.11838897317647934, Accuracy: 1.0\n",
            "  Batch 4330/8000, Loss: 0.3186990022659302, Accuracy: 1.0\n",
            "  Batch 4331/8000, Loss: 0.3460288345813751, Accuracy: 1.0\n",
            "  Batch 4332/8000, Loss: 0.8322337865829468, Accuracy: 0.0\n",
            "  Batch 4333/8000, Loss: 0.35922372341156006, Accuracy: 1.0\n",
            "  Batch 4334/8000, Loss: 0.4961438477039337, Accuracy: 1.0\n",
            "  Batch 4335/8000, Loss: 0.11412176489830017, Accuracy: 1.0\n",
            "  Batch 4336/8000, Loss: 1.7029629945755005, Accuracy: 0.0\n",
            "  Batch 4337/8000, Loss: 0.384502112865448, Accuracy: 1.0\n",
            "  Batch 4338/8000, Loss: 0.11643391847610474, Accuracy: 1.0\n",
            "  Batch 4339/8000, Loss: 0.3410922884941101, Accuracy: 1.0\n",
            "  Batch 4340/8000, Loss: 0.3212744891643524, Accuracy: 1.0\n",
            "  Batch 4341/8000, Loss: 0.1290135681629181, Accuracy: 1.0\n",
            "  Batch 4342/8000, Loss: 0.36306506395339966, Accuracy: 1.0\n",
            "  Batch 4343/8000, Loss: 0.3140656054019928, Accuracy: 1.0\n",
            "  Batch 4344/8000, Loss: 0.2838008403778076, Accuracy: 1.0\n",
            "  Batch 4345/8000, Loss: 0.5738584995269775, Accuracy: 1.0\n",
            "  Batch 4346/8000, Loss: 0.3642002046108246, Accuracy: 1.0\n",
            "  Batch 4347/8000, Loss: 1.452871322631836, Accuracy: 0.0\n",
            "  Batch 4348/8000, Loss: 0.4359596073627472, Accuracy: 1.0\n",
            "  Batch 4349/8000, Loss: 0.6829135417938232, Accuracy: 1.0\n",
            "  Batch 4350/8000, Loss: 0.2876984179019928, Accuracy: 1.0\n",
            "  Batch 4351/8000, Loss: 0.441686749458313, Accuracy: 1.0\n",
            "  Batch 4352/8000, Loss: 1.9762030839920044, Accuracy: 0.0\n",
            "  Batch 4353/8000, Loss: 1.8064883947372437, Accuracy: 0.0\n",
            "  Batch 4354/8000, Loss: 0.9604751467704773, Accuracy: 0.0\n",
            "  Batch 4355/8000, Loss: 0.4712544083595276, Accuracy: 1.0\n",
            "  Batch 4356/8000, Loss: 0.3113071918487549, Accuracy: 1.0\n",
            "  Batch 4357/8000, Loss: 0.7620716094970703, Accuracy: 1.0\n",
            "  Batch 4358/8000, Loss: 0.37304821610450745, Accuracy: 1.0\n",
            "  Batch 4359/8000, Loss: 0.336326003074646, Accuracy: 1.0\n",
            "  Batch 4360/8000, Loss: 1.8790538311004639, Accuracy: 0.0\n",
            "  Batch 4361/8000, Loss: 0.11531919240951538, Accuracy: 1.0\n",
            "  Batch 4362/8000, Loss: 0.9009656310081482, Accuracy: 0.0\n",
            "  Batch 4363/8000, Loss: 0.7261108160018921, Accuracy: 1.0\n",
            "  Batch 4364/8000, Loss: 1.635728359222412, Accuracy: 0.0\n",
            "  Batch 4365/8000, Loss: 0.32726216316223145, Accuracy: 1.0\n",
            "  Batch 4366/8000, Loss: 0.1230650246143341, Accuracy: 1.0\n",
            "  Batch 4367/8000, Loss: 1.1819461584091187, Accuracy: 0.0\n",
            "  Batch 4368/8000, Loss: 0.11606087535619736, Accuracy: 1.0\n",
            "  Batch 4369/8000, Loss: 0.6534667015075684, Accuracy: 1.0\n",
            "  Batch 4370/8000, Loss: 0.34421637654304504, Accuracy: 1.0\n",
            "  Batch 4371/8000, Loss: 0.37935560941696167, Accuracy: 1.0\n",
            "  Batch 4372/8000, Loss: 0.5976649522781372, Accuracy: 1.0\n",
            "  Batch 4373/8000, Loss: 0.3406256437301636, Accuracy: 1.0\n",
            "  Batch 4374/8000, Loss: 0.11489757895469666, Accuracy: 1.0\n",
            "  Batch 4375/8000, Loss: 0.11385232210159302, Accuracy: 1.0\n",
            "  Batch 4376/8000, Loss: 0.4331439435482025, Accuracy: 1.0\n",
            "  Batch 4377/8000, Loss: 1.3840546607971191, Accuracy: 0.0\n",
            "  Batch 4378/8000, Loss: 0.6984753608703613, Accuracy: 1.0\n",
            "  Batch 4379/8000, Loss: 0.38578420877456665, Accuracy: 1.0\n",
            "  Batch 4380/8000, Loss: 0.11419174820184708, Accuracy: 1.0\n",
            "  Batch 4381/8000, Loss: 0.3626280128955841, Accuracy: 1.0\n",
            "  Batch 4382/8000, Loss: 0.14128611981868744, Accuracy: 1.0\n",
            "  Batch 4383/8000, Loss: 1.0095452070236206, Accuracy: 0.0\n",
            "  Batch 4384/8000, Loss: 0.11379881948232651, Accuracy: 1.0\n",
            "  Batch 4385/8000, Loss: 0.11682190001010895, Accuracy: 1.0\n",
            "  Batch 4386/8000, Loss: 0.11415558308362961, Accuracy: 1.0\n",
            "  Batch 4387/8000, Loss: 1.6858646869659424, Accuracy: 0.0\n",
            "  Batch 4388/8000, Loss: 0.35559967160224915, Accuracy: 1.0\n",
            "  Batch 4389/8000, Loss: 0.6678991913795471, Accuracy: 1.0\n",
            "  Batch 4390/8000, Loss: 0.11499271541833878, Accuracy: 1.0\n",
            "  Batch 4391/8000, Loss: 0.4426650106906891, Accuracy: 1.0\n",
            "  Batch 4392/8000, Loss: 1.381076693534851, Accuracy: 0.0\n",
            "  Batch 4393/8000, Loss: 0.34560325741767883, Accuracy: 1.0\n",
            "  Batch 4394/8000, Loss: 0.1137612909078598, Accuracy: 1.0\n",
            "  Batch 4395/8000, Loss: 0.5413208603858948, Accuracy: 1.0\n",
            "  Batch 4396/8000, Loss: 0.11371690034866333, Accuracy: 1.0\n",
            "  Batch 4397/8000, Loss: 0.46200209856033325, Accuracy: 1.0\n",
            "  Batch 4398/8000, Loss: 0.638293445110321, Accuracy: 1.0\n",
            "  Batch 4399/8000, Loss: 0.44667932391166687, Accuracy: 1.0\n",
            "  Batch 4400/8000, Loss: 0.6718513369560242, Accuracy: 1.0\n",
            "  Batch 4401/8000, Loss: 1.032544732093811, Accuracy: 0.0\n",
            "  Batch 4402/8000, Loss: 1.2536640167236328, Accuracy: 0.0\n",
            "  Batch 4403/8000, Loss: 0.11454187333583832, Accuracy: 1.0\n",
            "  Batch 4404/8000, Loss: 0.4946802258491516, Accuracy: 1.0\n",
            "  Batch 4405/8000, Loss: 0.11408393085002899, Accuracy: 1.0\n",
            "  Batch 4406/8000, Loss: 0.36540237069129944, Accuracy: 1.0\n",
            "  Batch 4407/8000, Loss: 0.11941532045602798, Accuracy: 1.0\n",
            "  Batch 4408/8000, Loss: 2.347970962524414, Accuracy: 0.0\n",
            "  Batch 4409/8000, Loss: 0.6171537637710571, Accuracy: 1.0\n",
            "  Batch 4410/8000, Loss: 1.627923607826233, Accuracy: 0.0\n",
            "  Batch 4411/8000, Loss: 1.4884767532348633, Accuracy: 0.0\n",
            "  Batch 4412/8000, Loss: 0.3716312050819397, Accuracy: 1.0\n",
            "  Batch 4413/8000, Loss: 0.1154925525188446, Accuracy: 1.0\n",
            "  Batch 4414/8000, Loss: 0.11668726801872253, Accuracy: 1.0\n",
            "  Batch 4415/8000, Loss: 1.550602912902832, Accuracy: 0.0\n",
            "  Batch 4416/8000, Loss: 0.1191568374633789, Accuracy: 1.0\n",
            "  Batch 4417/8000, Loss: 0.11356456577777863, Accuracy: 1.0\n",
            "  Batch 4418/8000, Loss: 0.12222141027450562, Accuracy: 1.0\n",
            "  Batch 4419/8000, Loss: 1.6416795253753662, Accuracy: 0.0\n",
            "  Batch 4420/8000, Loss: 0.1136416494846344, Accuracy: 1.0\n",
            "  Batch 4421/8000, Loss: 0.12273712456226349, Accuracy: 1.0\n",
            "  Batch 4422/8000, Loss: 0.458334743976593, Accuracy: 1.0\n",
            "  Batch 4423/8000, Loss: 0.4118390679359436, Accuracy: 1.0\n",
            "  Batch 4424/8000, Loss: 1.5534650087356567, Accuracy: 0.0\n",
            "  Batch 4425/8000, Loss: 0.4805448651313782, Accuracy: 1.0\n",
            "  Batch 4426/8000, Loss: 0.5079581141471863, Accuracy: 1.0\n",
            "  Batch 4427/8000, Loss: 0.4198336899280548, Accuracy: 1.0\n",
            "  Batch 4428/8000, Loss: 0.7859146595001221, Accuracy: 1.0\n",
            "  Batch 4429/8000, Loss: 0.12294331938028336, Accuracy: 1.0\n",
            "  Batch 4430/8000, Loss: 0.11347343027591705, Accuracy: 1.0\n",
            "  Batch 4431/8000, Loss: 0.4524817168712616, Accuracy: 1.0\n",
            "  Batch 4432/8000, Loss: 0.377110093832016, Accuracy: 1.0\n",
            "  Batch 4433/8000, Loss: 0.68552565574646, Accuracy: 1.0\n",
            "  Batch 4434/8000, Loss: 1.5814646482467651, Accuracy: 0.0\n",
            "  Batch 4435/8000, Loss: 0.1181458830833435, Accuracy: 1.0\n",
            "  Batch 4436/8000, Loss: 0.37710103392601013, Accuracy: 1.0\n",
            "  Batch 4437/8000, Loss: 0.12964409589767456, Accuracy: 1.0\n",
            "  Batch 4438/8000, Loss: 0.4333924651145935, Accuracy: 1.0\n",
            "  Batch 4439/8000, Loss: 0.11346127092838287, Accuracy: 1.0\n",
            "  Batch 4440/8000, Loss: 0.11824501305818558, Accuracy: 1.0\n",
            "  Batch 4441/8000, Loss: 0.522361695766449, Accuracy: 1.0\n",
            "  Batch 4442/8000, Loss: 0.40422651171684265, Accuracy: 1.0\n",
            "  Batch 4443/8000, Loss: 0.36846834421157837, Accuracy: 1.0\n",
            "  Batch 4444/8000, Loss: 0.38253509998321533, Accuracy: 1.0\n",
            "  Batch 4445/8000, Loss: 0.5746649503707886, Accuracy: 1.0\n",
            "  Batch 4446/8000, Loss: 0.6180616617202759, Accuracy: 1.0\n",
            "  Batch 4447/8000, Loss: 0.43094658851623535, Accuracy: 1.0\n",
            "  Batch 4448/8000, Loss: 0.9328697919845581, Accuracy: 0.0\n",
            "  Batch 4449/8000, Loss: 0.11337487399578094, Accuracy: 1.0\n",
            "  Batch 4450/8000, Loss: 0.3835653066635132, Accuracy: 1.0\n",
            "  Batch 4451/8000, Loss: 1.585676908493042, Accuracy: 0.0\n",
            "  Batch 4452/8000, Loss: 1.222301721572876, Accuracy: 0.0\n",
            "  Batch 4453/8000, Loss: 0.45818454027175903, Accuracy: 1.0\n",
            "  Batch 4454/8000, Loss: 0.48075079917907715, Accuracy: 1.0\n",
            "  Batch 4455/8000, Loss: 0.1185355857014656, Accuracy: 1.0\n",
            "  Batch 4456/8000, Loss: 0.12510456144809723, Accuracy: 1.0\n",
            "  Batch 4457/8000, Loss: 0.11664045602083206, Accuracy: 1.0\n",
            "  Batch 4458/8000, Loss: 1.261811375617981, Accuracy: 0.0\n",
            "  Batch 4459/8000, Loss: 1.0756767988204956, Accuracy: 0.0\n",
            "  Batch 4460/8000, Loss: 0.12031576037406921, Accuracy: 1.0\n",
            "  Batch 4461/8000, Loss: 0.30920878052711487, Accuracy: 1.0\n",
            "  Batch 4462/8000, Loss: 0.3472951352596283, Accuracy: 1.0\n",
            "  Batch 4463/8000, Loss: 0.3851669132709503, Accuracy: 1.0\n",
            "  Batch 4464/8000, Loss: 0.5264050960540771, Accuracy: 1.0\n",
            "  Batch 4465/8000, Loss: 0.37247782945632935, Accuracy: 1.0\n",
            "  Batch 4466/8000, Loss: 0.5359684824943542, Accuracy: 1.0\n",
            "  Batch 4467/8000, Loss: 0.7562286853790283, Accuracy: 1.0\n",
            "  Batch 4468/8000, Loss: 0.6443907618522644, Accuracy: 1.0\n",
            "  Batch 4469/8000, Loss: 0.5225430727005005, Accuracy: 1.0\n",
            "  Batch 4470/8000, Loss: 0.3850392699241638, Accuracy: 1.0\n",
            "  Batch 4471/8000, Loss: 0.1161251962184906, Accuracy: 1.0\n",
            "  Batch 4472/8000, Loss: 0.3546503186225891, Accuracy: 1.0\n",
            "  Batch 4473/8000, Loss: 0.398726224899292, Accuracy: 1.0\n",
            "  Batch 4474/8000, Loss: 0.39324429631233215, Accuracy: 1.0\n",
            "  Batch 4475/8000, Loss: 0.7314382791519165, Accuracy: 1.0\n",
            "  Batch 4476/8000, Loss: 0.3765811324119568, Accuracy: 1.0\n",
            "  Batch 4477/8000, Loss: 0.11428841948509216, Accuracy: 1.0\n",
            "  Batch 4478/8000, Loss: 0.3388616144657135, Accuracy: 1.0\n",
            "  Batch 4479/8000, Loss: 0.11336340010166168, Accuracy: 1.0\n",
            "  Batch 4480/8000, Loss: 0.9521947503089905, Accuracy: 0.0\n",
            "  Batch 4481/8000, Loss: 1.8486955165863037, Accuracy: 0.0\n",
            "  Batch 4482/8000, Loss: 1.6797226667404175, Accuracy: 0.0\n",
            "  Batch 4483/8000, Loss: 0.8410829901695251, Accuracy: 0.0\n",
            "  Batch 4484/8000, Loss: 0.5593368411064148, Accuracy: 1.0\n",
            "  Batch 4485/8000, Loss: 0.15719124674797058, Accuracy: 1.0\n",
            "  Batch 4486/8000, Loss: 0.11664842069149017, Accuracy: 1.0\n",
            "  Batch 4487/8000, Loss: 0.5735833644866943, Accuracy: 1.0\n",
            "  Batch 4488/8000, Loss: 1.3500726222991943, Accuracy: 0.0\n",
            "  Batch 4489/8000, Loss: 0.3314399719238281, Accuracy: 1.0\n",
            "  Batch 4490/8000, Loss: 0.1212238222360611, Accuracy: 1.0\n",
            "  Batch 4491/8000, Loss: 1.6446188688278198, Accuracy: 0.0\n",
            "  Batch 4492/8000, Loss: 0.4251319169998169, Accuracy: 1.0\n",
            "  Batch 4493/8000, Loss: 0.33181488513946533, Accuracy: 1.0\n",
            "  Batch 4494/8000, Loss: 0.3417429029941559, Accuracy: 1.0\n",
            "  Batch 4495/8000, Loss: 0.558312714099884, Accuracy: 1.0\n",
            "  Batch 4496/8000, Loss: 0.44896888732910156, Accuracy: 1.0\n",
            "  Batch 4497/8000, Loss: 0.39210453629493713, Accuracy: 1.0\n",
            "  Batch 4498/8000, Loss: 0.4347565174102783, Accuracy: 1.0\n",
            "  Batch 4499/8000, Loss: 0.32829269766807556, Accuracy: 1.0\n",
            "  Batch 4500/8000, Loss: 0.11806992441415787, Accuracy: 1.0\n",
            "  Batch 4501/8000, Loss: 0.4870189130306244, Accuracy: 1.0\n",
            "  Batch 4502/8000, Loss: 0.11946292221546173, Accuracy: 1.0\n",
            "  Batch 4503/8000, Loss: 0.3121911585330963, Accuracy: 1.0\n",
            "  Batch 4504/8000, Loss: 0.7313537001609802, Accuracy: 1.0\n",
            "  Batch 4505/8000, Loss: 0.40195414423942566, Accuracy: 1.0\n",
            "  Batch 4506/8000, Loss: 0.4666603207588196, Accuracy: 1.0\n",
            "  Batch 4507/8000, Loss: 0.1139795184135437, Accuracy: 1.0\n",
            "  Batch 4508/8000, Loss: 1.0523087978363037, Accuracy: 0.0\n",
            "  Batch 4509/8000, Loss: 0.28009018301963806, Accuracy: 1.0\n",
            "  Batch 4510/8000, Loss: 1.6119595766067505, Accuracy: 0.0\n",
            "  Batch 4511/8000, Loss: 0.3164663314819336, Accuracy: 1.0\n",
            "  Batch 4512/8000, Loss: 0.36243847012519836, Accuracy: 1.0\n",
            "  Batch 4513/8000, Loss: 1.807633638381958, Accuracy: 0.0\n",
            "  Batch 4514/8000, Loss: 0.3252057135105133, Accuracy: 1.0\n",
            "  Batch 4515/8000, Loss: 0.29442912340164185, Accuracy: 1.0\n",
            "  Batch 4516/8000, Loss: 0.3300137519836426, Accuracy: 1.0\n",
            "  Batch 4517/8000, Loss: 0.3247649073600769, Accuracy: 1.0\n",
            "  Batch 4518/8000, Loss: 0.33274662494659424, Accuracy: 1.0\n",
            "  Batch 4519/8000, Loss: 1.5790889263153076, Accuracy: 0.0\n",
            "  Batch 4520/8000, Loss: 0.11851328611373901, Accuracy: 1.0\n",
            "  Batch 4521/8000, Loss: 0.4305534362792969, Accuracy: 1.0\n",
            "  Batch 4522/8000, Loss: 0.8553487658500671, Accuracy: 0.0\n",
            "  Batch 4523/8000, Loss: 0.605526328086853, Accuracy: 1.0\n",
            "  Batch 4524/8000, Loss: 0.32805562019348145, Accuracy: 1.0\n",
            "  Batch 4525/8000, Loss: 0.1178765594959259, Accuracy: 1.0\n",
            "  Batch 4526/8000, Loss: 0.11908281594514847, Accuracy: 1.0\n",
            "  Batch 4527/8000, Loss: 1.6506977081298828, Accuracy: 0.0\n",
            "  Batch 4528/8000, Loss: 0.9342803955078125, Accuracy: 0.0\n",
            "  Batch 4529/8000, Loss: 0.11625245213508606, Accuracy: 1.0\n",
            "  Batch 4530/8000, Loss: 0.7256250977516174, Accuracy: 1.0\n",
            "  Batch 4531/8000, Loss: 0.11659644544124603, Accuracy: 1.0\n",
            "  Batch 4532/8000, Loss: 0.6571310758590698, Accuracy: 1.0\n",
            "  Batch 4533/8000, Loss: 0.3533720374107361, Accuracy: 1.0\n",
            "  Batch 4534/8000, Loss: 0.38929665088653564, Accuracy: 1.0\n",
            "  Batch 4535/8000, Loss: 1.0242990255355835, Accuracy: 0.0\n",
            "  Batch 4536/8000, Loss: 0.3439474403858185, Accuracy: 1.0\n",
            "  Batch 4537/8000, Loss: 1.6657441854476929, Accuracy: 0.0\n",
            "  Batch 4538/8000, Loss: 0.11486870795488358, Accuracy: 1.0\n",
            "  Batch 4539/8000, Loss: 0.3526178002357483, Accuracy: 1.0\n",
            "  Batch 4540/8000, Loss: 0.33419331908226013, Accuracy: 1.0\n",
            "  Batch 4541/8000, Loss: 0.4769595265388489, Accuracy: 1.0\n",
            "  Batch 4542/8000, Loss: 0.3593672215938568, Accuracy: 1.0\n",
            "  Batch 4543/8000, Loss: 0.46712228655815125, Accuracy: 1.0\n",
            "  Batch 4544/8000, Loss: 0.11355981230735779, Accuracy: 1.0\n",
            "  Batch 4545/8000, Loss: 0.11431884765625, Accuracy: 1.0\n",
            "  Batch 4546/8000, Loss: 0.42658042907714844, Accuracy: 1.0\n",
            "  Batch 4547/8000, Loss: 0.11479973793029785, Accuracy: 1.0\n",
            "  Batch 4548/8000, Loss: 0.33943361043930054, Accuracy: 1.0\n",
            "  Batch 4549/8000, Loss: 0.3500993847846985, Accuracy: 1.0\n",
            "  Batch 4550/8000, Loss: 0.3361673355102539, Accuracy: 1.0\n",
            "  Batch 4551/8000, Loss: 0.47054532170295715, Accuracy: 1.0\n",
            "  Batch 4552/8000, Loss: 0.32211634516716003, Accuracy: 1.0\n",
            "  Batch 4553/8000, Loss: 0.34894415736198425, Accuracy: 1.0\n",
            "  Batch 4554/8000, Loss: 1.7868939638137817, Accuracy: 0.0\n",
            "  Batch 4555/8000, Loss: 1.7072503566741943, Accuracy: 0.0\n",
            "  Batch 4556/8000, Loss: 0.11388324201107025, Accuracy: 1.0\n",
            "  Batch 4557/8000, Loss: 0.11476883292198181, Accuracy: 1.0\n",
            "  Batch 4558/8000, Loss: 0.5812497138977051, Accuracy: 1.0\n",
            "  Batch 4559/8000, Loss: 0.3201899528503418, Accuracy: 1.0\n",
            "  Batch 4560/8000, Loss: 1.3128600120544434, Accuracy: 0.0\n",
            "  Batch 4561/8000, Loss: 0.11342795193195343, Accuracy: 1.0\n",
            "  Batch 4562/8000, Loss: 0.5922783017158508, Accuracy: 1.0\n",
            "  Batch 4563/8000, Loss: 0.3194722533226013, Accuracy: 1.0\n",
            "  Batch 4564/8000, Loss: 0.33520686626434326, Accuracy: 1.0\n",
            "  Batch 4565/8000, Loss: 1.6196690797805786, Accuracy: 0.0\n",
            "  Batch 4566/8000, Loss: 0.41297924518585205, Accuracy: 1.0\n",
            "  Batch 4567/8000, Loss: 1.2227972745895386, Accuracy: 0.0\n",
            "  Batch 4568/8000, Loss: 0.37110939621925354, Accuracy: 1.0\n",
            "  Batch 4569/8000, Loss: 0.9736752510070801, Accuracy: 0.0\n",
            "  Batch 4570/8000, Loss: 0.4849928319454193, Accuracy: 1.0\n",
            "  Batch 4571/8000, Loss: 0.4504644572734833, Accuracy: 1.0\n",
            "  Batch 4572/8000, Loss: 0.4680178761482239, Accuracy: 1.0\n",
            "  Batch 4573/8000, Loss: 0.11339759826660156, Accuracy: 1.0\n",
            "  Batch 4574/8000, Loss: 0.4058327376842499, Accuracy: 1.0\n",
            "  Batch 4575/8000, Loss: 0.4931885004043579, Accuracy: 1.0\n",
            "  Batch 4576/8000, Loss: 0.35765182971954346, Accuracy: 1.0\n",
            "  Batch 4577/8000, Loss: 1.484724998474121, Accuracy: 0.0\n",
            "  Batch 4578/8000, Loss: 0.40634018182754517, Accuracy: 1.0\n",
            "  Batch 4579/8000, Loss: 1.6599129438400269, Accuracy: 0.0\n",
            "  Batch 4580/8000, Loss: 0.11254743486642838, Accuracy: 1.0\n",
            "  Batch 4581/8000, Loss: 0.113087959587574, Accuracy: 1.0\n",
            "  Batch 4582/8000, Loss: 0.4203263223171234, Accuracy: 1.0\n",
            "  Batch 4583/8000, Loss: 1.4305603504180908, Accuracy: 0.0\n",
            "  Batch 4584/8000, Loss: 0.8850342631340027, Accuracy: 0.0\n",
            "  Batch 4585/8000, Loss: 0.3506230115890503, Accuracy: 1.0\n",
            "  Batch 4586/8000, Loss: 0.45026445388793945, Accuracy: 1.0\n",
            "  Batch 4587/8000, Loss: 0.4102975130081177, Accuracy: 1.0\n",
            "  Batch 4588/8000, Loss: 0.1289852112531662, Accuracy: 1.0\n",
            "  Batch 4589/8000, Loss: 1.0580953359603882, Accuracy: 0.0\n",
            "  Batch 4590/8000, Loss: 0.4935632646083832, Accuracy: 1.0\n",
            "  Batch 4591/8000, Loss: 1.077917456626892, Accuracy: 0.0\n",
            "  Batch 4592/8000, Loss: 3.3143861293792725, Accuracy: 0.0\n",
            "  Batch 4593/8000, Loss: 0.5603312253952026, Accuracy: 1.0\n",
            "  Batch 4594/8000, Loss: 0.42746415734291077, Accuracy: 1.0\n",
            "  Batch 4595/8000, Loss: 0.4252990186214447, Accuracy: 1.0\n",
            "  Batch 4596/8000, Loss: 1.6441826820373535, Accuracy: 0.0\n",
            "  Batch 4597/8000, Loss: 0.11246445775032043, Accuracy: 1.0\n",
            "  Batch 4598/8000, Loss: 1.551025390625, Accuracy: 0.0\n",
            "  Batch 4599/8000, Loss: 0.4978010952472687, Accuracy: 1.0\n",
            "  Batch 4600/8000, Loss: 0.12545941770076752, Accuracy: 1.0\n",
            "  Batch 4601/8000, Loss: 0.38645198941230774, Accuracy: 1.0\n",
            "  Batch 4602/8000, Loss: 0.16366294026374817, Accuracy: 1.0\n",
            "  Batch 4603/8000, Loss: 0.3561447560787201, Accuracy: 1.0\n",
            "  Batch 4604/8000, Loss: 0.12265348434448242, Accuracy: 1.0\n",
            "  Batch 4605/8000, Loss: 1.0913459062576294, Accuracy: 0.0\n",
            "  Batch 4606/8000, Loss: 0.5189282298088074, Accuracy: 1.0\n",
            "  Batch 4607/8000, Loss: 0.9302771091461182, Accuracy: 0.0\n",
            "  Batch 4608/8000, Loss: 0.6191908121109009, Accuracy: 1.0\n",
            "  Batch 4609/8000, Loss: 0.4546634256839752, Accuracy: 1.0\n",
            "  Batch 4610/8000, Loss: 0.11730492115020752, Accuracy: 1.0\n",
            "  Batch 4611/8000, Loss: 0.5862391591072083, Accuracy: 1.0\n",
            "  Batch 4612/8000, Loss: 0.42884594202041626, Accuracy: 1.0\n",
            "  Batch 4613/8000, Loss: 1.4974613189697266, Accuracy: 0.0\n",
            "  Batch 4614/8000, Loss: 0.11230554431676865, Accuracy: 1.0\n",
            "  Batch 4615/8000, Loss: 0.39395400881767273, Accuracy: 1.0\n",
            "  Batch 4616/8000, Loss: 1.5739109516143799, Accuracy: 0.0\n",
            "  Batch 4617/8000, Loss: 0.36651989817619324, Accuracy: 1.0\n",
            "  Batch 4618/8000, Loss: 0.4278467893600464, Accuracy: 1.0\n",
            "  Batch 4619/8000, Loss: 0.43346086144447327, Accuracy: 1.0\n",
            "  Batch 4620/8000, Loss: 0.8399673700332642, Accuracy: 0.0\n",
            "  Batch 4621/8000, Loss: 0.373309850692749, Accuracy: 1.0\n",
            "  Batch 4622/8000, Loss: 0.48177510499954224, Accuracy: 1.0\n",
            "  Batch 4623/8000, Loss: 0.15680503845214844, Accuracy: 1.0\n",
            "  Batch 4624/8000, Loss: 0.5628988742828369, Accuracy: 1.0\n",
            "  Batch 4625/8000, Loss: 0.35241103172302246, Accuracy: 1.0\n",
            "  Batch 4626/8000, Loss: 0.40451475977897644, Accuracy: 1.0\n",
            "  Batch 4627/8000, Loss: 0.11993551254272461, Accuracy: 1.0\n",
            "  Batch 4628/8000, Loss: 0.6528956890106201, Accuracy: 1.0\n",
            "  Batch 4629/8000, Loss: 0.3489551842212677, Accuracy: 1.0\n",
            "  Batch 4630/8000, Loss: 1.6541235446929932, Accuracy: 0.0\n",
            "  Batch 4631/8000, Loss: 0.1169758141040802, Accuracy: 1.0\n",
            "  Batch 4632/8000, Loss: 0.11680905520915985, Accuracy: 1.0\n",
            "  Batch 4633/8000, Loss: 0.3986867666244507, Accuracy: 1.0\n",
            "  Batch 4634/8000, Loss: 0.36224231123924255, Accuracy: 1.0\n",
            "  Batch 4635/8000, Loss: 0.45108091831207275, Accuracy: 1.0\n",
            "  Batch 4636/8000, Loss: 0.6868674755096436, Accuracy: 1.0\n",
            "  Batch 4637/8000, Loss: 0.5948424935340881, Accuracy: 1.0\n",
            "  Batch 4638/8000, Loss: 0.16276535391807556, Accuracy: 1.0\n",
            "  Batch 4639/8000, Loss: 0.34267139434814453, Accuracy: 1.0\n",
            "  Batch 4640/8000, Loss: 1.0174081325531006, Accuracy: 0.0\n",
            "  Batch 4641/8000, Loss: 1.3171027898788452, Accuracy: 0.0\n",
            "  Batch 4642/8000, Loss: 1.7103841304779053, Accuracy: 0.0\n",
            "  Batch 4643/8000, Loss: 0.5576349496841431, Accuracy: 1.0\n",
            "  Batch 4644/8000, Loss: 0.5785932540893555, Accuracy: 1.0\n",
            "  Batch 4645/8000, Loss: 0.5855363011360168, Accuracy: 1.0\n",
            "  Batch 4646/8000, Loss: 1.5933516025543213, Accuracy: 0.0\n",
            "  Batch 4647/8000, Loss: 0.3407858610153198, Accuracy: 1.0\n",
            "  Batch 4648/8000, Loss: 1.5555187463760376, Accuracy: 0.0\n",
            "  Batch 4649/8000, Loss: 1.5505098104476929, Accuracy: 0.0\n",
            "  Batch 4650/8000, Loss: 0.4497160315513611, Accuracy: 1.0\n",
            "  Batch 4651/8000, Loss: 0.37053290009498596, Accuracy: 1.0\n",
            "  Batch 4652/8000, Loss: 0.11218279600143433, Accuracy: 1.0\n",
            "  Batch 4653/8000, Loss: 0.4098478853702545, Accuracy: 1.0\n",
            "  Batch 4654/8000, Loss: 1.4417643547058105, Accuracy: 0.0\n",
            "  Batch 4655/8000, Loss: 0.44335442781448364, Accuracy: 1.0\n",
            "  Batch 4656/8000, Loss: 0.24687813222408295, Accuracy: 1.0\n",
            "  Batch 4657/8000, Loss: 0.472854346036911, Accuracy: 1.0\n",
            "  Batch 4658/8000, Loss: 0.43838030099868774, Accuracy: 1.0\n",
            "  Batch 4659/8000, Loss: 0.41014307737350464, Accuracy: 1.0\n",
            "  Batch 4660/8000, Loss: 0.40088096261024475, Accuracy: 1.0\n",
            "  Batch 4661/8000, Loss: 0.40272057056427, Accuracy: 1.0\n",
            "  Batch 4662/8000, Loss: 0.9945551753044128, Accuracy: 0.0\n",
            "  Batch 4663/8000, Loss: 0.6781445741653442, Accuracy: 1.0\n",
            "  Batch 4664/8000, Loss: 1.3171672821044922, Accuracy: 0.0\n",
            "  Batch 4665/8000, Loss: 0.94618159532547, Accuracy: 0.0\n",
            "  Batch 4666/8000, Loss: 0.6545752286911011, Accuracy: 1.0\n",
            "  Batch 4667/8000, Loss: 0.42655694484710693, Accuracy: 1.0\n",
            "  Batch 4668/8000, Loss: 0.752555787563324, Accuracy: 1.0\n",
            "  Batch 4669/8000, Loss: 0.376079261302948, Accuracy: 1.0\n",
            "  Batch 4670/8000, Loss: 0.11206769198179245, Accuracy: 1.0\n",
            "  Batch 4671/8000, Loss: 0.5355932712554932, Accuracy: 1.0\n",
            "  Batch 4672/8000, Loss: 0.6710872650146484, Accuracy: 1.0\n",
            "  Batch 4673/8000, Loss: 0.11222704499959946, Accuracy: 1.0\n",
            "  Batch 4674/8000, Loss: 0.1119358092546463, Accuracy: 1.0\n",
            "  Batch 4675/8000, Loss: 0.52898108959198, Accuracy: 1.0\n",
            "  Batch 4676/8000, Loss: 0.687729001045227, Accuracy: 1.0\n",
            "  Batch 4677/8000, Loss: 0.4594630002975464, Accuracy: 1.0\n",
            "  Batch 4678/8000, Loss: 1.5215600728988647, Accuracy: 0.0\n",
            "  Batch 4679/8000, Loss: 1.5884315967559814, Accuracy: 0.0\n",
            "  Batch 4680/8000, Loss: 0.965172529220581, Accuracy: 0.0\n",
            "  Batch 4681/8000, Loss: 0.475177526473999, Accuracy: 1.0\n",
            "  Batch 4682/8000, Loss: 0.6812458038330078, Accuracy: 1.0\n",
            "  Batch 4683/8000, Loss: 0.7559732794761658, Accuracy: 1.0\n",
            "  Batch 4684/8000, Loss: 0.7267490029335022, Accuracy: 1.0\n",
            "  Batch 4685/8000, Loss: 0.11389508843421936, Accuracy: 1.0\n",
            "  Batch 4686/8000, Loss: 0.47658979892730713, Accuracy: 1.0\n",
            "  Batch 4687/8000, Loss: 0.47353288531303406, Accuracy: 1.0\n",
            "  Batch 4688/8000, Loss: 0.417982816696167, Accuracy: 1.0\n",
            "  Batch 4689/8000, Loss: 0.48391541838645935, Accuracy: 1.0\n",
            "  Batch 4690/8000, Loss: 0.7910962104797363, Accuracy: 1.0\n",
            "  Batch 4691/8000, Loss: 0.11189986765384674, Accuracy: 1.0\n",
            "  Batch 4692/8000, Loss: 0.48166969418525696, Accuracy: 1.0\n",
            "  Batch 4693/8000, Loss: 0.4968477189540863, Accuracy: 1.0\n",
            "  Batch 4694/8000, Loss: 0.40733927488327026, Accuracy: 1.0\n",
            "  Batch 4695/8000, Loss: 0.11215952038764954, Accuracy: 1.0\n",
            "  Batch 4696/8000, Loss: 0.4668746292591095, Accuracy: 1.0\n",
            "  Batch 4697/8000, Loss: 0.123310886323452, Accuracy: 1.0\n",
            "  Batch 4698/8000, Loss: 0.1434703767299652, Accuracy: 1.0\n",
            "  Batch 4699/8000, Loss: 0.4851720333099365, Accuracy: 1.0\n",
            "  Batch 4700/8000, Loss: 0.6976428627967834, Accuracy: 1.0\n",
            "  Batch 4701/8000, Loss: 1.2555856704711914, Accuracy: 0.0\n",
            "  Batch 4702/8000, Loss: 0.11816239356994629, Accuracy: 1.0\n",
            "  Batch 4703/8000, Loss: 0.42070838809013367, Accuracy: 1.0\n",
            "  Batch 4704/8000, Loss: 0.6348002552986145, Accuracy: 1.0\n",
            "  Batch 4705/8000, Loss: 0.11565829068422318, Accuracy: 1.0\n",
            "  Batch 4706/8000, Loss: 0.40044349431991577, Accuracy: 1.0\n",
            "  Batch 4707/8000, Loss: 0.11387168616056442, Accuracy: 1.0\n",
            "  Batch 4708/8000, Loss: 1.0214611291885376, Accuracy: 0.0\n",
            "  Batch 4709/8000, Loss: 0.4679107367992401, Accuracy: 1.0\n",
            "  Batch 4710/8000, Loss: 0.4437815546989441, Accuracy: 1.0\n",
            "  Batch 4711/8000, Loss: 0.6485000848770142, Accuracy: 1.0\n",
            "  Batch 4712/8000, Loss: 0.11767213046550751, Accuracy: 1.0\n",
            "  Batch 4713/8000, Loss: 0.11412560939788818, Accuracy: 1.0\n",
            "  Batch 4714/8000, Loss: 0.11716844141483307, Accuracy: 1.0\n",
            "  Batch 4715/8000, Loss: 1.5265198945999146, Accuracy: 0.0\n",
            "  Batch 4716/8000, Loss: 0.3497409522533417, Accuracy: 1.0\n",
            "  Batch 4717/8000, Loss: 0.11170719563961029, Accuracy: 1.0\n",
            "  Batch 4718/8000, Loss: 0.34011244773864746, Accuracy: 1.0\n",
            "  Batch 4719/8000, Loss: 0.36442750692367554, Accuracy: 1.0\n",
            "  Batch 4720/8000, Loss: 0.31861674785614014, Accuracy: 1.0\n",
            "  Batch 4721/8000, Loss: 0.353934109210968, Accuracy: 1.0\n",
            "  Batch 4722/8000, Loss: 0.3941740095615387, Accuracy: 1.0\n",
            "  Batch 4723/8000, Loss: 0.11491858959197998, Accuracy: 1.0\n",
            "  Batch 4724/8000, Loss: 1.149886131286621, Accuracy: 0.0\n",
            "  Batch 4725/8000, Loss: 0.43816226720809937, Accuracy: 1.0\n",
            "  Batch 4726/8000, Loss: 1.0422977209091187, Accuracy: 0.0\n",
            "  Batch 4727/8000, Loss: 1.1842169761657715, Accuracy: 0.0\n",
            "  Batch 4728/8000, Loss: 0.12949350476264954, Accuracy: 1.0\n",
            "  Batch 4729/8000, Loss: 0.11169726401567459, Accuracy: 1.0\n",
            "  Batch 4730/8000, Loss: 0.3255305290222168, Accuracy: 1.0\n",
            "  Batch 4731/8000, Loss: 0.4422012269496918, Accuracy: 1.0\n",
            "  Batch 4732/8000, Loss: 0.45415472984313965, Accuracy: 1.0\n",
            "  Batch 4733/8000, Loss: 1.6743342876434326, Accuracy: 0.0\n",
            "  Batch 4734/8000, Loss: 1.297847032546997, Accuracy: 0.0\n",
            "  Batch 4735/8000, Loss: 1.4670640230178833, Accuracy: 0.0\n",
            "  Batch 4736/8000, Loss: 0.6911312937736511, Accuracy: 1.0\n",
            "  Batch 4737/8000, Loss: 0.7723155617713928, Accuracy: 1.0\n",
            "  Batch 4738/8000, Loss: 0.4922676086425781, Accuracy: 1.0\n",
            "  Batch 4739/8000, Loss: 0.4979981482028961, Accuracy: 1.0\n",
            "  Batch 4740/8000, Loss: 0.4953842759132385, Accuracy: 1.0\n",
            "  Batch 4741/8000, Loss: 1.4011508226394653, Accuracy: 0.0\n",
            "  Batch 4742/8000, Loss: 1.6016907691955566, Accuracy: 0.0\n",
            "  Batch 4743/8000, Loss: 0.5253682732582092, Accuracy: 1.0\n",
            "  Batch 4744/8000, Loss: 1.2199149131774902, Accuracy: 0.0\n",
            "  Batch 4745/8000, Loss: 0.11911750584840775, Accuracy: 1.0\n",
            "  Batch 4746/8000, Loss: 0.40552055835723877, Accuracy: 1.0\n",
            "  Batch 4747/8000, Loss: 0.3920785188674927, Accuracy: 1.0\n",
            "  Batch 4748/8000, Loss: 1.5311659574508667, Accuracy: 0.0\n",
            "  Batch 4749/8000, Loss: 0.11338795721530914, Accuracy: 1.0\n",
            "  Batch 4750/8000, Loss: 0.5625528693199158, Accuracy: 1.0\n",
            "  Batch 4751/8000, Loss: 0.1152384951710701, Accuracy: 1.0\n",
            "  Batch 4752/8000, Loss: 1.428888201713562, Accuracy: 0.0\n",
            "  Batch 4753/8000, Loss: 0.465681791305542, Accuracy: 1.0\n",
            "  Batch 4754/8000, Loss: 0.5465403199195862, Accuracy: 1.0\n",
            "  Batch 4755/8000, Loss: 0.19553518295288086, Accuracy: 1.0\n",
            "  Batch 4756/8000, Loss: 0.9117803573608398, Accuracy: 0.0\n",
            "  Batch 4757/8000, Loss: 1.3675696849822998, Accuracy: 0.0\n",
            "  Batch 4758/8000, Loss: 0.12229540199041367, Accuracy: 1.0\n",
            "  Batch 4759/8000, Loss: 0.6509171724319458, Accuracy: 1.0\n",
            "  Batch 4760/8000, Loss: 0.11658470332622528, Accuracy: 1.0\n",
            "  Batch 4761/8000, Loss: 0.7327976822853088, Accuracy: 1.0\n",
            "  Batch 4762/8000, Loss: 0.43570369482040405, Accuracy: 1.0\n",
            "  Batch 4763/8000, Loss: 0.1245303750038147, Accuracy: 1.0\n",
            "  Batch 4764/8000, Loss: 0.9031217694282532, Accuracy: 0.0\n",
            "  Batch 4765/8000, Loss: 1.3507250547409058, Accuracy: 0.0\n",
            "  Batch 4766/8000, Loss: 0.9622008800506592, Accuracy: 0.0\n",
            "  Batch 4767/8000, Loss: 1.1093947887420654, Accuracy: 0.0\n",
            "  Batch 4768/8000, Loss: 0.4724668860435486, Accuracy: 1.0\n",
            "  Batch 4769/8000, Loss: 0.11138308048248291, Accuracy: 1.0\n",
            "  Batch 4770/8000, Loss: 0.972920835018158, Accuracy: 0.0\n",
            "  Batch 4771/8000, Loss: 0.475426584482193, Accuracy: 1.0\n",
            "  Batch 4772/8000, Loss: 0.5383075475692749, Accuracy: 1.0\n",
            "  Batch 4773/8000, Loss: 1.1280730962753296, Accuracy: 0.0\n",
            "  Batch 4774/8000, Loss: 0.6763924956321716, Accuracy: 1.0\n",
            "  Batch 4775/8000, Loss: 0.4909929633140564, Accuracy: 1.0\n",
            "  Batch 4776/8000, Loss: 0.5210784673690796, Accuracy: 1.0\n",
            "  Batch 4777/8000, Loss: 0.44803741574287415, Accuracy: 1.0\n",
            "  Batch 4778/8000, Loss: 0.18736478686332703, Accuracy: 1.0\n",
            "  Batch 4779/8000, Loss: 0.462444007396698, Accuracy: 1.0\n",
            "  Batch 4780/8000, Loss: 0.17626339197158813, Accuracy: 1.0\n",
            "  Batch 4781/8000, Loss: 0.5585854649543762, Accuracy: 1.0\n",
            "  Batch 4782/8000, Loss: 0.5701956152915955, Accuracy: 1.0\n",
            "  Batch 4783/8000, Loss: 1.1734304428100586, Accuracy: 0.0\n",
            "  Batch 4784/8000, Loss: 0.16744309663772583, Accuracy: 1.0\n",
            "  Batch 4785/8000, Loss: 0.11161269247531891, Accuracy: 1.0\n",
            "  Batch 4786/8000, Loss: 1.5040838718414307, Accuracy: 0.0\n",
            "  Batch 4787/8000, Loss: 0.11364522576332092, Accuracy: 1.0\n",
            "  Batch 4788/8000, Loss: 0.11257050931453705, Accuracy: 1.0\n",
            "  Batch 4789/8000, Loss: 0.6908136010169983, Accuracy: 1.0\n",
            "  Batch 4790/8000, Loss: 0.41263070702552795, Accuracy: 1.0\n",
            "  Batch 4791/8000, Loss: 0.39028167724609375, Accuracy: 1.0\n",
            "  Batch 4792/8000, Loss: 1.0716195106506348, Accuracy: 0.0\n",
            "  Batch 4793/8000, Loss: 0.46681490540504456, Accuracy: 1.0\n",
            "  Batch 4794/8000, Loss: 0.7043322324752808, Accuracy: 1.0\n",
            "  Batch 4795/8000, Loss: 0.1144939735531807, Accuracy: 1.0\n",
            "  Batch 4796/8000, Loss: 0.42829617857933044, Accuracy: 1.0\n",
            "  Batch 4797/8000, Loss: 0.13833637535572052, Accuracy: 1.0\n",
            "  Batch 4798/8000, Loss: 0.4010842740535736, Accuracy: 1.0\n",
            "  Batch 4799/8000, Loss: 0.6746826767921448, Accuracy: 1.0\n",
            "  Batch 4800/8000, Loss: 0.1157522052526474, Accuracy: 1.0\n",
            "  Batch 4801/8000, Loss: 0.48629331588745117, Accuracy: 1.0\n",
            "  Batch 4802/8000, Loss: 0.8643110990524292, Accuracy: 0.0\n",
            "  Batch 4803/8000, Loss: 0.6066793203353882, Accuracy: 1.0\n",
            "  Batch 4804/8000, Loss: 0.11221784353256226, Accuracy: 1.0\n",
            "  Batch 4805/8000, Loss: 0.40542978048324585, Accuracy: 1.0\n",
            "  Batch 4806/8000, Loss: 0.11354218423366547, Accuracy: 1.0\n",
            "  Batch 4807/8000, Loss: 1.1067010164260864, Accuracy: 0.0\n",
            "  Batch 4808/8000, Loss: 0.34417909383773804, Accuracy: 1.0\n",
            "  Batch 4809/8000, Loss: 0.11464862525463104, Accuracy: 1.0\n",
            "  Batch 4810/8000, Loss: 0.401569664478302, Accuracy: 1.0\n",
            "  Batch 4811/8000, Loss: 0.1182754784822464, Accuracy: 1.0\n",
            "  Batch 4812/8000, Loss: 0.1111481785774231, Accuracy: 1.0\n",
            "  Batch 4813/8000, Loss: 0.1336645483970642, Accuracy: 1.0\n",
            "  Batch 4814/8000, Loss: 0.11318536102771759, Accuracy: 1.0\n",
            "  Batch 4815/8000, Loss: 0.3469269871711731, Accuracy: 1.0\n",
            "  Batch 4816/8000, Loss: 0.11369707435369492, Accuracy: 1.0\n",
            "  Batch 4817/8000, Loss: 0.35980209708213806, Accuracy: 1.0\n",
            "  Batch 4818/8000, Loss: 0.33829885721206665, Accuracy: 1.0\n",
            "  Batch 4819/8000, Loss: 0.3612288236618042, Accuracy: 1.0\n",
            "  Batch 4820/8000, Loss: 0.3515281677246094, Accuracy: 1.0\n",
            "  Batch 4821/8000, Loss: 0.3136465847492218, Accuracy: 1.0\n",
            "  Batch 4822/8000, Loss: 1.5730870962142944, Accuracy: 0.0\n",
            "  Batch 4823/8000, Loss: 0.11291226744651794, Accuracy: 1.0\n",
            "  Batch 4824/8000, Loss: 0.11359454691410065, Accuracy: 1.0\n",
            "  Batch 4825/8000, Loss: 0.24907472729682922, Accuracy: 1.0\n",
            "  Batch 4826/8000, Loss: 0.11639615893363953, Accuracy: 1.0\n",
            "  Batch 4827/8000, Loss: 0.4582700729370117, Accuracy: 1.0\n",
            "  Batch 4828/8000, Loss: 0.5775108933448792, Accuracy: 1.0\n",
            "  Batch 4829/8000, Loss: 0.1234053522348404, Accuracy: 1.0\n",
            "  Batch 4830/8000, Loss: 0.36116617918014526, Accuracy: 1.0\n",
            "  Batch 4831/8000, Loss: 0.4679909348487854, Accuracy: 1.0\n",
            "  Batch 4832/8000, Loss: 1.77557373046875, Accuracy: 0.0\n",
            "  Batch 4833/8000, Loss: 0.32841619849205017, Accuracy: 1.0\n",
            "  Batch 4834/8000, Loss: 0.11313831806182861, Accuracy: 1.0\n",
            "  Batch 4835/8000, Loss: 1.8900275230407715, Accuracy: 0.0\n",
            "  Batch 4836/8000, Loss: 0.49173620343208313, Accuracy: 1.0\n",
            "  Batch 4837/8000, Loss: 0.3374360203742981, Accuracy: 1.0\n",
            "  Batch 4838/8000, Loss: 0.31306472420692444, Accuracy: 1.0\n",
            "  Batch 4839/8000, Loss: 0.9328789710998535, Accuracy: 0.0\n",
            "  Batch 4840/8000, Loss: 0.28663986921310425, Accuracy: 1.0\n",
            "  Batch 4841/8000, Loss: 0.11187044531106949, Accuracy: 1.0\n",
            "  Batch 4842/8000, Loss: 1.1912919282913208, Accuracy: 0.0\n",
            "  Batch 4843/8000, Loss: 0.3315116763114929, Accuracy: 1.0\n",
            "  Batch 4844/8000, Loss: 0.3935425579547882, Accuracy: 1.0\n",
            "  Batch 4845/8000, Loss: 0.5572526454925537, Accuracy: 1.0\n",
            "  Batch 4846/8000, Loss: 0.4786382019519806, Accuracy: 1.0\n",
            "  Batch 4847/8000, Loss: 0.8311646580696106, Accuracy: 0.0\n",
            "  Batch 4848/8000, Loss: 0.28084462881088257, Accuracy: 1.0\n",
            "  Batch 4849/8000, Loss: 0.12280292063951492, Accuracy: 1.0\n",
            "  Batch 4850/8000, Loss: 0.11165151000022888, Accuracy: 1.0\n",
            "  Batch 4851/8000, Loss: 0.11337637901306152, Accuracy: 1.0\n",
            "  Batch 4852/8000, Loss: 0.6032500267028809, Accuracy: 1.0\n",
            "  Batch 4853/8000, Loss: 0.3157498240470886, Accuracy: 1.0\n",
            "  Batch 4854/8000, Loss: 0.3362029790878296, Accuracy: 1.0\n",
            "  Batch 4855/8000, Loss: 0.4299007058143616, Accuracy: 1.0\n",
            "  Batch 4856/8000, Loss: 0.4016721844673157, Accuracy: 1.0\n",
            "  Batch 4857/8000, Loss: 0.9138301610946655, Accuracy: 0.0\n",
            "  Batch 4858/8000, Loss: 1.4312512874603271, Accuracy: 0.0\n",
            "  Batch 4859/8000, Loss: 0.11465334892272949, Accuracy: 1.0\n",
            "  Batch 4860/8000, Loss: 1.7007725238800049, Accuracy: 0.0\n",
            "  Batch 4861/8000, Loss: 0.11218644678592682, Accuracy: 1.0\n",
            "  Batch 4862/8000, Loss: 0.46294325590133667, Accuracy: 1.0\n",
            "  Batch 4863/8000, Loss: 0.26744458079338074, Accuracy: 1.0\n",
            "  Batch 4864/8000, Loss: 0.30286896228790283, Accuracy: 1.0\n",
            "  Batch 4865/8000, Loss: 0.26534539461135864, Accuracy: 1.0\n",
            "  Batch 4866/8000, Loss: 0.2683316469192505, Accuracy: 1.0\n",
            "  Batch 4867/8000, Loss: 0.28284719586372375, Accuracy: 1.0\n",
            "  Batch 4868/8000, Loss: 0.2618321180343628, Accuracy: 1.0\n",
            "  Batch 4869/8000, Loss: 0.4800415337085724, Accuracy: 1.0\n",
            "  Batch 4870/8000, Loss: 1.7765635251998901, Accuracy: 0.0\n",
            "  Batch 4871/8000, Loss: 0.49971669912338257, Accuracy: 1.0\n",
            "  Batch 4872/8000, Loss: 0.39187687635421753, Accuracy: 1.0\n",
            "  Batch 4873/8000, Loss: 0.16054072976112366, Accuracy: 1.0\n",
            "  Batch 4874/8000, Loss: 0.11078250408172607, Accuracy: 1.0\n",
            "  Batch 4875/8000, Loss: 1.662713646888733, Accuracy: 0.0\n",
            "  Batch 4876/8000, Loss: 0.12113872170448303, Accuracy: 1.0\n",
            "  Batch 4877/8000, Loss: 0.110751211643219, Accuracy: 1.0\n",
            "  Batch 4878/8000, Loss: 1.8405513763427734, Accuracy: 0.0\n",
            "  Batch 4879/8000, Loss: 1.9090912342071533, Accuracy: 0.0\n",
            "  Batch 4880/8000, Loss: 0.497370183467865, Accuracy: 1.0\n",
            "  Batch 4881/8000, Loss: 0.3526974618434906, Accuracy: 1.0\n",
            "  Batch 4882/8000, Loss: 0.27564677596092224, Accuracy: 1.0\n",
            "  Batch 4883/8000, Loss: 0.519045352935791, Accuracy: 1.0\n",
            "  Batch 4884/8000, Loss: 0.3128281235694885, Accuracy: 1.0\n",
            "  Batch 4885/8000, Loss: 0.42715105414390564, Accuracy: 1.0\n",
            "  Batch 4886/8000, Loss: 0.2971307337284088, Accuracy: 1.0\n",
            "  Batch 4887/8000, Loss: 0.3336251676082611, Accuracy: 1.0\n",
            "  Batch 4888/8000, Loss: 0.11195513606071472, Accuracy: 1.0\n",
            "  Batch 4889/8000, Loss: 1.7292706966400146, Accuracy: 0.0\n",
            "  Batch 4890/8000, Loss: 1.3021321296691895, Accuracy: 0.0\n",
            "  Batch 4891/8000, Loss: 1.9307382106781006, Accuracy: 0.0\n",
            "  Batch 4892/8000, Loss: 1.756319522857666, Accuracy: 0.0\n",
            "  Batch 4893/8000, Loss: 0.3102250397205353, Accuracy: 1.0\n",
            "  Batch 4894/8000, Loss: 0.4265648126602173, Accuracy: 1.0\n",
            "  Batch 4895/8000, Loss: 0.34454458951950073, Accuracy: 1.0\n",
            "  Batch 4896/8000, Loss: 0.3999258577823639, Accuracy: 1.0\n",
            "  Batch 4897/8000, Loss: 0.3468005061149597, Accuracy: 1.0\n",
            "  Batch 4898/8000, Loss: 1.3115185499191284, Accuracy: 0.0\n",
            "  Batch 4899/8000, Loss: 0.37059521675109863, Accuracy: 1.0\n",
            "  Batch 4900/8000, Loss: 0.3591596186161041, Accuracy: 1.0\n",
            "  Batch 4901/8000, Loss: 0.3543023467063904, Accuracy: 1.0\n",
            "  Batch 4902/8000, Loss: 0.4896242320537567, Accuracy: 1.0\n",
            "  Batch 4903/8000, Loss: 0.39164450764656067, Accuracy: 1.0\n",
            "  Batch 4904/8000, Loss: 0.41204556822776794, Accuracy: 1.0\n",
            "  Batch 4905/8000, Loss: 0.3809545636177063, Accuracy: 1.0\n",
            "  Batch 4906/8000, Loss: 3.176083564758301, Accuracy: 0.0\n",
            "  Batch 4907/8000, Loss: 0.37060678005218506, Accuracy: 1.0\n",
            "  Batch 4908/8000, Loss: 1.6176890134811401, Accuracy: 0.0\n",
            "  Batch 4909/8000, Loss: 0.3332943618297577, Accuracy: 1.0\n",
            "  Batch 4910/8000, Loss: 0.35361602902412415, Accuracy: 1.0\n",
            "  Batch 4911/8000, Loss: 0.3361670970916748, Accuracy: 1.0\n",
            "  Batch 4912/8000, Loss: 0.11645705997943878, Accuracy: 1.0\n",
            "  Batch 4913/8000, Loss: 0.11465542018413544, Accuracy: 1.0\n",
            "  Batch 4914/8000, Loss: 0.11864472925662994, Accuracy: 1.0\n",
            "  Batch 4915/8000, Loss: 0.3306202292442322, Accuracy: 1.0\n",
            "  Batch 4916/8000, Loss: 0.32148146629333496, Accuracy: 1.0\n",
            "  Batch 4917/8000, Loss: 0.3879239857196808, Accuracy: 1.0\n",
            "  Batch 4918/8000, Loss: 0.39008718729019165, Accuracy: 1.0\n",
            "  Batch 4919/8000, Loss: 0.11078894138336182, Accuracy: 1.0\n",
            "  Batch 4920/8000, Loss: 0.3305867910385132, Accuracy: 1.0\n",
            "  Batch 4921/8000, Loss: 0.9992846250534058, Accuracy: 0.0\n",
            "  Batch 4922/8000, Loss: 0.11042134463787079, Accuracy: 1.0\n",
            "  Batch 4923/8000, Loss: 0.12112531065940857, Accuracy: 1.0\n",
            "  Batch 4924/8000, Loss: 0.12546952068805695, Accuracy: 1.0\n",
            "  Batch 4925/8000, Loss: 0.49359330534935, Accuracy: 1.0\n",
            "  Batch 4926/8000, Loss: 0.12450526654720306, Accuracy: 1.0\n",
            "  Batch 4927/8000, Loss: 0.12285691499710083, Accuracy: 1.0\n",
            "  Batch 4928/8000, Loss: 1.9050264358520508, Accuracy: 0.0\n",
            "  Batch 4929/8000, Loss: 1.001579761505127, Accuracy: 0.0\n",
            "  Batch 4930/8000, Loss: 0.317383736371994, Accuracy: 1.0\n",
            "  Batch 4931/8000, Loss: 0.35500699281692505, Accuracy: 1.0\n",
            "  Batch 4932/8000, Loss: 1.181068778038025, Accuracy: 0.0\n",
            "  Batch 4933/8000, Loss: 0.3508007228374481, Accuracy: 1.0\n",
            "  Batch 4934/8000, Loss: 0.12485039979219437, Accuracy: 1.0\n",
            "  Batch 4935/8000, Loss: 0.3822939693927765, Accuracy: 1.0\n",
            "  Batch 4936/8000, Loss: 0.8753238320350647, Accuracy: 0.0\n",
            "  Batch 4937/8000, Loss: 0.11039580404758453, Accuracy: 1.0\n",
            "  Batch 4938/8000, Loss: 1.3697080612182617, Accuracy: 0.0\n",
            "  Batch 4939/8000, Loss: 0.3291480243206024, Accuracy: 1.0\n",
            "  Batch 4940/8000, Loss: 0.8837759494781494, Accuracy: 0.0\n",
            "  Batch 4941/8000, Loss: 0.12454946339130402, Accuracy: 1.0\n",
            "  Batch 4942/8000, Loss: 0.11027668416500092, Accuracy: 1.0\n",
            "  Batch 4943/8000, Loss: 0.12497873604297638, Accuracy: 1.0\n",
            "  Batch 4944/8000, Loss: 0.11110502481460571, Accuracy: 1.0\n",
            "  Batch 4945/8000, Loss: 0.4035499393939972, Accuracy: 1.0\n",
            "  Batch 4946/8000, Loss: 0.3017786145210266, Accuracy: 1.0\n",
            "  Batch 4947/8000, Loss: 0.8161382675170898, Accuracy: 0.0\n",
            "  Batch 4948/8000, Loss: 0.11027452349662781, Accuracy: 1.0\n",
            "  Batch 4949/8000, Loss: 0.11838458478450775, Accuracy: 1.0\n",
            "  Batch 4950/8000, Loss: 0.321925550699234, Accuracy: 1.0\n",
            "  Batch 4951/8000, Loss: 0.11070127040147781, Accuracy: 1.0\n",
            "  Batch 4952/8000, Loss: 1.6787254810333252, Accuracy: 0.0\n",
            "  Batch 4953/8000, Loss: 0.11062001436948776, Accuracy: 1.0\n",
            "  Batch 4954/8000, Loss: 0.30152615904808044, Accuracy: 1.0\n",
            "  Batch 4955/8000, Loss: 0.687846302986145, Accuracy: 1.0\n",
            "  Batch 4956/8000, Loss: 0.20519857108592987, Accuracy: 1.0\n",
            "  Batch 4957/8000, Loss: 1.1728459596633911, Accuracy: 0.0\n",
            "  Batch 4958/8000, Loss: 0.11021959781646729, Accuracy: 1.0\n",
            "  Batch 4959/8000, Loss: 0.31439825892448425, Accuracy: 1.0\n",
            "  Batch 4960/8000, Loss: 1.470024585723877, Accuracy: 0.0\n",
            "  Batch 4961/8000, Loss: 1.7428146600723267, Accuracy: 0.0\n",
            "  Batch 4962/8000, Loss: 0.7563737630844116, Accuracy: 1.0\n",
            "  Batch 4963/8000, Loss: 0.1103137731552124, Accuracy: 1.0\n",
            "  Batch 4964/8000, Loss: 0.11423233151435852, Accuracy: 1.0\n",
            "  Batch 4965/8000, Loss: 0.3449876606464386, Accuracy: 1.0\n",
            "  Batch 4966/8000, Loss: 0.1180274486541748, Accuracy: 1.0\n",
            "  Batch 4967/8000, Loss: 0.7119941711425781, Accuracy: 1.0\n",
            "  Batch 4968/8000, Loss: 0.11427053809165955, Accuracy: 1.0\n",
            "  Batch 4969/8000, Loss: 0.11688697338104248, Accuracy: 1.0\n",
            "  Batch 4970/8000, Loss: 0.39888113737106323, Accuracy: 1.0\n",
            "  Batch 4971/8000, Loss: 0.11290024220943451, Accuracy: 1.0\n",
            "  Batch 4972/8000, Loss: 0.37777867913246155, Accuracy: 1.0\n",
            "  Batch 4973/8000, Loss: 0.11220994591712952, Accuracy: 1.0\n",
            "  Batch 4974/8000, Loss: 1.1092606782913208, Accuracy: 0.0\n",
            "  Batch 4975/8000, Loss: 0.3660697937011719, Accuracy: 1.0\n",
            "  Batch 4976/8000, Loss: 0.4072328209877014, Accuracy: 1.0\n",
            "  Batch 4977/8000, Loss: 0.5889368653297424, Accuracy: 1.0\n",
            "  Batch 4978/8000, Loss: 0.11166691780090332, Accuracy: 1.0\n",
            "  Batch 4979/8000, Loss: 0.8621858358383179, Accuracy: 0.0\n",
            "  Batch 4980/8000, Loss: 0.3499722182750702, Accuracy: 1.0\n",
            "  Batch 4981/8000, Loss: 0.39472538232803345, Accuracy: 1.0\n",
            "  Batch 4982/8000, Loss: 0.11060082912445068, Accuracy: 1.0\n",
            "  Batch 4983/8000, Loss: 0.11742858588695526, Accuracy: 1.0\n",
            "  Batch 4984/8000, Loss: 1.700098991394043, Accuracy: 0.0\n",
            "  Batch 4985/8000, Loss: 1.33780038356781, Accuracy: 0.0\n",
            "  Batch 4986/8000, Loss: 0.8023979067802429, Accuracy: 1.0\n",
            "  Batch 4987/8000, Loss: 0.44439786672592163, Accuracy: 1.0\n",
            "  Batch 4988/8000, Loss: 0.3789312541484833, Accuracy: 1.0\n",
            "  Batch 4989/8000, Loss: 0.48815643787384033, Accuracy: 1.0\n",
            "  Batch 4990/8000, Loss: 1.2894856929779053, Accuracy: 0.0\n",
            "  Batch 4991/8000, Loss: 1.6401880979537964, Accuracy: 0.0\n",
            "  Batch 4992/8000, Loss: 0.14026305079460144, Accuracy: 1.0\n",
            "  Batch 4993/8000, Loss: 0.3597826063632965, Accuracy: 1.0\n",
            "  Batch 4994/8000, Loss: 0.1242755725979805, Accuracy: 1.0\n",
            "  Batch 4995/8000, Loss: 0.9516357183456421, Accuracy: 0.0\n",
            "  Batch 4996/8000, Loss: 0.11432038992643356, Accuracy: 1.0\n",
            "  Batch 4997/8000, Loss: 0.43558138608932495, Accuracy: 1.0\n",
            "  Batch 4998/8000, Loss: 0.4878634512424469, Accuracy: 1.0\n",
            "  Batch 4999/8000, Loss: 0.36358463764190674, Accuracy: 1.0\n",
            "  Batch 5000/8000, Loss: 0.3759996294975281, Accuracy: 1.0\n",
            "  Batch 5001/8000, Loss: 0.9178139567375183, Accuracy: 0.0\n",
            "  Batch 5002/8000, Loss: 0.3680502772331238, Accuracy: 1.0\n",
            "  Batch 5003/8000, Loss: 0.33489012718200684, Accuracy: 1.0\n",
            "  Batch 5004/8000, Loss: 1.422485113143921, Accuracy: 0.0\n",
            "  Batch 5005/8000, Loss: 0.33098146319389343, Accuracy: 1.0\n",
            "  Batch 5006/8000, Loss: 0.4042041003704071, Accuracy: 1.0\n",
            "  Batch 5007/8000, Loss: 0.3475778102874756, Accuracy: 1.0\n",
            "  Batch 5008/8000, Loss: 0.41946303844451904, Accuracy: 1.0\n",
            "  Batch 5009/8000, Loss: 0.12004750221967697, Accuracy: 1.0\n",
            "  Batch 5010/8000, Loss: 0.1981816589832306, Accuracy: 1.0\n",
            "  Batch 5011/8000, Loss: 0.6306513547897339, Accuracy: 1.0\n",
            "  Batch 5012/8000, Loss: 0.5538171529769897, Accuracy: 1.0\n",
            "  Batch 5013/8000, Loss: 1.6825436353683472, Accuracy: 0.0\n",
            "  Batch 5014/8000, Loss: 0.4917764961719513, Accuracy: 1.0\n",
            "  Batch 5015/8000, Loss: 0.9068818092346191, Accuracy: 0.0\n",
            "  Batch 5016/8000, Loss: 1.7509030103683472, Accuracy: 0.0\n",
            "  Batch 5017/8000, Loss: 1.0043126344680786, Accuracy: 0.0\n",
            "  Batch 5018/8000, Loss: 0.3533291220664978, Accuracy: 1.0\n",
            "  Batch 5019/8000, Loss: 0.35178738832473755, Accuracy: 1.0\n",
            "  Batch 5020/8000, Loss: 0.3087449371814728, Accuracy: 1.0\n",
            "  Batch 5021/8000, Loss: 0.1159840077161789, Accuracy: 1.0\n",
            "  Batch 5022/8000, Loss: 0.109915591776371, Accuracy: 1.0\n",
            "  Batch 5023/8000, Loss: 0.3476681113243103, Accuracy: 1.0\n",
            "  Batch 5024/8000, Loss: 0.8132299780845642, Accuracy: 0.0\n",
            "  Batch 5025/8000, Loss: 0.3166092038154602, Accuracy: 1.0\n",
            "  Batch 5026/8000, Loss: 0.37013277411460876, Accuracy: 1.0\n",
            "  Batch 5027/8000, Loss: 0.2756081819534302, Accuracy: 1.0\n",
            "  Batch 5028/8000, Loss: 0.5036858320236206, Accuracy: 1.0\n",
            "  Batch 5029/8000, Loss: 0.7360835075378418, Accuracy: 1.0\n",
            "  Batch 5030/8000, Loss: 0.3290005028247833, Accuracy: 1.0\n",
            "  Batch 5031/8000, Loss: 0.5088441371917725, Accuracy: 1.0\n",
            "  Batch 5032/8000, Loss: 0.30790531635284424, Accuracy: 1.0\n",
            "  Batch 5033/8000, Loss: 1.074021577835083, Accuracy: 0.0\n",
            "  Batch 5034/8000, Loss: 1.3390073776245117, Accuracy: 0.0\n",
            "  Batch 5035/8000, Loss: 0.30980291962623596, Accuracy: 1.0\n",
            "  Batch 5036/8000, Loss: 0.1438627690076828, Accuracy: 1.0\n",
            "  Batch 5037/8000, Loss: 0.915378212928772, Accuracy: 0.0\n",
            "  Batch 5038/8000, Loss: 0.10989657789468765, Accuracy: 1.0\n",
            "  Batch 5039/8000, Loss: 0.1159830093383789, Accuracy: 1.0\n",
            "  Batch 5040/8000, Loss: 0.9001405835151672, Accuracy: 0.0\n",
            "  Batch 5041/8000, Loss: 0.10978333652019501, Accuracy: 1.0\n",
            "  Batch 5042/8000, Loss: 1.1919142007827759, Accuracy: 0.0\n",
            "  Batch 5043/8000, Loss: 1.3581024408340454, Accuracy: 0.0\n",
            "  Batch 5044/8000, Loss: 0.669999897480011, Accuracy: 1.0\n",
            "  Batch 5045/8000, Loss: 0.11506988108158112, Accuracy: 1.0\n",
            "  Batch 5046/8000, Loss: 1.128170132637024, Accuracy: 0.0\n",
            "  Batch 5047/8000, Loss: 1.097050666809082, Accuracy: 0.0\n",
            "  Batch 5048/8000, Loss: 0.11566627025604248, Accuracy: 1.0\n",
            "  Batch 5049/8000, Loss: 0.4330247640609741, Accuracy: 1.0\n",
            "  Batch 5050/8000, Loss: 0.4514343738555908, Accuracy: 1.0\n",
            "  Batch 5051/8000, Loss: 0.35167402029037476, Accuracy: 1.0\n",
            "  Batch 5052/8000, Loss: 0.7661749720573425, Accuracy: 1.0\n",
            "  Batch 5053/8000, Loss: 0.35833632946014404, Accuracy: 1.0\n",
            "  Batch 5054/8000, Loss: 0.684491753578186, Accuracy: 1.0\n",
            "  Batch 5055/8000, Loss: 0.519782304763794, Accuracy: 1.0\n",
            "  Batch 5056/8000, Loss: 0.3891676068305969, Accuracy: 1.0\n",
            "  Batch 5057/8000, Loss: 0.6435325145721436, Accuracy: 1.0\n",
            "  Batch 5058/8000, Loss: 0.11478328704833984, Accuracy: 1.0\n",
            "  Batch 5059/8000, Loss: 0.3488909900188446, Accuracy: 1.0\n",
            "  Batch 5060/8000, Loss: 0.48941153287887573, Accuracy: 1.0\n",
            "  Batch 5061/8000, Loss: 0.48086991906166077, Accuracy: 1.0\n",
            "  Batch 5062/8000, Loss: 0.11161959916353226, Accuracy: 1.0\n",
            "  Batch 5063/8000, Loss: 1.2837622165679932, Accuracy: 0.0\n",
            "  Batch 5064/8000, Loss: 0.32970452308654785, Accuracy: 1.0\n",
            "  Batch 5065/8000, Loss: 0.562784731388092, Accuracy: 1.0\n",
            "  Batch 5066/8000, Loss: 0.7676748633384705, Accuracy: 1.0\n",
            "  Batch 5067/8000, Loss: 0.3556811809539795, Accuracy: 1.0\n",
            "  Batch 5068/8000, Loss: 0.10966247320175171, Accuracy: 1.0\n",
            "  Batch 5069/8000, Loss: 0.11409339308738708, Accuracy: 1.0\n",
            "  Batch 5070/8000, Loss: 0.3603452742099762, Accuracy: 1.0\n",
            "  Batch 5071/8000, Loss: 0.42684945464134216, Accuracy: 1.0\n",
            "  Batch 5072/8000, Loss: 3.948211908340454, Accuracy: 0.0\n",
            "  Batch 5073/8000, Loss: 1.784695029258728, Accuracy: 0.0\n",
            "  Batch 5074/8000, Loss: 0.7650945782661438, Accuracy: 1.0\n",
            "  Batch 5075/8000, Loss: 1.0909396409988403, Accuracy: 0.0\n",
            "  Batch 5076/8000, Loss: 0.48516735434532166, Accuracy: 1.0\n",
            "  Batch 5077/8000, Loss: 0.11921457946300507, Accuracy: 1.0\n",
            "  Batch 5078/8000, Loss: 1.7345091104507446, Accuracy: 0.0\n",
            "  Batch 5079/8000, Loss: 0.5061730146408081, Accuracy: 1.0\n",
            "  Batch 5080/8000, Loss: 0.11659302562475204, Accuracy: 1.0\n",
            "  Batch 5081/8000, Loss: 1.7079033851623535, Accuracy: 0.0\n",
            "  Batch 5082/8000, Loss: 0.14011593163013458, Accuracy: 1.0\n",
            "  Batch 5083/8000, Loss: 0.10981224477291107, Accuracy: 1.0\n",
            "  Batch 5084/8000, Loss: 0.44655996561050415, Accuracy: 1.0\n",
            "  Batch 5085/8000, Loss: 0.10975685715675354, Accuracy: 1.0\n",
            "  Batch 5086/8000, Loss: 1.7489186525344849, Accuracy: 0.0\n",
            "  Batch 5087/8000, Loss: 1.6865754127502441, Accuracy: 0.0\n",
            "  Batch 5088/8000, Loss: 1.5255202054977417, Accuracy: 0.0\n",
            "  Batch 5089/8000, Loss: 0.1335289180278778, Accuracy: 1.0\n",
            "  Batch 5090/8000, Loss: 0.13070057332515717, Accuracy: 1.0\n",
            "  Batch 5091/8000, Loss: 0.8361833691596985, Accuracy: 0.0\n",
            "  Batch 5092/8000, Loss: 0.9499342441558838, Accuracy: 0.0\n",
            "  Batch 5093/8000, Loss: 0.23268188536167145, Accuracy: 1.0\n",
            "  Batch 5094/8000, Loss: 0.5488452315330505, Accuracy: 1.0\n",
            "  Batch 5095/8000, Loss: 0.39491191506385803, Accuracy: 1.0\n",
            "  Batch 5096/8000, Loss: 0.13351061940193176, Accuracy: 1.0\n",
            "  Batch 5097/8000, Loss: 0.5284469127655029, Accuracy: 1.0\n",
            "  Batch 5098/8000, Loss: 1.5037788152694702, Accuracy: 0.0\n",
            "  Batch 5099/8000, Loss: 0.7249265909194946, Accuracy: 1.0\n",
            "  Batch 5100/8000, Loss: 0.19499284029006958, Accuracy: 1.0\n",
            "  Batch 5101/8000, Loss: 0.31245166063308716, Accuracy: 1.0\n",
            "  Batch 5102/8000, Loss: 0.46144983172416687, Accuracy: 1.0\n",
            "  Batch 5103/8000, Loss: 0.5838077068328857, Accuracy: 1.0\n",
            "  Batch 5104/8000, Loss: 1.1196107864379883, Accuracy: 0.0\n",
            "  Batch 5105/8000, Loss: 0.11007285118103027, Accuracy: 1.0\n",
            "  Batch 5106/8000, Loss: 0.10973776876926422, Accuracy: 1.0\n",
            "  Batch 5107/8000, Loss: 0.3996092677116394, Accuracy: 1.0\n",
            "  Batch 5108/8000, Loss: 0.1313679814338684, Accuracy: 1.0\n",
            "  Batch 5109/8000, Loss: 0.4380991756916046, Accuracy: 1.0\n",
            "  Batch 5110/8000, Loss: 0.5707857012748718, Accuracy: 1.0\n",
            "  Batch 5111/8000, Loss: 0.44506531953811646, Accuracy: 1.0\n",
            "  Batch 5112/8000, Loss: 0.5075262784957886, Accuracy: 1.0\n",
            "  Batch 5113/8000, Loss: 0.9386183023452759, Accuracy: 0.0\n",
            "  Batch 5114/8000, Loss: 0.48879486322402954, Accuracy: 1.0\n",
            "  Batch 5115/8000, Loss: 0.11392413079738617, Accuracy: 1.0\n",
            "  Batch 5116/8000, Loss: 0.42441511154174805, Accuracy: 1.0\n",
            "  Batch 5117/8000, Loss: 0.20242103934288025, Accuracy: 1.0\n",
            "  Batch 5118/8000, Loss: 1.4301245212554932, Accuracy: 0.0\n",
            "  Batch 5119/8000, Loss: 0.11769542098045349, Accuracy: 1.0\n",
            "  Batch 5120/8000, Loss: 0.49258261919021606, Accuracy: 1.0\n",
            "  Batch 5121/8000, Loss: 0.1329030990600586, Accuracy: 1.0\n",
            "  Batch 5122/8000, Loss: 0.5536562204360962, Accuracy: 1.0\n",
            "  Batch 5123/8000, Loss: 0.1354518085718155, Accuracy: 1.0\n",
            "  Batch 5124/8000, Loss: 0.11659273505210876, Accuracy: 1.0\n",
            "  Batch 5125/8000, Loss: 0.7588319182395935, Accuracy: 1.0\n",
            "  Batch 5126/8000, Loss: 0.42155492305755615, Accuracy: 1.0\n",
            "  Batch 5127/8000, Loss: 0.11329574882984161, Accuracy: 1.0\n",
            "  Batch 5128/8000, Loss: 0.11159540712833405, Accuracy: 1.0\n",
            "  Batch 5129/8000, Loss: 0.7577847838401794, Accuracy: 1.0\n",
            "  Batch 5130/8000, Loss: 0.22202853858470917, Accuracy: 1.0\n",
            "  Batch 5131/8000, Loss: 0.4311744272708893, Accuracy: 1.0\n",
            "  Batch 5132/8000, Loss: 0.7417880296707153, Accuracy: 1.0\n",
            "  Batch 5133/8000, Loss: 0.7055981159210205, Accuracy: 1.0\n",
            "  Batch 5134/8000, Loss: 0.460693359375, Accuracy: 1.0\n",
            "  Batch 5135/8000, Loss: 0.115728460252285, Accuracy: 1.0\n",
            "  Batch 5136/8000, Loss: 0.11350738257169724, Accuracy: 1.0\n",
            "  Batch 5137/8000, Loss: 0.11478845030069351, Accuracy: 1.0\n",
            "  Batch 5138/8000, Loss: 0.3545133173465729, Accuracy: 1.0\n",
            "  Batch 5139/8000, Loss: 0.11581006646156311, Accuracy: 1.0\n",
            "  Batch 5140/8000, Loss: 0.4204694926738739, Accuracy: 1.0\n",
            "  Batch 5141/8000, Loss: 0.3566136360168457, Accuracy: 1.0\n",
            "  Batch 5142/8000, Loss: 0.11442317068576813, Accuracy: 1.0\n",
            "  Batch 5143/8000, Loss: 1.381776213645935, Accuracy: 0.0\n",
            "  Batch 5144/8000, Loss: 0.11508378386497498, Accuracy: 1.0\n",
            "  Batch 5145/8000, Loss: 0.4758725166320801, Accuracy: 1.0\n",
            "  Batch 5146/8000, Loss: 1.475475788116455, Accuracy: 0.0\n",
            "  Batch 5147/8000, Loss: 0.6008248329162598, Accuracy: 1.0\n",
            "  Batch 5148/8000, Loss: 0.11316992342472076, Accuracy: 1.0\n",
            "  Batch 5149/8000, Loss: 0.38865238428115845, Accuracy: 1.0\n",
            "  Batch 5150/8000, Loss: 0.11300990730524063, Accuracy: 1.0\n",
            "  Batch 5151/8000, Loss: 0.3887600004673004, Accuracy: 1.0\n",
            "  Batch 5152/8000, Loss: 0.654426097869873, Accuracy: 1.0\n",
            "  Batch 5153/8000, Loss: 1.559740424156189, Accuracy: 0.0\n",
            "  Batch 5154/8000, Loss: 0.11298775672912598, Accuracy: 1.0\n",
            "  Batch 5155/8000, Loss: 1.1880598068237305, Accuracy: 0.0\n",
            "  Batch 5156/8000, Loss: 0.7177079319953918, Accuracy: 1.0\n",
            "  Batch 5157/8000, Loss: 1.1302368640899658, Accuracy: 0.0\n",
            "  Batch 5158/8000, Loss: 0.10916896164417267, Accuracy: 1.0\n",
            "  Batch 5159/8000, Loss: 0.386005163192749, Accuracy: 1.0\n",
            "  Batch 5160/8000, Loss: 0.505799412727356, Accuracy: 1.0\n",
            "  Batch 5161/8000, Loss: 0.1127595454454422, Accuracy: 1.0\n",
            "  Batch 5162/8000, Loss: 0.5054978728294373, Accuracy: 1.0\n",
            "  Batch 5163/8000, Loss: 0.525141716003418, Accuracy: 1.0\n",
            "  Batch 5164/8000, Loss: 0.6561269760131836, Accuracy: 1.0\n",
            "  Batch 5165/8000, Loss: 0.42522549629211426, Accuracy: 1.0\n",
            "  Batch 5166/8000, Loss: 0.11211161315441132, Accuracy: 1.0\n",
            "  Batch 5167/8000, Loss: 0.39627188444137573, Accuracy: 1.0\n",
            "  Batch 5168/8000, Loss: 1.5597472190856934, Accuracy: 0.0\n",
            "  Batch 5169/8000, Loss: 0.12443123757839203, Accuracy: 1.0\n",
            "  Batch 5170/8000, Loss: 0.38209980726242065, Accuracy: 1.0\n",
            "  Batch 5171/8000, Loss: 0.11206096410751343, Accuracy: 1.0\n",
            "  Batch 5172/8000, Loss: 0.11793941259384155, Accuracy: 1.0\n",
            "  Batch 5173/8000, Loss: 0.3794579803943634, Accuracy: 1.0\n",
            "  Batch 5174/8000, Loss: 0.3781053125858307, Accuracy: 1.0\n",
            "  Batch 5175/8000, Loss: 1.6036255359649658, Accuracy: 0.0\n",
            "  Batch 5176/8000, Loss: 0.3768022954463959, Accuracy: 1.0\n",
            "  Batch 5177/8000, Loss: 0.12833870947360992, Accuracy: 1.0\n",
            "  Batch 5178/8000, Loss: 0.5057823657989502, Accuracy: 1.0\n",
            "  Batch 5179/8000, Loss: 0.10995572805404663, Accuracy: 1.0\n",
            "  Batch 5180/8000, Loss: 0.3572923243045807, Accuracy: 1.0\n",
            "  Batch 5181/8000, Loss: 0.5778632760047913, Accuracy: 1.0\n",
            "  Batch 5182/8000, Loss: 0.11154568940401077, Accuracy: 1.0\n",
            "  Batch 5183/8000, Loss: 0.5461010932922363, Accuracy: 1.0\n",
            "  Batch 5184/8000, Loss: 0.40906044840812683, Accuracy: 1.0\n",
            "  Batch 5185/8000, Loss: 0.37378159165382385, Accuracy: 1.0\n",
            "  Batch 5186/8000, Loss: 0.11059160530567169, Accuracy: 1.0\n",
            "  Batch 5187/8000, Loss: 0.48459187150001526, Accuracy: 1.0\n",
            "  Batch 5188/8000, Loss: 0.11836100369691849, Accuracy: 1.0\n",
            "  Batch 5189/8000, Loss: 0.10918904840946198, Accuracy: 1.0\n",
            "  Batch 5190/8000, Loss: 0.7631692290306091, Accuracy: 1.0\n",
            "  Batch 5191/8000, Loss: 0.11965534090995789, Accuracy: 1.0\n",
            "  Batch 5192/8000, Loss: 0.11022865772247314, Accuracy: 1.0\n",
            "  Batch 5193/8000, Loss: 1.6263885498046875, Accuracy: 0.0\n",
            "  Batch 5194/8000, Loss: 0.35665449500083923, Accuracy: 1.0\n",
            "  Batch 5195/8000, Loss: 0.11004631221294403, Accuracy: 1.0\n",
            "  Batch 5196/8000, Loss: 1.1979305744171143, Accuracy: 0.0\n",
            "  Batch 5197/8000, Loss: 0.10894078761339188, Accuracy: 1.0\n",
            "  Batch 5198/8000, Loss: 0.11175848543643951, Accuracy: 1.0\n",
            "  Batch 5199/8000, Loss: 0.1211606115102768, Accuracy: 1.0\n",
            "  Batch 5200/8000, Loss: 1.2450138330459595, Accuracy: 0.0\n",
            "  Batch 5201/8000, Loss: 0.3856589198112488, Accuracy: 1.0\n",
            "  Batch 5202/8000, Loss: 0.47381073236465454, Accuracy: 1.0\n",
            "  Batch 5203/8000, Loss: 1.6188323497772217, Accuracy: 0.0\n",
            "  Batch 5204/8000, Loss: 0.1105257123708725, Accuracy: 1.0\n",
            "  Batch 5205/8000, Loss: 0.11025731265544891, Accuracy: 1.0\n",
            "  Batch 5206/8000, Loss: 0.11173351109027863, Accuracy: 1.0\n",
            "  Batch 5207/8000, Loss: 0.44368067383766174, Accuracy: 1.0\n",
            "  Batch 5208/8000, Loss: 1.1899216175079346, Accuracy: 0.0\n",
            "  Batch 5209/8000, Loss: 1.2895222902297974, Accuracy: 0.0\n",
            "  Batch 5210/8000, Loss: 0.46055036783218384, Accuracy: 1.0\n",
            "  Batch 5211/8000, Loss: 0.10894718766212463, Accuracy: 1.0\n",
            "  Batch 5212/8000, Loss: 0.367716908454895, Accuracy: 1.0\n",
            "  Batch 5213/8000, Loss: 0.1105777770280838, Accuracy: 1.0\n",
            "  Batch 5214/8000, Loss: 0.12003527581691742, Accuracy: 1.0\n",
            "  Batch 5215/8000, Loss: 0.3613983988761902, Accuracy: 1.0\n",
            "  Batch 5216/8000, Loss: 0.9112128019332886, Accuracy: 0.0\n",
            "  Batch 5217/8000, Loss: 0.3933470845222473, Accuracy: 1.0\n",
            "  Batch 5218/8000, Loss: 0.11282449215650558, Accuracy: 1.0\n",
            "  Batch 5219/8000, Loss: 0.11414311826229095, Accuracy: 1.0\n",
            "  Batch 5220/8000, Loss: 0.11115410178899765, Accuracy: 1.0\n",
            "  Batch 5221/8000, Loss: 0.10880087316036224, Accuracy: 1.0\n",
            "  Batch 5222/8000, Loss: 0.5449787974357605, Accuracy: 1.0\n",
            "  Batch 5223/8000, Loss: 0.45352792739868164, Accuracy: 1.0\n",
            "  Batch 5224/8000, Loss: 0.12278521060943604, Accuracy: 1.0\n",
            "  Batch 5225/8000, Loss: 0.1124904453754425, Accuracy: 1.0\n",
            "  Batch 5226/8000, Loss: 0.35695719718933105, Accuracy: 1.0\n",
            "  Batch 5227/8000, Loss: 0.1087690144777298, Accuracy: 1.0\n",
            "  Batch 5228/8000, Loss: 0.3535126745700836, Accuracy: 1.0\n",
            "  Batch 5229/8000, Loss: 1.6130152940750122, Accuracy: 0.0\n",
            "  Batch 5230/8000, Loss: 0.36482545733451843, Accuracy: 1.0\n",
            "  Batch 5231/8000, Loss: 0.3899548649787903, Accuracy: 1.0\n",
            "  Batch 5232/8000, Loss: 0.19109822809696198, Accuracy: 1.0\n",
            "  Batch 5233/8000, Loss: 0.4102652966976166, Accuracy: 1.0\n",
            "  Batch 5234/8000, Loss: 0.3652071952819824, Accuracy: 1.0\n",
            "  Batch 5235/8000, Loss: 0.7499182224273682, Accuracy: 1.0\n",
            "  Batch 5236/8000, Loss: 0.11247707158327103, Accuracy: 1.0\n",
            "  Batch 5237/8000, Loss: 0.7909502983093262, Accuracy: 1.0\n",
            "  Batch 5238/8000, Loss: 0.33689895272254944, Accuracy: 1.0\n",
            "  Batch 5239/8000, Loss: 0.14842379093170166, Accuracy: 1.0\n",
            "  Batch 5240/8000, Loss: 0.5376352667808533, Accuracy: 1.0\n",
            "  Batch 5241/8000, Loss: 0.34239640831947327, Accuracy: 1.0\n",
            "  Batch 5242/8000, Loss: 1.7412004470825195, Accuracy: 0.0\n",
            "  Batch 5243/8000, Loss: 0.4144439101219177, Accuracy: 1.0\n",
            "  Batch 5244/8000, Loss: 0.33048170804977417, Accuracy: 1.0\n",
            "  Batch 5245/8000, Loss: 1.7232565879821777, Accuracy: 0.0\n",
            "  Batch 5246/8000, Loss: 0.5795786380767822, Accuracy: 1.0\n",
            "  Batch 5247/8000, Loss: 0.10867716372013092, Accuracy: 1.0\n",
            "  Batch 5248/8000, Loss: 0.330427885055542, Accuracy: 1.0\n",
            "  Batch 5249/8000, Loss: 1.7401798963546753, Accuracy: 0.0\n",
            "  Batch 5250/8000, Loss: 0.5532789826393127, Accuracy: 1.0\n",
            "  Batch 5251/8000, Loss: 0.37309202551841736, Accuracy: 1.0\n",
            "  Batch 5252/8000, Loss: 1.6240278482437134, Accuracy: 0.0\n",
            "  Batch 5253/8000, Loss: 0.8437432646751404, Accuracy: 0.0\n",
            "  Batch 5254/8000, Loss: 1.4466394186019897, Accuracy: 0.0\n",
            "  Batch 5255/8000, Loss: 0.18499353528022766, Accuracy: 1.0\n",
            "  Batch 5256/8000, Loss: 0.32216382026672363, Accuracy: 1.0\n",
            "  Batch 5257/8000, Loss: 1.66800057888031, Accuracy: 0.0\n",
            "  Batch 5258/8000, Loss: 0.10859687626361847, Accuracy: 1.0\n",
            "  Batch 5259/8000, Loss: 1.441567301750183, Accuracy: 0.0\n",
            "  Batch 5260/8000, Loss: 0.43271034955978394, Accuracy: 1.0\n",
            "  Batch 5261/8000, Loss: 0.3541065454483032, Accuracy: 1.0\n",
            "  Batch 5262/8000, Loss: 1.5407053232192993, Accuracy: 0.0\n",
            "  Batch 5263/8000, Loss: 0.447756826877594, Accuracy: 1.0\n",
            "  Batch 5264/8000, Loss: 0.10870124399662018, Accuracy: 1.0\n",
            "  Batch 5265/8000, Loss: 1.6587518453598022, Accuracy: 0.0\n",
            "  Batch 5266/8000, Loss: 0.10859014093875885, Accuracy: 1.0\n",
            "  Batch 5267/8000, Loss: 0.3842550814151764, Accuracy: 1.0\n",
            "  Batch 5268/8000, Loss: 1.2666609287261963, Accuracy: 0.0\n",
            "  Batch 5269/8000, Loss: 0.11068351566791534, Accuracy: 1.0\n",
            "  Batch 5270/8000, Loss: 0.12027940154075623, Accuracy: 1.0\n",
            "  Batch 5271/8000, Loss: 0.4665885865688324, Accuracy: 1.0\n",
            "  Batch 5272/8000, Loss: 0.6774640679359436, Accuracy: 1.0\n",
            "  Batch 5273/8000, Loss: 0.3994493782520294, Accuracy: 1.0\n",
            "  Batch 5274/8000, Loss: 0.41961508989334106, Accuracy: 1.0\n",
            "  Batch 5275/8000, Loss: 1.4263287782669067, Accuracy: 0.0\n",
            "  Batch 5276/8000, Loss: 1.5519739389419556, Accuracy: 0.0\n",
            "  Batch 5277/8000, Loss: 0.5047306418418884, Accuracy: 1.0\n",
            "  Batch 5278/8000, Loss: 0.11332783102989197, Accuracy: 1.0\n",
            "  Batch 5279/8000, Loss: 0.11102685332298279, Accuracy: 1.0\n",
            "  Batch 5280/8000, Loss: 1.2987231016159058, Accuracy: 0.0\n",
            "  Batch 5281/8000, Loss: 0.10848499834537506, Accuracy: 1.0\n",
            "  Batch 5282/8000, Loss: 0.40866196155548096, Accuracy: 1.0\n",
            "  Batch 5283/8000, Loss: 0.41564977169036865, Accuracy: 1.0\n",
            "  Batch 5284/8000, Loss: 0.394706130027771, Accuracy: 1.0\n",
            "  Batch 5285/8000, Loss: 0.41673967242240906, Accuracy: 1.0\n",
            "  Batch 5286/8000, Loss: 0.2953311502933502, Accuracy: 1.0\n",
            "  Batch 5287/8000, Loss: 1.3718920946121216, Accuracy: 0.0\n",
            "  Batch 5288/8000, Loss: 0.4664760231971741, Accuracy: 1.0\n",
            "  Batch 5289/8000, Loss: 0.44990476965904236, Accuracy: 1.0\n",
            "  Batch 5290/8000, Loss: 0.41429561376571655, Accuracy: 1.0\n",
            "  Batch 5291/8000, Loss: 0.11370484530925751, Accuracy: 1.0\n",
            "  Batch 5292/8000, Loss: 0.6574504971504211, Accuracy: 1.0\n",
            "  Batch 5293/8000, Loss: 0.46542561054229736, Accuracy: 1.0\n",
            "  Batch 5294/8000, Loss: 0.10969620198011398, Accuracy: 1.0\n",
            "  Batch 5295/8000, Loss: 0.42886433005332947, Accuracy: 1.0\n",
            "  Batch 5296/8000, Loss: 0.10991609841585159, Accuracy: 1.0\n",
            "  Batch 5297/8000, Loss: 0.1096084713935852, Accuracy: 1.0\n",
            "  Batch 5298/8000, Loss: 1.2390685081481934, Accuracy: 0.0\n",
            "  Batch 5299/8000, Loss: 0.10844965279102325, Accuracy: 1.0\n",
            "  Batch 5300/8000, Loss: 1.0787310600280762, Accuracy: 0.0\n",
            "  Batch 5301/8000, Loss: 0.10969191789627075, Accuracy: 1.0\n",
            "  Batch 5302/8000, Loss: 0.4571000039577484, Accuracy: 1.0\n",
            "  Batch 5303/8000, Loss: 1.3550559282302856, Accuracy: 0.0\n",
            "  Batch 5304/8000, Loss: 0.40833526849746704, Accuracy: 1.0\n",
            "  Batch 5305/8000, Loss: 0.5199394226074219, Accuracy: 1.0\n",
            "  Batch 5306/8000, Loss: 0.4513801634311676, Accuracy: 1.0\n",
            "  Batch 5307/8000, Loss: 1.3886955976486206, Accuracy: 0.0\n",
            "  Batch 5308/8000, Loss: 0.4313583970069885, Accuracy: 1.0\n",
            "  Batch 5309/8000, Loss: 0.1083504855632782, Accuracy: 1.0\n",
            "  Batch 5310/8000, Loss: 0.6666637063026428, Accuracy: 1.0\n",
            "  Batch 5311/8000, Loss: 0.5071160793304443, Accuracy: 1.0\n",
            "  Batch 5312/8000, Loss: 0.10867244005203247, Accuracy: 1.0\n",
            "  Batch 5313/8000, Loss: 0.10916057229042053, Accuracy: 1.0\n",
            "  Batch 5314/8000, Loss: 0.10849136114120483, Accuracy: 1.0\n",
            "  Batch 5315/8000, Loss: 0.10961393266916275, Accuracy: 1.0\n",
            "  Batch 5316/8000, Loss: 1.3001991510391235, Accuracy: 0.0\n",
            "  Batch 5317/8000, Loss: 0.45909395813941956, Accuracy: 1.0\n",
            "  Batch 5318/8000, Loss: 1.3179347515106201, Accuracy: 0.0\n",
            "  Batch 5319/8000, Loss: 0.49135711789131165, Accuracy: 1.0\n",
            "  Batch 5320/8000, Loss: 0.45725569128990173, Accuracy: 1.0\n",
            "  Batch 5321/8000, Loss: 0.4505549371242523, Accuracy: 1.0\n",
            "  Batch 5322/8000, Loss: 0.4212336838245392, Accuracy: 1.0\n",
            "  Batch 5323/8000, Loss: 1.453731894493103, Accuracy: 0.0\n",
            "  Batch 5324/8000, Loss: 1.0980629920959473, Accuracy: 0.0\n",
            "  Batch 5325/8000, Loss: 1.7019466161727905, Accuracy: 0.0\n",
            "  Batch 5326/8000, Loss: 0.10817884653806686, Accuracy: 1.0\n",
            "  Batch 5327/8000, Loss: 0.5449429154396057, Accuracy: 1.0\n",
            "  Batch 5328/8000, Loss: 0.884982705116272, Accuracy: 0.0\n",
            "  Batch 5329/8000, Loss: 1.371801733970642, Accuracy: 0.0\n",
            "  Batch 5330/8000, Loss: 0.4693131148815155, Accuracy: 1.0\n",
            "  Batch 5331/8000, Loss: 1.3434656858444214, Accuracy: 0.0\n",
            "  Batch 5332/8000, Loss: 0.46899282932281494, Accuracy: 1.0\n",
            "  Batch 5333/8000, Loss: 0.4688973128795624, Accuracy: 1.0\n",
            "  Batch 5334/8000, Loss: 0.10820266604423523, Accuracy: 1.0\n",
            "  Batch 5335/8000, Loss: 0.4743258059024811, Accuracy: 1.0\n",
            "  Batch 5336/8000, Loss: 0.7933780550956726, Accuracy: 1.0\n",
            "  Batch 5337/8000, Loss: 0.11094891279935837, Accuracy: 1.0\n",
            "  Batch 5338/8000, Loss: 1.2407125234603882, Accuracy: 0.0\n",
            "  Batch 5339/8000, Loss: 0.12133647501468658, Accuracy: 1.0\n",
            "  Batch 5340/8000, Loss: 0.11833994090557098, Accuracy: 1.0\n",
            "  Batch 5341/8000, Loss: 0.4635979235172272, Accuracy: 1.0\n",
            "  Batch 5342/8000, Loss: 1.020178198814392, Accuracy: 0.0\n",
            "  Batch 5343/8000, Loss: 0.10819739103317261, Accuracy: 1.0\n",
            "  Batch 5344/8000, Loss: 0.41412290930747986, Accuracy: 1.0\n",
            "  Batch 5345/8000, Loss: 0.7034579515457153, Accuracy: 1.0\n",
            "  Batch 5346/8000, Loss: 0.9864084720611572, Accuracy: 0.0\n",
            "  Batch 5347/8000, Loss: 0.543670654296875, Accuracy: 1.0\n",
            "  Batch 5348/8000, Loss: 0.47833284735679626, Accuracy: 1.0\n",
            "  Batch 5349/8000, Loss: 1.2201594114303589, Accuracy: 0.0\n",
            "  Batch 5350/8000, Loss: 1.273206114768982, Accuracy: 0.0\n",
            "  Batch 5351/8000, Loss: 0.11600062251091003, Accuracy: 1.0\n",
            "  Batch 5352/8000, Loss: 0.4667467474937439, Accuracy: 1.0\n",
            "  Batch 5353/8000, Loss: 0.6562333106994629, Accuracy: 1.0\n",
            "  Batch 5354/8000, Loss: 1.0825105905532837, Accuracy: 0.0\n",
            "  Batch 5355/8000, Loss: 0.5048448443412781, Accuracy: 1.0\n",
            "  Batch 5356/8000, Loss: 0.4607647955417633, Accuracy: 1.0\n",
            "  Batch 5357/8000, Loss: 0.45890864729881287, Accuracy: 1.0\n",
            "  Batch 5358/8000, Loss: 1.236764907836914, Accuracy: 0.0\n",
            "  Batch 5359/8000, Loss: 0.8082414269447327, Accuracy: 0.0\n",
            "  Batch 5360/8000, Loss: 0.648314356803894, Accuracy: 1.0\n",
            "  Batch 5361/8000, Loss: 0.9833317399024963, Accuracy: 0.0\n",
            "  Batch 5362/8000, Loss: 0.9299412369728088, Accuracy: 0.0\n",
            "  Batch 5363/8000, Loss: 0.5264853835105896, Accuracy: 1.0\n",
            "  Batch 5364/8000, Loss: 0.5498981475830078, Accuracy: 1.0\n",
            "  Batch 5365/8000, Loss: 0.49664705991744995, Accuracy: 1.0\n",
            "  Batch 5366/8000, Loss: 0.5149626731872559, Accuracy: 1.0\n",
            "  Batch 5367/8000, Loss: 0.5534301400184631, Accuracy: 1.0\n",
            "  Batch 5368/8000, Loss: 1.2009837627410889, Accuracy: 0.0\n",
            "  Batch 5369/8000, Loss: 0.4976773262023926, Accuracy: 1.0\n",
            "  Batch 5370/8000, Loss: 1.0467220544815063, Accuracy: 0.0\n",
            "  Batch 5371/8000, Loss: 0.47460535168647766, Accuracy: 1.0\n",
            "  Batch 5372/8000, Loss: 0.7166481018066406, Accuracy: 1.0\n",
            "  Batch 5373/8000, Loss: 0.10793818533420563, Accuracy: 1.0\n",
            "  Batch 5374/8000, Loss: 0.11098866164684296, Accuracy: 1.0\n",
            "  Batch 5375/8000, Loss: 0.9912344813346863, Accuracy: 0.0\n",
            "  Batch 5376/8000, Loss: 1.1984665393829346, Accuracy: 0.0\n",
            "  Batch 5377/8000, Loss: 0.37523967027664185, Accuracy: 1.0\n",
            "  Batch 5378/8000, Loss: 0.91547030210495, Accuracy: 0.0\n",
            "  Batch 5379/8000, Loss: 0.20330770313739777, Accuracy: 1.0\n",
            "  Batch 5380/8000, Loss: 0.47280916571617126, Accuracy: 1.0\n",
            "  Batch 5381/8000, Loss: 0.11652098596096039, Accuracy: 1.0\n",
            "  Batch 5382/8000, Loss: 0.4958246350288391, Accuracy: 1.0\n",
            "  Batch 5383/8000, Loss: 1.243665337562561, Accuracy: 0.0\n",
            "  Batch 5384/8000, Loss: 0.5225947499275208, Accuracy: 1.0\n",
            "  Batch 5385/8000, Loss: 0.47384747862815857, Accuracy: 1.0\n",
            "  Batch 5386/8000, Loss: 0.47940772771835327, Accuracy: 1.0\n",
            "  Batch 5387/8000, Loss: 0.11038713902235031, Accuracy: 1.0\n",
            "  Batch 5388/8000, Loss: 0.3211888372898102, Accuracy: 1.0\n",
            "  Batch 5389/8000, Loss: 0.10791139304637909, Accuracy: 1.0\n",
            "  Batch 5390/8000, Loss: 0.8919109106063843, Accuracy: 0.0\n",
            "  Batch 5391/8000, Loss: 0.568739652633667, Accuracy: 1.0\n",
            "  Batch 5392/8000, Loss: 1.15444815158844, Accuracy: 0.0\n",
            "  Batch 5393/8000, Loss: 1.274531364440918, Accuracy: 0.0\n",
            "  Batch 5394/8000, Loss: 0.7862823605537415, Accuracy: 1.0\n",
            "  Batch 5395/8000, Loss: 0.9292747974395752, Accuracy: 0.0\n",
            "  Batch 5396/8000, Loss: 1.1960837841033936, Accuracy: 0.0\n",
            "  Batch 5397/8000, Loss: 0.18296867609024048, Accuracy: 1.0\n",
            "  Batch 5398/8000, Loss: 1.1638538837432861, Accuracy: 0.0\n",
            "  Batch 5399/8000, Loss: 0.5315001010894775, Accuracy: 1.0\n",
            "  Batch 5400/8000, Loss: 0.5099050402641296, Accuracy: 1.0\n",
            "  Batch 5401/8000, Loss: 1.1113861799240112, Accuracy: 0.0\n",
            "  Batch 5402/8000, Loss: 0.7932419180870056, Accuracy: 1.0\n",
            "  Batch 5403/8000, Loss: 0.11017710715532303, Accuracy: 1.0\n",
            "  Batch 5404/8000, Loss: 0.5985373854637146, Accuracy: 1.0\n",
            "  Batch 5405/8000, Loss: 0.7674649953842163, Accuracy: 1.0\n",
            "  Batch 5406/8000, Loss: 0.7217961549758911, Accuracy: 1.0\n",
            "  Batch 5407/8000, Loss: 0.10844513773918152, Accuracy: 1.0\n",
            "  Batch 5408/8000, Loss: 0.7139939665794373, Accuracy: 1.0\n",
            "  Batch 5409/8000, Loss: 0.5342146158218384, Accuracy: 1.0\n",
            "  Batch 5410/8000, Loss: 1.1777580976486206, Accuracy: 0.0\n",
            "  Batch 5411/8000, Loss: 0.5306877493858337, Accuracy: 1.0\n",
            "  Batch 5412/8000, Loss: 0.5827383995056152, Accuracy: 1.0\n",
            "  Batch 5413/8000, Loss: 1.1249319314956665, Accuracy: 0.0\n",
            "  Batch 5414/8000, Loss: 0.5816918015480042, Accuracy: 1.0\n",
            "  Batch 5415/8000, Loss: 0.10828125476837158, Accuracy: 1.0\n",
            "  Batch 5416/8000, Loss: 0.5668609142303467, Accuracy: 1.0\n",
            "  Batch 5417/8000, Loss: 4.693364143371582, Accuracy: 0.0\n",
            "  Batch 5418/8000, Loss: 0.7092344760894775, Accuracy: 1.0\n",
            "  Batch 5419/8000, Loss: 0.5604179501533508, Accuracy: 1.0\n",
            "  Batch 5420/8000, Loss: 0.11630269885063171, Accuracy: 1.0\n",
            "  Batch 5421/8000, Loss: 0.5711057782173157, Accuracy: 1.0\n",
            "  Batch 5422/8000, Loss: 0.10777512937784195, Accuracy: 1.0\n",
            "  Batch 5423/8000, Loss: 0.11722338944673538, Accuracy: 1.0\n",
            "  Batch 5424/8000, Loss: 0.4613328278064728, Accuracy: 1.0\n",
            "  Batch 5425/8000, Loss: 0.46080833673477173, Accuracy: 1.0\n",
            "  Batch 5426/8000, Loss: 0.4965772330760956, Accuracy: 1.0\n",
            "  Batch 5427/8000, Loss: 0.47084105014801025, Accuracy: 1.0\n",
            "  Batch 5428/8000, Loss: 0.6992834210395813, Accuracy: 1.0\n",
            "  Batch 5429/8000, Loss: 0.10823263227939606, Accuracy: 1.0\n",
            "  Batch 5430/8000, Loss: 0.46857285499572754, Accuracy: 1.0\n",
            "  Batch 5431/8000, Loss: 1.1593775749206543, Accuracy: 0.0\n",
            "  Batch 5432/8000, Loss: 0.5033155083656311, Accuracy: 1.0\n",
            "  Batch 5433/8000, Loss: 0.13660244643688202, Accuracy: 1.0\n",
            "  Batch 5434/8000, Loss: 0.4527243971824646, Accuracy: 1.0\n",
            "  Batch 5435/8000, Loss: 0.7035588026046753, Accuracy: 1.0\n",
            "  Batch 5436/8000, Loss: 1.3577704429626465, Accuracy: 0.0\n",
            "  Batch 5437/8000, Loss: 0.9184157252311707, Accuracy: 0.0\n",
            "  Batch 5438/8000, Loss: 0.10759279876947403, Accuracy: 1.0\n",
            "  Batch 5439/8000, Loss: 1.337054967880249, Accuracy: 0.0\n",
            "  Batch 5440/8000, Loss: 0.20594505965709686, Accuracy: 1.0\n",
            "  Batch 5441/8000, Loss: 0.4241985082626343, Accuracy: 1.0\n",
            "  Batch 5442/8000, Loss: 1.3450007438659668, Accuracy: 0.0\n",
            "  Batch 5443/8000, Loss: 1.185120701789856, Accuracy: 0.0\n",
            "  Batch 5444/8000, Loss: 0.8816922903060913, Accuracy: 0.0\n",
            "  Batch 5445/8000, Loss: 0.45776045322418213, Accuracy: 1.0\n",
            "  Batch 5446/8000, Loss: 0.6148211359977722, Accuracy: 1.0\n",
            "  Batch 5447/8000, Loss: 0.5257721543312073, Accuracy: 1.0\n",
            "  Batch 5448/8000, Loss: 0.5451087355613708, Accuracy: 1.0\n",
            "  Batch 5449/8000, Loss: 1.0227659940719604, Accuracy: 0.0\n",
            "  Batch 5450/8000, Loss: 0.12323413044214249, Accuracy: 1.0\n",
            "  Batch 5451/8000, Loss: 0.7078279852867126, Accuracy: 1.0\n",
            "  Batch 5452/8000, Loss: 0.4863545298576355, Accuracy: 1.0\n",
            "  Batch 5453/8000, Loss: 0.4641864597797394, Accuracy: 1.0\n",
            "  Batch 5454/8000, Loss: 0.563147246837616, Accuracy: 1.0\n",
            "  Batch 5455/8000, Loss: 0.5118049383163452, Accuracy: 1.0\n",
            "  Batch 5456/8000, Loss: 0.538457453250885, Accuracy: 1.0\n",
            "  Batch 5457/8000, Loss: 0.657248318195343, Accuracy: 1.0\n",
            "  Batch 5458/8000, Loss: 1.2790184020996094, Accuracy: 0.0\n",
            "  Batch 5459/8000, Loss: 0.7647444605827332, Accuracy: 1.0\n",
            "  Batch 5460/8000, Loss: 0.15599864721298218, Accuracy: 1.0\n",
            "  Batch 5461/8000, Loss: 0.4631296992301941, Accuracy: 1.0\n",
            "  Batch 5462/8000, Loss: 0.45814353227615356, Accuracy: 1.0\n",
            "  Batch 5463/8000, Loss: 0.10797996073961258, Accuracy: 1.0\n",
            "  Batch 5464/8000, Loss: 0.4241921603679657, Accuracy: 1.0\n",
            "  Batch 5465/8000, Loss: 0.1076250821352005, Accuracy: 1.0\n",
            "  Batch 5466/8000, Loss: 0.12802837789058685, Accuracy: 1.0\n",
            "  Batch 5467/8000, Loss: 1.1748582124710083, Accuracy: 0.0\n",
            "  Batch 5468/8000, Loss: 0.9940611720085144, Accuracy: 0.0\n",
            "  Batch 5469/8000, Loss: 1.152672290802002, Accuracy: 0.0\n",
            "  Batch 5470/8000, Loss: 0.42524781823158264, Accuracy: 1.0\n",
            "  Batch 5471/8000, Loss: 0.12069526314735413, Accuracy: 1.0\n",
            "  Batch 5472/8000, Loss: 0.13915157318115234, Accuracy: 1.0\n",
            "  Batch 5473/8000, Loss: 0.45468994975090027, Accuracy: 1.0\n",
            "  Batch 5474/8000, Loss: 1.384943962097168, Accuracy: 0.0\n",
            "  Batch 5475/8000, Loss: 0.11469168215990067, Accuracy: 1.0\n",
            "  Batch 5476/8000, Loss: 0.44258540868759155, Accuracy: 1.0\n",
            "  Batch 5477/8000, Loss: 0.11608538031578064, Accuracy: 1.0\n",
            "  Batch 5478/8000, Loss: 0.4345169961452484, Accuracy: 1.0\n",
            "  Batch 5479/8000, Loss: 0.41667795181274414, Accuracy: 1.0\n",
            "  Batch 5480/8000, Loss: 0.4721170663833618, Accuracy: 1.0\n",
            "  Batch 5481/8000, Loss: 0.47318679094314575, Accuracy: 1.0\n",
            "  Batch 5482/8000, Loss: 1.3533967733383179, Accuracy: 0.0\n",
            "  Batch 5483/8000, Loss: 0.8162157535552979, Accuracy: 0.0\n",
            "  Batch 5484/8000, Loss: 0.5379754900932312, Accuracy: 1.0\n",
            "  Batch 5485/8000, Loss: 0.10757610201835632, Accuracy: 1.0\n",
            "  Batch 5486/8000, Loss: 0.4345388412475586, Accuracy: 1.0\n",
            "  Batch 5487/8000, Loss: 0.4462890326976776, Accuracy: 1.0\n",
            "  Batch 5488/8000, Loss: 0.5552487969398499, Accuracy: 1.0\n",
            "  Batch 5489/8000, Loss: 1.3801486492156982, Accuracy: 0.0\n",
            "  Batch 5490/8000, Loss: 0.43854737281799316, Accuracy: 1.0\n",
            "  Batch 5491/8000, Loss: 0.4596928656101227, Accuracy: 1.0\n",
            "  Batch 5492/8000, Loss: 1.2351046800613403, Accuracy: 0.0\n",
            "  Batch 5493/8000, Loss: 0.14182725548744202, Accuracy: 1.0\n",
            "  Batch 5494/8000, Loss: 0.43815991282463074, Accuracy: 1.0\n",
            "  Batch 5495/8000, Loss: 0.11908145248889923, Accuracy: 1.0\n",
            "  Batch 5496/8000, Loss: 0.7275307178497314, Accuracy: 1.0\n",
            "  Batch 5497/8000, Loss: 0.4576546251773834, Accuracy: 1.0\n",
            "  Batch 5498/8000, Loss: 0.4359859526157379, Accuracy: 1.0\n",
            "  Batch 5499/8000, Loss: 1.4049723148345947, Accuracy: 0.0\n",
            "  Batch 5500/8000, Loss: 1.211657166481018, Accuracy: 0.0\n",
            "  Batch 5501/8000, Loss: 0.12477925419807434, Accuracy: 1.0\n",
            "  Batch 5502/8000, Loss: 1.1820604801177979, Accuracy: 0.0\n",
            "  Batch 5503/8000, Loss: 0.4609692096710205, Accuracy: 1.0\n",
            "  Batch 5504/8000, Loss: 0.39332520961761475, Accuracy: 1.0\n",
            "  Batch 5505/8000, Loss: 0.45303046703338623, Accuracy: 1.0\n",
            "  Batch 5506/8000, Loss: 0.43632298707962036, Accuracy: 1.0\n",
            "  Batch 5507/8000, Loss: 0.14813826978206635, Accuracy: 1.0\n",
            "  Batch 5508/8000, Loss: 1.3626173734664917, Accuracy: 0.0\n",
            "  Batch 5509/8000, Loss: 0.4518328607082367, Accuracy: 1.0\n",
            "  Batch 5510/8000, Loss: 0.45088618993759155, Accuracy: 1.0\n",
            "  Batch 5511/8000, Loss: 0.4058716297149658, Accuracy: 1.0\n",
            "  Batch 5512/8000, Loss: 0.11435261368751526, Accuracy: 1.0\n",
            "  Batch 5513/8000, Loss: 0.42645782232284546, Accuracy: 1.0\n",
            "  Batch 5514/8000, Loss: 0.6479323506355286, Accuracy: 1.0\n",
            "  Batch 5515/8000, Loss: 0.44017452001571655, Accuracy: 1.0\n",
            "  Batch 5516/8000, Loss: 0.4298105537891388, Accuracy: 1.0\n",
            "  Batch 5517/8000, Loss: 1.1796551942825317, Accuracy: 0.0\n",
            "  Batch 5518/8000, Loss: 0.44540953636169434, Accuracy: 1.0\n",
            "  Batch 5519/8000, Loss: 0.4373306632041931, Accuracy: 1.0\n",
            "  Batch 5520/8000, Loss: 0.47547125816345215, Accuracy: 1.0\n",
            "  Batch 5521/8000, Loss: 0.41812142729759216, Accuracy: 1.0\n",
            "  Batch 5522/8000, Loss: 0.16294971108436584, Accuracy: 1.0\n",
            "  Batch 5523/8000, Loss: 0.6233240962028503, Accuracy: 1.0\n",
            "  Batch 5524/8000, Loss: 0.10715289413928986, Accuracy: 1.0\n",
            "  Batch 5525/8000, Loss: 0.3837129473686218, Accuracy: 1.0\n",
            "  Batch 5526/8000, Loss: 0.7688627243041992, Accuracy: 1.0\n",
            "  Batch 5527/8000, Loss: 0.11449070274829865, Accuracy: 1.0\n",
            "  Batch 5528/8000, Loss: 0.49673277139663696, Accuracy: 1.0\n",
            "  Batch 5529/8000, Loss: 1.6205288171768188, Accuracy: 0.0\n",
            "  Batch 5530/8000, Loss: 0.4759068489074707, Accuracy: 1.0\n",
            "  Batch 5531/8000, Loss: 0.4143199324607849, Accuracy: 1.0\n",
            "  Batch 5532/8000, Loss: 0.12294069677591324, Accuracy: 1.0\n",
            "  Batch 5533/8000, Loss: 0.24049806594848633, Accuracy: 1.0\n",
            "  Batch 5534/8000, Loss: 1.2079048156738281, Accuracy: 0.0\n",
            "  Batch 5535/8000, Loss: 0.3922741711139679, Accuracy: 1.0\n",
            "  Batch 5536/8000, Loss: 0.43718814849853516, Accuracy: 1.0\n",
            "  Batch 5537/8000, Loss: 0.40084928274154663, Accuracy: 1.0\n",
            "  Batch 5538/8000, Loss: 0.8748165965080261, Accuracy: 0.0\n",
            "  Batch 5539/8000, Loss: 0.10714274644851685, Accuracy: 1.0\n",
            "  Batch 5540/8000, Loss: 0.11622446775436401, Accuracy: 1.0\n",
            "  Batch 5541/8000, Loss: 0.42883774638175964, Accuracy: 1.0\n",
            "  Batch 5542/8000, Loss: 0.8078845739364624, Accuracy: 0.0\n",
            "  Batch 5543/8000, Loss: 1.0211021900177002, Accuracy: 0.0\n",
            "  Batch 5544/8000, Loss: 0.6742699146270752, Accuracy: 1.0\n",
            "  Batch 5545/8000, Loss: 1.3850246667861938, Accuracy: 0.0\n",
            "  Batch 5546/8000, Loss: 1.296329140663147, Accuracy: 0.0\n",
            "  Batch 5547/8000, Loss: 0.3933308720588684, Accuracy: 1.0\n",
            "  Batch 5548/8000, Loss: 0.679649293422699, Accuracy: 1.0\n",
            "  Batch 5549/8000, Loss: 0.8002921342849731, Accuracy: 0.0\n",
            "  Batch 5550/8000, Loss: 1.5916873216629028, Accuracy: 0.0\n",
            "  Batch 5551/8000, Loss: 0.422086626291275, Accuracy: 1.0\n",
            "  Batch 5552/8000, Loss: 0.40370893478393555, Accuracy: 1.0\n",
            "  Batch 5553/8000, Loss: 1.0649954080581665, Accuracy: 0.0\n",
            "  Batch 5554/8000, Loss: 0.8585891127586365, Accuracy: 0.0\n",
            "  Batch 5555/8000, Loss: 0.2994084656238556, Accuracy: 1.0\n",
            "  Batch 5556/8000, Loss: 2.1394612789154053, Accuracy: 0.0\n",
            "  Batch 5557/8000, Loss: 0.43945422768592834, Accuracy: 1.0\n",
            "  Batch 5558/8000, Loss: 0.10701458156108856, Accuracy: 1.0\n",
            "  Batch 5559/8000, Loss: 1.3737345933914185, Accuracy: 0.0\n",
            "  Batch 5560/8000, Loss: 0.6805909872055054, Accuracy: 1.0\n",
            "  Batch 5561/8000, Loss: 0.7977128028869629, Accuracy: 1.0\n",
            "  Batch 5562/8000, Loss: 0.4457012414932251, Accuracy: 1.0\n",
            "  Batch 5563/8000, Loss: 0.9435371160507202, Accuracy: 0.0\n",
            "  Batch 5564/8000, Loss: 0.117894247174263, Accuracy: 1.0\n",
            "  Batch 5565/8000, Loss: 0.11586305499076843, Accuracy: 1.0\n",
            "  Batch 5566/8000, Loss: 0.5295450091362, Accuracy: 1.0\n",
            "  Batch 5567/8000, Loss: 1.363837480545044, Accuracy: 0.0\n",
            "  Batch 5568/8000, Loss: 0.12119307368993759, Accuracy: 1.0\n",
            "  Batch 5569/8000, Loss: 0.11065872013568878, Accuracy: 1.0\n",
            "  Batch 5570/8000, Loss: 0.2564171552658081, Accuracy: 1.0\n",
            "  Batch 5571/8000, Loss: 0.4501439034938812, Accuracy: 1.0\n",
            "  Batch 5572/8000, Loss: 0.46419399976730347, Accuracy: 1.0\n",
            "  Batch 5573/8000, Loss: 1.089439868927002, Accuracy: 0.0\n",
            "  Batch 5574/8000, Loss: 0.4756700098514557, Accuracy: 1.0\n",
            "  Batch 5575/8000, Loss: 0.11151491105556488, Accuracy: 1.0\n",
            "  Batch 5576/8000, Loss: 0.7067534327507019, Accuracy: 1.0\n",
            "  Batch 5577/8000, Loss: 0.4600371718406677, Accuracy: 1.0\n",
            "  Batch 5578/8000, Loss: 0.5403786897659302, Accuracy: 1.0\n",
            "  Batch 5579/8000, Loss: 0.11361934244632721, Accuracy: 1.0\n",
            "  Batch 5580/8000, Loss: 0.5212290287017822, Accuracy: 1.0\n",
            "  Batch 5581/8000, Loss: 0.7696048617362976, Accuracy: 1.0\n",
            "  Batch 5582/8000, Loss: 0.11876198649406433, Accuracy: 1.0\n",
            "  Batch 5583/8000, Loss: 0.539259135723114, Accuracy: 1.0\n",
            "  Batch 5584/8000, Loss: 0.2985442578792572, Accuracy: 1.0\n",
            "  Batch 5585/8000, Loss: 0.4104844331741333, Accuracy: 1.0\n",
            "  Batch 5586/8000, Loss: 0.4751359224319458, Accuracy: 1.0\n",
            "  Batch 5587/8000, Loss: 0.4335688650608063, Accuracy: 1.0\n",
            "  Batch 5588/8000, Loss: 0.7741799354553223, Accuracy: 1.0\n",
            "  Batch 5589/8000, Loss: 0.662591814994812, Accuracy: 1.0\n",
            "  Batch 5590/8000, Loss: 0.404107928276062, Accuracy: 1.0\n",
            "  Batch 5591/8000, Loss: 0.11194872111082077, Accuracy: 1.0\n",
            "  Batch 5592/8000, Loss: 0.1069314256310463, Accuracy: 1.0\n",
            "  Batch 5593/8000, Loss: 0.11236021667718887, Accuracy: 1.0\n",
            "  Batch 5594/8000, Loss: 0.7994476556777954, Accuracy: 1.0\n",
            "  Batch 5595/8000, Loss: 0.6645308136940002, Accuracy: 1.0\n",
            "  Batch 5596/8000, Loss: 0.11233282089233398, Accuracy: 1.0\n",
            "  Batch 5597/8000, Loss: 0.10680937767028809, Accuracy: 1.0\n",
            "  Batch 5598/8000, Loss: 0.39564263820648193, Accuracy: 1.0\n",
            "  Batch 5599/8000, Loss: 0.11297456920146942, Accuracy: 1.0\n",
            "  Batch 5600/8000, Loss: 0.11971944570541382, Accuracy: 1.0\n",
            "  Batch 5601/8000, Loss: 0.6804211139678955, Accuracy: 1.0\n",
            "  Batch 5602/8000, Loss: 0.41803741455078125, Accuracy: 1.0\n",
            "  Batch 5603/8000, Loss: 0.12445151060819626, Accuracy: 1.0\n",
            "  Batch 5604/8000, Loss: 0.5233060121536255, Accuracy: 1.0\n",
            "  Batch 5605/8000, Loss: 0.9494749307632446, Accuracy: 0.0\n",
            "  Batch 5606/8000, Loss: 0.10678328573703766, Accuracy: 1.0\n",
            "  Batch 5607/8000, Loss: 1.241753339767456, Accuracy: 0.0\n",
            "  Batch 5608/8000, Loss: 1.523263692855835, Accuracy: 0.0\n",
            "  Batch 5609/8000, Loss: 0.11029712855815887, Accuracy: 1.0\n",
            "  Batch 5610/8000, Loss: 0.36260300874710083, Accuracy: 1.0\n",
            "  Batch 5611/8000, Loss: 0.3696606159210205, Accuracy: 1.0\n",
            "  Batch 5612/8000, Loss: 0.4585321843624115, Accuracy: 1.0\n",
            "  Batch 5613/8000, Loss: 0.43487048149108887, Accuracy: 1.0\n",
            "  Batch 5614/8000, Loss: 1.2565275430679321, Accuracy: 0.0\n",
            "  Batch 5615/8000, Loss: 0.3883563280105591, Accuracy: 1.0\n",
            "  Batch 5616/8000, Loss: 0.4732150435447693, Accuracy: 1.0\n",
            "  Batch 5617/8000, Loss: 1.5235978364944458, Accuracy: 0.0\n",
            "  Batch 5618/8000, Loss: 0.10857271403074265, Accuracy: 1.0\n",
            "  Batch 5619/8000, Loss: 0.720447301864624, Accuracy: 1.0\n",
            "  Batch 5620/8000, Loss: 0.599024772644043, Accuracy: 1.0\n",
            "  Batch 5621/8000, Loss: 0.3816206455230713, Accuracy: 1.0\n",
            "  Batch 5622/8000, Loss: 0.10954101383686066, Accuracy: 1.0\n",
            "  Batch 5623/8000, Loss: 0.418384313583374, Accuracy: 1.0\n",
            "  Batch 5624/8000, Loss: 0.3697294294834137, Accuracy: 1.0\n",
            "  Batch 5625/8000, Loss: 0.35373884439468384, Accuracy: 1.0\n",
            "  Batch 5626/8000, Loss: 0.11267352104187012, Accuracy: 1.0\n",
            "  Batch 5627/8000, Loss: 0.41123929619789124, Accuracy: 1.0\n",
            "  Batch 5628/8000, Loss: 1.348215103149414, Accuracy: 0.0\n",
            "  Batch 5629/8000, Loss: 1.1169018745422363, Accuracy: 0.0\n",
            "  Batch 5630/8000, Loss: 0.11087170988321304, Accuracy: 1.0\n",
            "  Batch 5631/8000, Loss: 0.41198089718818665, Accuracy: 1.0\n",
            "  Batch 5632/8000, Loss: 0.11015390604734421, Accuracy: 1.0\n",
            "  Batch 5633/8000, Loss: 1.5706939697265625, Accuracy: 0.0\n",
            "  Batch 5634/8000, Loss: 0.3969820737838745, Accuracy: 1.0\n",
            "  Batch 5635/8000, Loss: 0.10664455592632294, Accuracy: 1.0\n",
            "  Batch 5636/8000, Loss: 0.10958029329776764, Accuracy: 1.0\n",
            "  Batch 5637/8000, Loss: 0.1097705140709877, Accuracy: 1.0\n",
            "  Batch 5638/8000, Loss: 1.6008632183074951, Accuracy: 0.0\n",
            "  Batch 5639/8000, Loss: 0.11506800353527069, Accuracy: 1.0\n",
            "  Batch 5640/8000, Loss: 0.11125002801418304, Accuracy: 1.0\n",
            "  Batch 5641/8000, Loss: 1.3538742065429688, Accuracy: 0.0\n",
            "  Batch 5642/8000, Loss: 0.11451941728591919, Accuracy: 1.0\n",
            "  Batch 5643/8000, Loss: 0.1066911369562149, Accuracy: 1.0\n",
            "  Batch 5644/8000, Loss: 0.3760097622871399, Accuracy: 1.0\n",
            "  Batch 5645/8000, Loss: 0.42158663272857666, Accuracy: 1.0\n",
            "  Batch 5646/8000, Loss: 0.1148781031370163, Accuracy: 1.0\n",
            "  Batch 5647/8000, Loss: 0.35721033811569214, Accuracy: 1.0\n",
            "  Batch 5648/8000, Loss: 1.4326341152191162, Accuracy: 0.0\n",
            "  Batch 5649/8000, Loss: 0.43800678849220276, Accuracy: 1.0\n",
            "  Batch 5650/8000, Loss: 0.377529114484787, Accuracy: 1.0\n",
            "  Batch 5651/8000, Loss: 0.6883149743080139, Accuracy: 1.0\n",
            "  Batch 5652/8000, Loss: 0.8712635636329651, Accuracy: 0.0\n",
            "  Batch 5653/8000, Loss: 0.45005613565444946, Accuracy: 1.0\n",
            "  Batch 5654/8000, Loss: 0.8834967613220215, Accuracy: 0.0\n",
            "  Batch 5655/8000, Loss: 0.4483908712863922, Accuracy: 1.0\n",
            "  Batch 5656/8000, Loss: 0.11115744709968567, Accuracy: 1.0\n",
            "  Batch 5657/8000, Loss: 0.38900595903396606, Accuracy: 1.0\n",
            "  Batch 5658/8000, Loss: 0.42267000675201416, Accuracy: 1.0\n",
            "  Batch 5659/8000, Loss: 0.46839797496795654, Accuracy: 1.0\n",
            "  Batch 5660/8000, Loss: 0.4116971790790558, Accuracy: 1.0\n",
            "  Batch 5661/8000, Loss: 0.10647642612457275, Accuracy: 1.0\n",
            "  Batch 5662/8000, Loss: 0.3609117865562439, Accuracy: 1.0\n",
            "  Batch 5663/8000, Loss: 0.6343407034873962, Accuracy: 1.0\n",
            "  Batch 5664/8000, Loss: 0.106452077627182, Accuracy: 1.0\n",
            "  Batch 5665/8000, Loss: 0.37169215083122253, Accuracy: 1.0\n",
            "  Batch 5666/8000, Loss: 0.1066344827413559, Accuracy: 1.0\n",
            "  Batch 5667/8000, Loss: 1.6004769802093506, Accuracy: 0.0\n",
            "  Batch 5668/8000, Loss: 0.3662792146205902, Accuracy: 1.0\n",
            "  Batch 5669/8000, Loss: 0.33966317772865295, Accuracy: 1.0\n",
            "  Batch 5670/8000, Loss: 1.4862444400787354, Accuracy: 0.0\n",
            "  Batch 5671/8000, Loss: 0.35626721382141113, Accuracy: 1.0\n",
            "  Batch 5672/8000, Loss: 0.4514594078063965, Accuracy: 1.0\n",
            "  Batch 5673/8000, Loss: 0.954970121383667, Accuracy: 0.0\n",
            "  Batch 5674/8000, Loss: 0.32344353199005127, Accuracy: 1.0\n",
            "  Batch 5675/8000, Loss: 0.33318623900413513, Accuracy: 1.0\n",
            "  Batch 5676/8000, Loss: 0.1064939871430397, Accuracy: 1.0\n",
            "  Batch 5677/8000, Loss: 0.32898184657096863, Accuracy: 1.0\n",
            "  Batch 5678/8000, Loss: 0.35642558336257935, Accuracy: 1.0\n",
            "  Batch 5679/8000, Loss: 0.41589856147766113, Accuracy: 1.0\n",
            "  Batch 5680/8000, Loss: 0.39831846952438354, Accuracy: 1.0\n",
            "  Batch 5681/8000, Loss: 1.403608798980713, Accuracy: 0.0\n",
            "  Batch 5682/8000, Loss: 1.3151365518569946, Accuracy: 0.0\n",
            "  Batch 5683/8000, Loss: 0.3502108156681061, Accuracy: 1.0\n",
            "  Batch 5684/8000, Loss: 0.3588125705718994, Accuracy: 1.0\n",
            "  Batch 5685/8000, Loss: 0.39326390624046326, Accuracy: 1.0\n",
            "  Batch 5686/8000, Loss: 0.350841224193573, Accuracy: 1.0\n",
            "  Batch 5687/8000, Loss: 0.10848625004291534, Accuracy: 1.0\n",
            "  Batch 5688/8000, Loss: 0.9931681156158447, Accuracy: 0.0\n",
            "  Batch 5689/8000, Loss: 0.1064784824848175, Accuracy: 1.0\n",
            "  Batch 5690/8000, Loss: 0.10997259616851807, Accuracy: 1.0\n",
            "  Batch 5691/8000, Loss: 0.8880341053009033, Accuracy: 0.0\n",
            "  Batch 5692/8000, Loss: 0.3477911949157715, Accuracy: 1.0\n",
            "  Batch 5693/8000, Loss: 0.31620094180107117, Accuracy: 1.0\n",
            "  Batch 5694/8000, Loss: 0.11475087702274323, Accuracy: 1.0\n",
            "  Batch 5695/8000, Loss: 0.3524596393108368, Accuracy: 1.0\n",
            "  Batch 5696/8000, Loss: 0.35604867339134216, Accuracy: 1.0\n",
            "  Batch 5697/8000, Loss: 1.750478982925415, Accuracy: 0.0\n",
            "  Batch 5698/8000, Loss: 0.3621445894241333, Accuracy: 1.0\n",
            "  Batch 5699/8000, Loss: 0.3148421347141266, Accuracy: 1.0\n",
            "  Batch 5700/8000, Loss: 1.3102991580963135, Accuracy: 0.0\n",
            "  Batch 5701/8000, Loss: 0.3035255968570709, Accuracy: 1.0\n",
            "  Batch 5702/8000, Loss: 0.3648976683616638, Accuracy: 1.0\n",
            "  Batch 5703/8000, Loss: 0.620732307434082, Accuracy: 1.0\n",
            "  Batch 5704/8000, Loss: 0.4745412766933441, Accuracy: 1.0\n",
            "  Batch 5705/8000, Loss: 1.5862212181091309, Accuracy: 0.0\n",
            "  Batch 5706/8000, Loss: 1.056483268737793, Accuracy: 0.0\n",
            "  Batch 5707/8000, Loss: 0.7173492312431335, Accuracy: 1.0\n",
            "  Batch 5708/8000, Loss: 0.35958153009414673, Accuracy: 1.0\n",
            "  Batch 5709/8000, Loss: 0.3274405300617218, Accuracy: 1.0\n",
            "  Batch 5710/8000, Loss: 0.42021748423576355, Accuracy: 1.0\n",
            "  Batch 5711/8000, Loss: 0.34168922901153564, Accuracy: 1.0\n",
            "  Batch 5712/8000, Loss: 0.36106985807418823, Accuracy: 1.0\n",
            "  Batch 5713/8000, Loss: 0.10779701173305511, Accuracy: 1.0\n",
            "  Batch 5714/8000, Loss: 0.45929309725761414, Accuracy: 1.0\n",
            "  Batch 5715/8000, Loss: 0.3725970983505249, Accuracy: 1.0\n",
            "  Batch 5716/8000, Loss: 1.6319688558578491, Accuracy: 0.0\n",
            "  Batch 5717/8000, Loss: 0.10633832216262817, Accuracy: 1.0\n",
            "  Batch 5718/8000, Loss: 0.4388316869735718, Accuracy: 1.0\n",
            "  Batch 5719/8000, Loss: 0.3273501396179199, Accuracy: 1.0\n",
            "  Batch 5720/8000, Loss: 0.39503800868988037, Accuracy: 1.0\n",
            "  Batch 5721/8000, Loss: 0.4633979797363281, Accuracy: 1.0\n",
            "  Batch 5722/8000, Loss: 0.10937169194221497, Accuracy: 1.0\n",
            "  Batch 5723/8000, Loss: 0.34792813658714294, Accuracy: 1.0\n",
            "  Batch 5724/8000, Loss: 0.4395267963409424, Accuracy: 1.0\n",
            "  Batch 5725/8000, Loss: 0.1089894026517868, Accuracy: 1.0\n",
            "  Batch 5726/8000, Loss: 0.10898855328559875, Accuracy: 1.0\n",
            "  Batch 5727/8000, Loss: 1.733330488204956, Accuracy: 0.0\n",
            "  Batch 5728/8000, Loss: 0.347462922334671, Accuracy: 1.0\n",
            "  Batch 5729/8000, Loss: 0.37115275859832764, Accuracy: 1.0\n",
            "  Batch 5730/8000, Loss: 0.30539125204086304, Accuracy: 1.0\n",
            "  Batch 5731/8000, Loss: 0.30454009771347046, Accuracy: 1.0\n",
            "  Batch 5732/8000, Loss: 0.3370491564273834, Accuracy: 1.0\n",
            "  Batch 5733/8000, Loss: 0.11163017153739929, Accuracy: 1.0\n",
            "  Batch 5734/8000, Loss: 1.9357631206512451, Accuracy: 0.0\n",
            "  Batch 5735/8000, Loss: 0.3364672064781189, Accuracy: 1.0\n",
            "  Batch 5736/8000, Loss: 0.7252622842788696, Accuracy: 1.0\n",
            "  Batch 5737/8000, Loss: 0.300204336643219, Accuracy: 1.0\n",
            "  Batch 5738/8000, Loss: 0.34027421474456787, Accuracy: 1.0\n",
            "  Batch 5739/8000, Loss: 0.1389998346567154, Accuracy: 1.0\n",
            "  Batch 5740/8000, Loss: 1.385981798171997, Accuracy: 0.0\n",
            "  Batch 5741/8000, Loss: 0.37247514724731445, Accuracy: 1.0\n",
            "  Batch 5742/8000, Loss: 1.0799285173416138, Accuracy: 0.0\n",
            "  Batch 5743/8000, Loss: 0.3557092547416687, Accuracy: 1.0\n",
            "  Batch 5744/8000, Loss: 0.3772607743740082, Accuracy: 1.0\n",
            "  Batch 5745/8000, Loss: 0.38790813088417053, Accuracy: 1.0\n",
            "  Batch 5746/8000, Loss: 0.1085641086101532, Accuracy: 1.0\n",
            "  Batch 5747/8000, Loss: 0.1159798875451088, Accuracy: 1.0\n",
            "  Batch 5748/8000, Loss: 0.5937293171882629, Accuracy: 1.0\n",
            "  Batch 5749/8000, Loss: 0.33365553617477417, Accuracy: 1.0\n",
            "  Batch 5750/8000, Loss: 0.3103601634502411, Accuracy: 1.0\n",
            "  Batch 5751/8000, Loss: 0.3316698968410492, Accuracy: 1.0\n",
            "  Batch 5752/8000, Loss: 0.10823911428451538, Accuracy: 1.0\n",
            "  Batch 5753/8000, Loss: 0.32038018107414246, Accuracy: 1.0\n",
            "  Batch 5754/8000, Loss: 0.6130817532539368, Accuracy: 1.0\n",
            "  Batch 5755/8000, Loss: 1.1661014556884766, Accuracy: 0.0\n",
            "  Batch 5756/8000, Loss: 0.2900330424308777, Accuracy: 1.0\n",
            "  Batch 5757/8000, Loss: 0.9777863025665283, Accuracy: 0.0\n",
            "  Batch 5758/8000, Loss: 0.297313392162323, Accuracy: 1.0\n",
            "  Batch 5759/8000, Loss: 0.37926849722862244, Accuracy: 1.0\n",
            "  Batch 5760/8000, Loss: 0.8245589733123779, Accuracy: 0.0\n",
            "  Batch 5761/8000, Loss: 0.10604734718799591, Accuracy: 1.0\n",
            "  Batch 5762/8000, Loss: 0.28773045539855957, Accuracy: 1.0\n",
            "  Batch 5763/8000, Loss: 0.7669450044631958, Accuracy: 1.0\n",
            "  Batch 5764/8000, Loss: 0.27298179268836975, Accuracy: 1.0\n",
            "  Batch 5765/8000, Loss: 0.7353214025497437, Accuracy: 1.0\n",
            "  Batch 5766/8000, Loss: 1.1543186902999878, Accuracy: 0.0\n",
            "  Batch 5767/8000, Loss: 0.10715416073799133, Accuracy: 1.0\n",
            "  Batch 5768/8000, Loss: 0.8675152063369751, Accuracy: 0.0\n",
            "  Batch 5769/8000, Loss: 0.11254113912582397, Accuracy: 1.0\n",
            "  Batch 5770/8000, Loss: 0.591838538646698, Accuracy: 1.0\n",
            "  Batch 5771/8000, Loss: 1.9079030752182007, Accuracy: 0.0\n",
            "  Batch 5772/8000, Loss: 0.29641884565353394, Accuracy: 1.0\n",
            "  Batch 5773/8000, Loss: 0.10900892317295074, Accuracy: 1.0\n",
            "  Batch 5774/8000, Loss: 0.4027892053127289, Accuracy: 1.0\n",
            "  Batch 5775/8000, Loss: 0.1084180548787117, Accuracy: 1.0\n",
            "  Batch 5776/8000, Loss: 0.2979896068572998, Accuracy: 1.0\n",
            "  Batch 5777/8000, Loss: 0.3799228072166443, Accuracy: 1.0\n",
            "  Batch 5778/8000, Loss: 0.10729717463254929, Accuracy: 1.0\n",
            "  Batch 5779/8000, Loss: 0.377422571182251, Accuracy: 1.0\n",
            "  Batch 5780/8000, Loss: 0.2768455147743225, Accuracy: 1.0\n",
            "  Batch 5781/8000, Loss: 0.7821852564811707, Accuracy: 1.0\n",
            "  Batch 5782/8000, Loss: 0.10762877762317657, Accuracy: 1.0\n",
            "  Batch 5783/8000, Loss: 1.0784194469451904, Accuracy: 0.0\n",
            "  Batch 5784/8000, Loss: 0.8179052472114563, Accuracy: 0.0\n",
            "  Batch 5785/8000, Loss: 0.30864375829696655, Accuracy: 1.0\n",
            "  Batch 5786/8000, Loss: 0.10609327256679535, Accuracy: 1.0\n",
            "  Batch 5787/8000, Loss: 0.28286582231521606, Accuracy: 1.0\n",
            "  Batch 5788/8000, Loss: 0.31022772192955017, Accuracy: 1.0\n",
            "  Batch 5789/8000, Loss: 0.9868531227111816, Accuracy: 0.0\n",
            "  Batch 5790/8000, Loss: 0.40460240840911865, Accuracy: 1.0\n",
            "  Batch 5791/8000, Loss: 0.10687519609928131, Accuracy: 1.0\n",
            "  Batch 5792/8000, Loss: 0.3013725280761719, Accuracy: 1.0\n",
            "  Batch 5793/8000, Loss: 0.10600274056196213, Accuracy: 1.0\n",
            "  Batch 5794/8000, Loss: 0.10960196703672409, Accuracy: 1.0\n",
            "  Batch 5795/8000, Loss: 0.3321700096130371, Accuracy: 1.0\n",
            "  Batch 5796/8000, Loss: 0.38550907373428345, Accuracy: 1.0\n",
            "  Batch 5797/8000, Loss: 0.26933997869491577, Accuracy: 1.0\n",
            "  Batch 5798/8000, Loss: 0.10687732696533203, Accuracy: 1.0\n",
            "  Batch 5799/8000, Loss: 0.10722331702709198, Accuracy: 1.0\n",
            "  Batch 5800/8000, Loss: 0.27770063281059265, Accuracy: 1.0\n",
            "  Batch 5801/8000, Loss: 0.7262275218963623, Accuracy: 1.0\n",
            "  Batch 5802/8000, Loss: 0.9357131719589233, Accuracy: 0.0\n",
            "  Batch 5803/8000, Loss: 0.37873226404190063, Accuracy: 1.0\n",
            "  Batch 5804/8000, Loss: 0.31625327467918396, Accuracy: 1.0\n",
            "  Batch 5805/8000, Loss: 0.9439245462417603, Accuracy: 0.0\n",
            "  Batch 5806/8000, Loss: 0.7823202610015869, Accuracy: 1.0\n",
            "  Batch 5807/8000, Loss: 0.10586562752723694, Accuracy: 1.0\n",
            "  Batch 5808/8000, Loss: 0.24301142990589142, Accuracy: 1.0\n",
            "  Batch 5809/8000, Loss: 0.2926395535469055, Accuracy: 1.0\n",
            "  Batch 5810/8000, Loss: 0.22543928027153015, Accuracy: 1.0\n",
            "  Batch 5811/8000, Loss: 0.9074380397796631, Accuracy: 0.0\n",
            "  Batch 5812/8000, Loss: 0.6133587956428528, Accuracy: 1.0\n",
            "  Batch 5813/8000, Loss: 2.0727381706237793, Accuracy: 0.0\n",
            "  Batch 5814/8000, Loss: 0.10764957964420319, Accuracy: 1.0\n",
            "  Batch 5815/8000, Loss: 0.10731448233127594, Accuracy: 1.0\n",
            "  Batch 5816/8000, Loss: 0.10581034421920776, Accuracy: 1.0\n",
            "  Batch 5817/8000, Loss: 0.22282490134239197, Accuracy: 1.0\n",
            "  Batch 5818/8000, Loss: 0.3215942978858948, Accuracy: 1.0\n",
            "  Batch 5819/8000, Loss: 1.5328803062438965, Accuracy: 0.0\n",
            "  Batch 5820/8000, Loss: 2.0627641677856445, Accuracy: 0.0\n",
            "  Batch 5821/8000, Loss: 0.1058163121342659, Accuracy: 1.0\n",
            "  Batch 5822/8000, Loss: 1.4633371829986572, Accuracy: 0.0\n",
            "  Batch 5823/8000, Loss: 0.243353933095932, Accuracy: 1.0\n",
            "  Batch 5824/8000, Loss: 0.10581380128860474, Accuracy: 1.0\n",
            "  Batch 5825/8000, Loss: 0.3435019254684448, Accuracy: 1.0\n",
            "  Batch 5826/8000, Loss: 0.26334089040756226, Accuracy: 1.0\n",
            "  Batch 5827/8000, Loss: 0.5743567943572998, Accuracy: 1.0\n",
            "  Batch 5828/8000, Loss: 0.7965508699417114, Accuracy: 1.0\n",
            "  Batch 5829/8000, Loss: 0.2617731988430023, Accuracy: 1.0\n",
            "  Batch 5830/8000, Loss: 0.6949148178100586, Accuracy: 1.0\n",
            "  Batch 5831/8000, Loss: 0.10792939364910126, Accuracy: 1.0\n",
            "  Batch 5832/8000, Loss: 0.27570706605911255, Accuracy: 1.0\n",
            "  Batch 5833/8000, Loss: 0.3965335488319397, Accuracy: 1.0\n",
            "  Batch 5834/8000, Loss: 0.7474861145019531, Accuracy: 1.0\n",
            "  Batch 5835/8000, Loss: 1.122632384300232, Accuracy: 0.0\n",
            "  Batch 5836/8000, Loss: 1.4892990589141846, Accuracy: 0.0\n",
            "  Batch 5837/8000, Loss: 0.10888615250587463, Accuracy: 1.0\n",
            "  Batch 5838/8000, Loss: 0.41302233934402466, Accuracy: 1.0\n",
            "  Batch 5839/8000, Loss: 0.3223039209842682, Accuracy: 1.0\n",
            "  Batch 5840/8000, Loss: 0.1083974540233612, Accuracy: 1.0\n",
            "  Batch 5841/8000, Loss: 0.826630711555481, Accuracy: 0.0\n",
            "  Batch 5842/8000, Loss: 0.2558373808860779, Accuracy: 1.0\n",
            "  Batch 5843/8000, Loss: 0.32678255438804626, Accuracy: 1.0\n",
            "  Batch 5844/8000, Loss: 0.26566365361213684, Accuracy: 1.0\n",
            "  Batch 5845/8000, Loss: 0.10754489153623581, Accuracy: 1.0\n",
            "  Batch 5846/8000, Loss: 0.2709457278251648, Accuracy: 1.0\n",
            "  Batch 5847/8000, Loss: 0.24592594802379608, Accuracy: 1.0\n",
            "  Batch 5848/8000, Loss: 1.0123937129974365, Accuracy: 0.0\n",
            "  Batch 5849/8000, Loss: 1.4924900531768799, Accuracy: 0.0\n",
            "  Batch 5850/8000, Loss: 0.2649761438369751, Accuracy: 1.0\n",
            "  Batch 5851/8000, Loss: 0.24419543147087097, Accuracy: 1.0\n",
            "  Batch 5852/8000, Loss: 1.325859785079956, Accuracy: 0.0\n",
            "  Batch 5853/8000, Loss: 0.2714071273803711, Accuracy: 1.0\n",
            "  Batch 5854/8000, Loss: 0.35399457812309265, Accuracy: 1.0\n",
            "  Batch 5855/8000, Loss: 0.340745210647583, Accuracy: 1.0\n",
            "  Batch 5856/8000, Loss: 0.5145147442817688, Accuracy: 1.0\n",
            "  Batch 5857/8000, Loss: 0.10678915679454803, Accuracy: 1.0\n",
            "  Batch 5858/8000, Loss: 0.3335171341896057, Accuracy: 1.0\n",
            "  Batch 5859/8000, Loss: 0.3297503590583801, Accuracy: 1.0\n",
            "  Batch 5860/8000, Loss: 1.9529086351394653, Accuracy: 0.0\n",
            "  Batch 5861/8000, Loss: 1.2764580249786377, Accuracy: 0.0\n",
            "  Batch 5862/8000, Loss: 0.10650999844074249, Accuracy: 1.0\n",
            "  Batch 5863/8000, Loss: 1.9033513069152832, Accuracy: 0.0\n",
            "  Batch 5864/8000, Loss: 0.3104471266269684, Accuracy: 1.0\n",
            "  Batch 5865/8000, Loss: 0.4042775630950928, Accuracy: 1.0\n",
            "  Batch 5866/8000, Loss: 1.112845540046692, Accuracy: 0.0\n",
            "  Batch 5867/8000, Loss: 0.36135995388031006, Accuracy: 1.0\n",
            "  Batch 5868/8000, Loss: 0.10632158815860748, Accuracy: 1.0\n",
            "  Batch 5869/8000, Loss: 0.2768784761428833, Accuracy: 1.0\n",
            "  Batch 5870/8000, Loss: 0.2830011248588562, Accuracy: 1.0\n",
            "  Batch 5871/8000, Loss: 1.4503917694091797, Accuracy: 0.0\n",
            "  Batch 5872/8000, Loss: 0.29793310165405273, Accuracy: 1.0\n",
            "  Batch 5873/8000, Loss: 0.29437321424484253, Accuracy: 1.0\n",
            "  Batch 5874/8000, Loss: 0.10548318922519684, Accuracy: 1.0\n",
            "  Batch 5875/8000, Loss: 0.31224316358566284, Accuracy: 1.0\n",
            "  Batch 5876/8000, Loss: 0.45099544525146484, Accuracy: 1.0\n",
            "  Batch 5877/8000, Loss: 0.40338602662086487, Accuracy: 1.0\n",
            "  Batch 5878/8000, Loss: 0.1128201112151146, Accuracy: 1.0\n",
            "  Batch 5879/8000, Loss: 1.1977335214614868, Accuracy: 0.0\n",
            "  Batch 5880/8000, Loss: 0.10550511628389359, Accuracy: 1.0\n",
            "  Batch 5881/8000, Loss: 0.37016692757606506, Accuracy: 1.0\n",
            "  Batch 5882/8000, Loss: 0.36536693572998047, Accuracy: 1.0\n",
            "  Batch 5883/8000, Loss: 0.26906436681747437, Accuracy: 1.0\n",
            "  Batch 5884/8000, Loss: 0.10641971230506897, Accuracy: 1.0\n",
            "  Batch 5885/8000, Loss: 1.8527653217315674, Accuracy: 0.0\n",
            "  Batch 5886/8000, Loss: 0.16261360049247742, Accuracy: 1.0\n",
            "  Batch 5887/8000, Loss: 0.10543718934059143, Accuracy: 1.0\n",
            "  Batch 5888/8000, Loss: 1.9500041007995605, Accuracy: 0.0\n",
            "  Batch 5889/8000, Loss: 0.3074006140232086, Accuracy: 1.0\n",
            "  Batch 5890/8000, Loss: 0.3112196922302246, Accuracy: 1.0\n",
            "  Batch 5891/8000, Loss: 0.29998281598091125, Accuracy: 1.0\n",
            "  Batch 5892/8000, Loss: 0.1060723066329956, Accuracy: 1.0\n",
            "  Batch 5893/8000, Loss: 0.3870619535446167, Accuracy: 1.0\n",
            "  Batch 5894/8000, Loss: 0.31872138381004333, Accuracy: 1.0\n",
            "  Batch 5895/8000, Loss: 0.3691042363643646, Accuracy: 1.0\n",
            "  Batch 5896/8000, Loss: 0.550754964351654, Accuracy: 1.0\n",
            "  Batch 5897/8000, Loss: 0.10749409347772598, Accuracy: 1.0\n",
            "  Batch 5898/8000, Loss: 0.30172187089920044, Accuracy: 1.0\n",
            "  Batch 5899/8000, Loss: 0.38983142375946045, Accuracy: 1.0\n",
            "  Batch 5900/8000, Loss: 1.8545101881027222, Accuracy: 0.0\n",
            "  Batch 5901/8000, Loss: 0.3252696394920349, Accuracy: 1.0\n",
            "  Batch 5902/8000, Loss: 0.10639546066522598, Accuracy: 1.0\n",
            "  Batch 5903/8000, Loss: 0.5323769450187683, Accuracy: 1.0\n",
            "  Batch 5904/8000, Loss: 0.880624532699585, Accuracy: 0.0\n",
            "  Batch 5905/8000, Loss: 0.10929038375616074, Accuracy: 1.0\n",
            "  Batch 5906/8000, Loss: 0.106512650847435, Accuracy: 1.0\n",
            "  Batch 5907/8000, Loss: 0.37002038955688477, Accuracy: 1.0\n",
            "  Batch 5908/8000, Loss: 0.2714178264141083, Accuracy: 1.0\n",
            "  Batch 5909/8000, Loss: 0.4075123071670532, Accuracy: 1.0\n",
            "  Batch 5910/8000, Loss: 0.29914015531539917, Accuracy: 1.0\n",
            "  Batch 5911/8000, Loss: 0.3297748267650604, Accuracy: 1.0\n",
            "  Batch 5912/8000, Loss: 0.322244256734848, Accuracy: 1.0\n",
            "  Batch 5913/8000, Loss: 0.25821882486343384, Accuracy: 1.0\n",
            "  Batch 5914/8000, Loss: 0.3292267322540283, Accuracy: 1.0\n",
            "  Batch 5915/8000, Loss: 0.10693743079900742, Accuracy: 1.0\n",
            "  Batch 5916/8000, Loss: 0.3606685698032379, Accuracy: 1.0\n",
            "  Batch 5917/8000, Loss: 0.25183820724487305, Accuracy: 1.0\n",
            "  Batch 5918/8000, Loss: 0.26388120651245117, Accuracy: 1.0\n",
            "  Batch 5919/8000, Loss: 0.2512410879135132, Accuracy: 1.0\n",
            "  Batch 5920/8000, Loss: 0.3587489128112793, Accuracy: 1.0\n",
            "  Batch 5921/8000, Loss: 0.24425429105758667, Accuracy: 1.0\n",
            "  Batch 5922/8000, Loss: 0.13523238897323608, Accuracy: 1.0\n",
            "  Batch 5923/8000, Loss: 0.21864071488380432, Accuracy: 1.0\n",
            "  Batch 5924/8000, Loss: 1.5431950092315674, Accuracy: 0.0\n",
            "  Batch 5925/8000, Loss: 0.2344668060541153, Accuracy: 1.0\n",
            "  Batch 5926/8000, Loss: 0.3605705797672272, Accuracy: 1.0\n",
            "  Batch 5927/8000, Loss: 0.8577755689620972, Accuracy: 0.0\n",
            "  Batch 5928/8000, Loss: 0.23016104102134705, Accuracy: 1.0\n",
            "  Batch 5929/8000, Loss: 0.2959286570549011, Accuracy: 1.0\n",
            "  Batch 5930/8000, Loss: 2.429729700088501, Accuracy: 0.0\n",
            "  Batch 5931/8000, Loss: 0.22371920943260193, Accuracy: 1.0\n",
            "  Batch 5932/8000, Loss: 0.10623036324977875, Accuracy: 1.0\n",
            "  Batch 5933/8000, Loss: 0.11869235336780548, Accuracy: 1.0\n",
            "  Batch 5934/8000, Loss: 0.11036263406276703, Accuracy: 1.0\n",
            "  Batch 5935/8000, Loss: 1.0267224311828613, Accuracy: 0.0\n",
            "  Batch 5936/8000, Loss: 0.6389093399047852, Accuracy: 1.0\n",
            "  Batch 5937/8000, Loss: 0.24424688518047333, Accuracy: 1.0\n",
            "  Batch 5938/8000, Loss: 0.289129376411438, Accuracy: 1.0\n",
            "  Batch 5939/8000, Loss: 0.13276712596416473, Accuracy: 1.0\n",
            "  Batch 5940/8000, Loss: 0.10516297817230225, Accuracy: 1.0\n",
            "  Batch 5941/8000, Loss: 0.26886236667633057, Accuracy: 1.0\n",
            "  Batch 5942/8000, Loss: 0.8829615712165833, Accuracy: 0.0\n",
            "  Batch 5943/8000, Loss: 0.26017382740974426, Accuracy: 1.0\n",
            "  Batch 5944/8000, Loss: 0.10627399384975433, Accuracy: 1.0\n",
            "  Batch 5945/8000, Loss: 0.6491488814353943, Accuracy: 1.0\n",
            "  Batch 5946/8000, Loss: 0.23409314453601837, Accuracy: 1.0\n",
            "  Batch 5947/8000, Loss: 0.2348782867193222, Accuracy: 1.0\n",
            "  Batch 5948/8000, Loss: 0.32340893149375916, Accuracy: 1.0\n",
            "  Batch 5949/8000, Loss: 2.157923936843872, Accuracy: 0.0\n",
            "  Batch 5950/8000, Loss: 0.11091990768909454, Accuracy: 1.0\n",
            "  Batch 5951/8000, Loss: 0.10866805911064148, Accuracy: 1.0\n",
            "  Batch 5952/8000, Loss: 0.12108058482408524, Accuracy: 1.0\n",
            "  Batch 5953/8000, Loss: 0.1050979271531105, Accuracy: 1.0\n",
            "  Batch 5954/8000, Loss: 0.11716034263372421, Accuracy: 1.0\n",
            "  Batch 5955/8000, Loss: 0.30711629986763, Accuracy: 1.0\n",
            "  Batch 5956/8000, Loss: 0.24541181325912476, Accuracy: 1.0\n",
            "  Batch 5957/8000, Loss: 0.2456229329109192, Accuracy: 1.0\n",
            "  Batch 5958/8000, Loss: 0.2263251543045044, Accuracy: 1.0\n",
            "  Batch 5959/8000, Loss: 0.24247972667217255, Accuracy: 1.0\n",
            "  Batch 5960/8000, Loss: 0.10504786670207977, Accuracy: 1.0\n",
            "  Batch 5961/8000, Loss: 0.10506287217140198, Accuracy: 1.0\n",
            "  Batch 5962/8000, Loss: 0.838915228843689, Accuracy: 0.0\n",
            "  Batch 5963/8000, Loss: 0.2952479124069214, Accuracy: 1.0\n",
            "  Batch 5964/8000, Loss: 0.49456408619880676, Accuracy: 1.0\n",
            "  Batch 5965/8000, Loss: 0.11054636538028717, Accuracy: 1.0\n",
            "  Batch 5966/8000, Loss: 0.10514974594116211, Accuracy: 1.0\n",
            "  Batch 5967/8000, Loss: 0.10563956201076508, Accuracy: 1.0\n",
            "  Batch 5968/8000, Loss: 1.7726391553878784, Accuracy: 0.0\n",
            "  Batch 5969/8000, Loss: 0.10516452044248581, Accuracy: 1.0\n",
            "  Batch 5970/8000, Loss: 0.10518646240234375, Accuracy: 1.0\n",
            "  Batch 5971/8000, Loss: 0.3134276866912842, Accuracy: 1.0\n",
            "  Batch 5972/8000, Loss: 0.10554085671901703, Accuracy: 1.0\n",
            "  Batch 5973/8000, Loss: 0.2506527304649353, Accuracy: 1.0\n",
            "  Batch 5974/8000, Loss: 1.040628433227539, Accuracy: 0.0\n",
            "  Batch 5975/8000, Loss: 0.1067560464143753, Accuracy: 1.0\n",
            "  Batch 5976/8000, Loss: 0.3916715979576111, Accuracy: 1.0\n",
            "  Batch 5977/8000, Loss: 0.24795660376548767, Accuracy: 1.0\n",
            "  Batch 5978/8000, Loss: 0.27792808413505554, Accuracy: 1.0\n",
            "  Batch 5979/8000, Loss: 0.4127347469329834, Accuracy: 1.0\n",
            "  Batch 5980/8000, Loss: 0.10498036444187164, Accuracy: 1.0\n",
            "  Batch 5981/8000, Loss: 1.7634801864624023, Accuracy: 0.0\n",
            "  Batch 5982/8000, Loss: 0.2503477931022644, Accuracy: 1.0\n",
            "  Batch 5983/8000, Loss: 0.10659923404455185, Accuracy: 1.0\n",
            "  Batch 5984/8000, Loss: 0.26159611344337463, Accuracy: 1.0\n",
            "  Batch 5985/8000, Loss: 2.0736868381500244, Accuracy: 0.0\n",
            "  Batch 5986/8000, Loss: 0.1049305647611618, Accuracy: 1.0\n",
            "  Batch 5987/8000, Loss: 0.24849596619606018, Accuracy: 1.0\n",
            "  Batch 5988/8000, Loss: 0.27780652046203613, Accuracy: 1.0\n",
            "  Batch 5989/8000, Loss: 2.1047706604003906, Accuracy: 0.0\n",
            "  Batch 5990/8000, Loss: 0.10561954975128174, Accuracy: 1.0\n",
            "  Batch 5991/8000, Loss: 1.5611084699630737, Accuracy: 0.0\n",
            "  Batch 5992/8000, Loss: 0.10488685965538025, Accuracy: 1.0\n",
            "  Batch 5993/8000, Loss: 0.4662584364414215, Accuracy: 1.0\n",
            "  Batch 5994/8000, Loss: 0.11308669298887253, Accuracy: 1.0\n",
            "  Batch 5995/8000, Loss: 0.866067111492157, Accuracy: 0.0\n",
            "  Batch 5996/8000, Loss: 1.0228127241134644, Accuracy: 0.0\n",
            "  Batch 5997/8000, Loss: 0.3196686804294586, Accuracy: 1.0\n",
            "  Batch 5998/8000, Loss: 0.4108983278274536, Accuracy: 1.0\n",
            "  Batch 5999/8000, Loss: 0.9436154961585999, Accuracy: 0.0\n",
            "  Batch 6000/8000, Loss: 0.28996315598487854, Accuracy: 1.0\n",
            "  Batch 6001/8000, Loss: 0.12450528889894485, Accuracy: 1.0\n",
            "  Batch 6002/8000, Loss: 0.3958042860031128, Accuracy: 1.0\n",
            "  Batch 6003/8000, Loss: 0.27817317843437195, Accuracy: 1.0\n",
            "  Batch 6004/8000, Loss: 0.4643409252166748, Accuracy: 1.0\n",
            "  Batch 6005/8000, Loss: 0.49003610014915466, Accuracy: 1.0\n",
            "  Batch 6006/8000, Loss: 2.9501969814300537, Accuracy: 0.0\n",
            "  Batch 6007/8000, Loss: 0.36001041531562805, Accuracy: 1.0\n",
            "  Batch 6008/8000, Loss: 0.10626247525215149, Accuracy: 1.0\n",
            "  Batch 6009/8000, Loss: 0.29683494567871094, Accuracy: 1.0\n",
            "  Batch 6010/8000, Loss: 0.7526260614395142, Accuracy: 1.0\n",
            "  Batch 6011/8000, Loss: 0.3945674002170563, Accuracy: 1.0\n",
            "  Batch 6012/8000, Loss: 0.10615310072898865, Accuracy: 1.0\n",
            "  Batch 6013/8000, Loss: 0.43099918961524963, Accuracy: 1.0\n",
            "  Batch 6014/8000, Loss: 0.9707370400428772, Accuracy: 0.0\n",
            "  Batch 6015/8000, Loss: 0.6719331741333008, Accuracy: 1.0\n",
            "  Batch 6016/8000, Loss: 0.274800568819046, Accuracy: 1.0\n",
            "  Batch 6017/8000, Loss: 0.2401459813117981, Accuracy: 1.0\n",
            "  Batch 6018/8000, Loss: 0.8202335238456726, Accuracy: 0.0\n",
            "  Batch 6019/8000, Loss: 0.5982487797737122, Accuracy: 1.0\n",
            "  Batch 6020/8000, Loss: 0.2482704222202301, Accuracy: 1.0\n",
            "  Batch 6021/8000, Loss: 0.6078408360481262, Accuracy: 1.0\n",
            "  Batch 6022/8000, Loss: 0.26524215936660767, Accuracy: 1.0\n",
            "  Batch 6023/8000, Loss: 0.2640392482280731, Accuracy: 1.0\n",
            "  Batch 6024/8000, Loss: 0.7379518747329712, Accuracy: 1.0\n",
            "  Batch 6025/8000, Loss: 2.271059036254883, Accuracy: 0.0\n",
            "  Batch 6026/8000, Loss: 0.2855633795261383, Accuracy: 1.0\n",
            "  Batch 6027/8000, Loss: 0.3286285400390625, Accuracy: 1.0\n",
            "  Batch 6028/8000, Loss: 0.7393288612365723, Accuracy: 1.0\n",
            "  Batch 6029/8000, Loss: 0.23892715573310852, Accuracy: 1.0\n",
            "  Batch 6030/8000, Loss: 0.2395673394203186, Accuracy: 1.0\n",
            "  Batch 6031/8000, Loss: 0.2866356670856476, Accuracy: 1.0\n",
            "  Batch 6032/8000, Loss: 2.2131519317626953, Accuracy: 0.0\n",
            "  Batch 6033/8000, Loss: 0.21096304059028625, Accuracy: 1.0\n",
            "  Batch 6034/8000, Loss: 0.25742676854133606, Accuracy: 1.0\n",
            "  Batch 6035/8000, Loss: 0.7324547171592712, Accuracy: 1.0\n",
            "  Batch 6036/8000, Loss: 0.25513947010040283, Accuracy: 1.0\n",
            "  Batch 6037/8000, Loss: 0.2167460024356842, Accuracy: 1.0\n",
            "  Batch 6038/8000, Loss: 0.2814882695674896, Accuracy: 1.0\n",
            "  Batch 6039/8000, Loss: 0.107021763920784, Accuracy: 1.0\n",
            "  Batch 6040/8000, Loss: 1.4929287433624268, Accuracy: 0.0\n",
            "  Batch 6041/8000, Loss: 0.10835593938827515, Accuracy: 1.0\n",
            "  Batch 6042/8000, Loss: 0.34141895174980164, Accuracy: 1.0\n",
            "  Batch 6043/8000, Loss: 0.10461940616369247, Accuracy: 1.0\n",
            "  Batch 6044/8000, Loss: 0.1081589087843895, Accuracy: 1.0\n",
            "  Batch 6045/8000, Loss: 0.10471460968255997, Accuracy: 1.0\n",
            "  Batch 6046/8000, Loss: 0.95340895652771, Accuracy: 0.0\n",
            "  Batch 6047/8000, Loss: 0.272574245929718, Accuracy: 1.0\n",
            "  Batch 6048/8000, Loss: 0.7314651608467102, Accuracy: 1.0\n",
            "  Batch 6049/8000, Loss: 0.6894873976707458, Accuracy: 1.0\n",
            "  Batch 6050/8000, Loss: 0.3196742832660675, Accuracy: 1.0\n",
            "  Batch 6051/8000, Loss: 0.26251789927482605, Accuracy: 1.0\n",
            "  Batch 6052/8000, Loss: 0.22485023736953735, Accuracy: 1.0\n",
            "  Batch 6053/8000, Loss: 0.2580336928367615, Accuracy: 1.0\n",
            "  Batch 6054/8000, Loss: 0.22296395897865295, Accuracy: 1.0\n",
            "  Batch 6055/8000, Loss: 0.4722944498062134, Accuracy: 1.0\n",
            "  Batch 6056/8000, Loss: 0.11116932332515717, Accuracy: 1.0\n",
            "  Batch 6057/8000, Loss: 0.2801682949066162, Accuracy: 1.0\n",
            "  Batch 6058/8000, Loss: 1.1624746322631836, Accuracy: 0.0\n",
            "  Batch 6059/8000, Loss: 0.10477574169635773, Accuracy: 1.0\n",
            "  Batch 6060/8000, Loss: 0.10931508243083954, Accuracy: 1.0\n",
            "  Batch 6061/8000, Loss: 0.25595712661743164, Accuracy: 1.0\n",
            "  Batch 6062/8000, Loss: 0.22404472529888153, Accuracy: 1.0\n",
            "  Batch 6063/8000, Loss: 0.2327018529176712, Accuracy: 1.0\n",
            "  Batch 6064/8000, Loss: 0.5577764511108398, Accuracy: 1.0\n",
            "  Batch 6065/8000, Loss: 0.2928989827632904, Accuracy: 1.0\n",
            "  Batch 6066/8000, Loss: 0.26193293929100037, Accuracy: 1.0\n",
            "  Batch 6067/8000, Loss: 0.22369813919067383, Accuracy: 1.0\n",
            "  Batch 6068/8000, Loss: 0.568322479724884, Accuracy: 1.0\n",
            "  Batch 6069/8000, Loss: 1.0536977052688599, Accuracy: 0.0\n",
            "  Batch 6070/8000, Loss: 0.30560582876205444, Accuracy: 1.0\n",
            "  Batch 6071/8000, Loss: 0.10449592769145966, Accuracy: 1.0\n",
            "  Batch 6072/8000, Loss: 0.10937722027301788, Accuracy: 1.0\n",
            "  Batch 6073/8000, Loss: 0.2299296110868454, Accuracy: 1.0\n",
            "  Batch 6074/8000, Loss: 0.11026430130004883, Accuracy: 1.0\n",
            "  Batch 6075/8000, Loss: 2.5033838748931885, Accuracy: 0.0\n",
            "  Batch 6076/8000, Loss: 0.18571454286575317, Accuracy: 1.0\n",
            "  Batch 6077/8000, Loss: 0.10448291897773743, Accuracy: 1.0\n",
            "  Batch 6078/8000, Loss: 0.20066697895526886, Accuracy: 1.0\n",
            "  Batch 6079/8000, Loss: 0.22371838986873627, Accuracy: 1.0\n",
            "  Batch 6080/8000, Loss: 0.1044568419456482, Accuracy: 1.0\n",
            "  Batch 6081/8000, Loss: 2.424182891845703, Accuracy: 0.0\n",
            "  Batch 6082/8000, Loss: 0.9140166640281677, Accuracy: 0.0\n",
            "  Batch 6083/8000, Loss: 0.57988440990448, Accuracy: 1.0\n",
            "  Batch 6084/8000, Loss: 0.8011616468429565, Accuracy: 0.0\n",
            "  Batch 6085/8000, Loss: 0.11202892661094666, Accuracy: 1.0\n",
            "  Batch 6086/8000, Loss: 0.10826572775840759, Accuracy: 1.0\n",
            "  Batch 6087/8000, Loss: 0.8040691614151001, Accuracy: 0.0\n",
            "  Batch 6088/8000, Loss: 0.10900445282459259, Accuracy: 1.0\n",
            "  Batch 6089/8000, Loss: 0.10738734155893326, Accuracy: 1.0\n",
            "  Batch 6090/8000, Loss: 0.3723490238189697, Accuracy: 1.0\n",
            "  Batch 6091/8000, Loss: 0.22912539541721344, Accuracy: 1.0\n",
            "  Batch 6092/8000, Loss: 0.9308209419250488, Accuracy: 0.0\n",
            "  Batch 6093/8000, Loss: 0.21565422415733337, Accuracy: 1.0\n",
            "  Batch 6094/8000, Loss: 0.10893245041370392, Accuracy: 1.0\n",
            "  Batch 6095/8000, Loss: 0.48696887493133545, Accuracy: 1.0\n",
            "  Batch 6096/8000, Loss: 0.11272463947534561, Accuracy: 1.0\n",
            "  Batch 6097/8000, Loss: 2.1935696601867676, Accuracy: 0.0\n",
            "  Batch 6098/8000, Loss: 0.22610124945640564, Accuracy: 1.0\n",
            "  Batch 6099/8000, Loss: 0.10761737823486328, Accuracy: 1.0\n",
            "  Batch 6100/8000, Loss: 0.1106986254453659, Accuracy: 1.0\n",
            "  Batch 6101/8000, Loss: 0.20331329107284546, Accuracy: 1.0\n",
            "  Batch 6102/8000, Loss: 0.10954229533672333, Accuracy: 1.0\n",
            "  Batch 6103/8000, Loss: 0.19671480357646942, Accuracy: 1.0\n",
            "  Batch 6104/8000, Loss: 0.25852900743484497, Accuracy: 1.0\n",
            "  Batch 6105/8000, Loss: 0.2439601868391037, Accuracy: 1.0\n",
            "  Batch 6106/8000, Loss: 2.471628427505493, Accuracy: 0.0\n",
            "  Batch 6107/8000, Loss: 0.40530911087989807, Accuracy: 1.0\n",
            "  Batch 6108/8000, Loss: 0.22344666719436646, Accuracy: 1.0\n",
            "  Batch 6109/8000, Loss: 2.015742778778076, Accuracy: 0.0\n",
            "  Batch 6110/8000, Loss: 0.19377325475215912, Accuracy: 1.0\n",
            "  Batch 6111/8000, Loss: 0.7801645398139954, Accuracy: 1.0\n",
            "  Batch 6112/8000, Loss: 0.11309090256690979, Accuracy: 1.0\n",
            "  Batch 6113/8000, Loss: 0.20333777368068695, Accuracy: 1.0\n",
            "  Batch 6114/8000, Loss: 0.11117696762084961, Accuracy: 1.0\n",
            "  Batch 6115/8000, Loss: 1.8968836069107056, Accuracy: 0.0\n",
            "  Batch 6116/8000, Loss: 0.21713508665561676, Accuracy: 1.0\n",
            "  Batch 6117/8000, Loss: 0.46162039041519165, Accuracy: 1.0\n",
            "  Batch 6118/8000, Loss: 2.276710271835327, Accuracy: 0.0\n",
            "  Batch 6119/8000, Loss: 0.2525327503681183, Accuracy: 1.0\n",
            "  Batch 6120/8000, Loss: 0.35346275568008423, Accuracy: 1.0\n",
            "  Batch 6121/8000, Loss: 0.6970812082290649, Accuracy: 1.0\n",
            "  Batch 6122/8000, Loss: 1.2197234630584717, Accuracy: 0.0\n",
            "  Batch 6123/8000, Loss: 0.5140746831893921, Accuracy: 1.0\n",
            "  Batch 6124/8000, Loss: 0.46670055389404297, Accuracy: 1.0\n",
            "  Batch 6125/8000, Loss: 0.16165676712989807, Accuracy: 1.0\n",
            "  Batch 6126/8000, Loss: 0.5076227784156799, Accuracy: 1.0\n",
            "  Batch 6127/8000, Loss: 0.3209528923034668, Accuracy: 1.0\n",
            "  Batch 6128/8000, Loss: 0.3120400905609131, Accuracy: 1.0\n",
            "  Batch 6129/8000, Loss: 0.3219617009162903, Accuracy: 1.0\n",
            "  Batch 6130/8000, Loss: 0.1111702248454094, Accuracy: 1.0\n",
            "  Batch 6131/8000, Loss: 0.2844300866127014, Accuracy: 1.0\n",
            "  Batch 6132/8000, Loss: 0.9988006353378296, Accuracy: 0.0\n",
            "  Batch 6133/8000, Loss: 0.11010746657848358, Accuracy: 1.0\n",
            "  Batch 6134/8000, Loss: 0.3061705529689789, Accuracy: 1.0\n",
            "  Batch 6135/8000, Loss: 0.28181686997413635, Accuracy: 1.0\n",
            "  Batch 6136/8000, Loss: 1.097474217414856, Accuracy: 0.0\n",
            "  Batch 6137/8000, Loss: 0.894834041595459, Accuracy: 0.0\n",
            "  Batch 6138/8000, Loss: 0.4587319493293762, Accuracy: 1.0\n",
            "  Batch 6139/8000, Loss: 0.401107519865036, Accuracy: 1.0\n",
            "  Batch 6140/8000, Loss: 2.021885633468628, Accuracy: 0.0\n",
            "  Batch 6141/8000, Loss: 1.9885098934173584, Accuracy: 0.0\n",
            "  Batch 6142/8000, Loss: 0.26580193638801575, Accuracy: 1.0\n",
            "  Batch 6143/8000, Loss: 0.38126686215400696, Accuracy: 1.0\n",
            "  Batch 6144/8000, Loss: 0.2732364535331726, Accuracy: 1.0\n",
            "  Batch 6145/8000, Loss: 0.2999507188796997, Accuracy: 1.0\n",
            "  Batch 6146/8000, Loss: 0.40760013461112976, Accuracy: 1.0\n",
            "  Batch 6147/8000, Loss: 0.29975026845932007, Accuracy: 1.0\n",
            "  Batch 6148/8000, Loss: 0.30361610651016235, Accuracy: 1.0\n",
            "  Batch 6149/8000, Loss: 1.0008801221847534, Accuracy: 0.0\n",
            "  Batch 6150/8000, Loss: 1.7451531887054443, Accuracy: 0.0\n",
            "  Batch 6151/8000, Loss: 0.13639035820960999, Accuracy: 1.0\n",
            "  Batch 6152/8000, Loss: 0.44334590435028076, Accuracy: 1.0\n",
            "  Batch 6153/8000, Loss: 0.3014211356639862, Accuracy: 1.0\n",
            "  Batch 6154/8000, Loss: 0.5008030533790588, Accuracy: 1.0\n",
            "  Batch 6155/8000, Loss: 1.769547700881958, Accuracy: 0.0\n",
            "  Batch 6156/8000, Loss: 1.411555528640747, Accuracy: 0.0\n",
            "  Batch 6157/8000, Loss: 0.10628867149353027, Accuracy: 1.0\n",
            "  Batch 6158/8000, Loss: 0.30002009868621826, Accuracy: 1.0\n",
            "  Batch 6159/8000, Loss: 0.661084771156311, Accuracy: 1.0\n",
            "  Batch 6160/8000, Loss: 0.8752092719078064, Accuracy: 0.0\n",
            "  Batch 6161/8000, Loss: 0.333598256111145, Accuracy: 1.0\n",
            "  Batch 6162/8000, Loss: 0.10397973656654358, Accuracy: 1.0\n",
            "  Batch 6163/8000, Loss: 0.8386725187301636, Accuracy: 0.0\n",
            "  Batch 6164/8000, Loss: 0.44491663575172424, Accuracy: 1.0\n",
            "  Batch 6165/8000, Loss: 0.356623113155365, Accuracy: 1.0\n",
            "  Batch 6166/8000, Loss: 0.10619459301233292, Accuracy: 1.0\n",
            "  Batch 6167/8000, Loss: 0.10664494335651398, Accuracy: 1.0\n",
            "  Batch 6168/8000, Loss: 0.33268240094184875, Accuracy: 1.0\n",
            "  Batch 6169/8000, Loss: 0.10671637952327728, Accuracy: 1.0\n",
            "  Batch 6170/8000, Loss: 1.0702842473983765, Accuracy: 0.0\n",
            "  Batch 6171/8000, Loss: 0.5891156792640686, Accuracy: 1.0\n",
            "  Batch 6172/8000, Loss: 0.17942820489406586, Accuracy: 1.0\n",
            "  Batch 6173/8000, Loss: 0.8468841314315796, Accuracy: 0.0\n",
            "  Batch 6174/8000, Loss: 0.1059252992272377, Accuracy: 1.0\n",
            "  Batch 6175/8000, Loss: 0.36470943689346313, Accuracy: 1.0\n",
            "  Batch 6176/8000, Loss: 0.10390473902225494, Accuracy: 1.0\n",
            "  Batch 6177/8000, Loss: 1.048119068145752, Accuracy: 0.0\n",
            "  Batch 6178/8000, Loss: 0.739332377910614, Accuracy: 1.0\n",
            "  Batch 6179/8000, Loss: 1.8260960578918457, Accuracy: 0.0\n",
            "  Batch 6180/8000, Loss: 0.11128957569599152, Accuracy: 1.0\n",
            "  Batch 6181/8000, Loss: 0.10388138145208359, Accuracy: 1.0\n",
            "  Batch 6182/8000, Loss: 0.12002241611480713, Accuracy: 1.0\n",
            "  Batch 6183/8000, Loss: 1.1840715408325195, Accuracy: 0.0\n",
            "  Batch 6184/8000, Loss: 0.4683448076248169, Accuracy: 1.0\n",
            "  Batch 6185/8000, Loss: 0.32471129298210144, Accuracy: 1.0\n",
            "  Batch 6186/8000, Loss: 0.10554361343383789, Accuracy: 1.0\n",
            "  Batch 6187/8000, Loss: 1.046410322189331, Accuracy: 0.0\n",
            "  Batch 6188/8000, Loss: 0.3202139735221863, Accuracy: 1.0\n",
            "  Batch 6189/8000, Loss: 1.5949277877807617, Accuracy: 0.0\n",
            "  Batch 6190/8000, Loss: 0.10479611158370972, Accuracy: 1.0\n",
            "  Batch 6191/8000, Loss: 0.36474841833114624, Accuracy: 1.0\n",
            "  Batch 6192/8000, Loss: 0.1055610179901123, Accuracy: 1.0\n",
            "  Batch 6193/8000, Loss: 0.4098733067512512, Accuracy: 1.0\n",
            "  Batch 6194/8000, Loss: 0.3941883146762848, Accuracy: 1.0\n",
            "  Batch 6195/8000, Loss: 0.8074812293052673, Accuracy: 0.0\n",
            "  Batch 6196/8000, Loss: 0.3368798494338989, Accuracy: 1.0\n",
            "  Batch 6197/8000, Loss: 0.3441123068332672, Accuracy: 1.0\n",
            "  Batch 6198/8000, Loss: 0.1049933210015297, Accuracy: 1.0\n",
            "  Batch 6199/8000, Loss: 0.10386361181735992, Accuracy: 1.0\n",
            "  Batch 6200/8000, Loss: 0.36712297797203064, Accuracy: 1.0\n",
            "  Batch 6201/8000, Loss: 1.6522290706634521, Accuracy: 0.0\n",
            "  Batch 6202/8000, Loss: 1.5932486057281494, Accuracy: 0.0\n",
            "  Batch 6203/8000, Loss: 0.3570880591869354, Accuracy: 1.0\n",
            "  Batch 6204/8000, Loss: 0.2818669378757477, Accuracy: 1.0\n",
            "  Batch 6205/8000, Loss: 0.34267693758010864, Accuracy: 1.0\n",
            "  Batch 6206/8000, Loss: 0.10668040812015533, Accuracy: 1.0\n",
            "  Batch 6207/8000, Loss: 0.7515650391578674, Accuracy: 1.0\n",
            "  Batch 6208/8000, Loss: 0.5508127808570862, Accuracy: 1.0\n",
            "  Batch 6209/8000, Loss: 0.11208328604698181, Accuracy: 1.0\n",
            "  Batch 6210/8000, Loss: 0.12398576736450195, Accuracy: 1.0\n",
            "  Batch 6211/8000, Loss: 0.35956043004989624, Accuracy: 1.0\n",
            "  Batch 6212/8000, Loss: 0.33017775416374207, Accuracy: 1.0\n",
            "  Batch 6213/8000, Loss: 0.35328570008277893, Accuracy: 1.0\n",
            "  Batch 6214/8000, Loss: 0.10379603505134583, Accuracy: 1.0\n",
            "  Batch 6215/8000, Loss: 1.1878021955490112, Accuracy: 0.0\n",
            "  Batch 6216/8000, Loss: 0.3956873118877411, Accuracy: 1.0\n",
            "  Batch 6217/8000, Loss: 0.9620122909545898, Accuracy: 0.0\n",
            "  Batch 6218/8000, Loss: 0.10524699091911316, Accuracy: 1.0\n",
            "  Batch 6219/8000, Loss: 0.3725021481513977, Accuracy: 1.0\n",
            "  Batch 6220/8000, Loss: 0.10372070968151093, Accuracy: 1.0\n",
            "  Batch 6221/8000, Loss: 0.7539882659912109, Accuracy: 1.0\n",
            "  Batch 6222/8000, Loss: 0.3514552712440491, Accuracy: 1.0\n",
            "  Batch 6223/8000, Loss: 0.336540162563324, Accuracy: 1.0\n",
            "  Batch 6224/8000, Loss: 0.10372449457645416, Accuracy: 1.0\n",
            "  Batch 6225/8000, Loss: 0.3459068834781647, Accuracy: 1.0\n",
            "  Batch 6226/8000, Loss: 0.105120450258255, Accuracy: 1.0\n",
            "  Batch 6227/8000, Loss: 0.10380043834447861, Accuracy: 1.0\n",
            "  Batch 6228/8000, Loss: 0.48477500677108765, Accuracy: 1.0\n",
            "  Batch 6229/8000, Loss: 0.34143543243408203, Accuracy: 1.0\n",
            "  Batch 6230/8000, Loss: 0.10362077504396439, Accuracy: 1.0\n",
            "  Batch 6231/8000, Loss: 0.7960221767425537, Accuracy: 1.0\n",
            "  Batch 6232/8000, Loss: 1.7013285160064697, Accuracy: 0.0\n",
            "  Batch 6233/8000, Loss: 0.653794527053833, Accuracy: 1.0\n",
            "  Batch 6234/8000, Loss: 0.6486144661903381, Accuracy: 1.0\n",
            "  Batch 6235/8000, Loss: 0.47800150513648987, Accuracy: 1.0\n",
            "  Batch 6236/8000, Loss: 0.9530647993087769, Accuracy: 0.0\n",
            "  Batch 6237/8000, Loss: 0.6262592077255249, Accuracy: 1.0\n",
            "  Batch 6238/8000, Loss: 0.3265649676322937, Accuracy: 1.0\n",
            "  Batch 6239/8000, Loss: 0.1469551920890808, Accuracy: 1.0\n",
            "  Batch 6240/8000, Loss: 0.33318543434143066, Accuracy: 1.0\n",
            "  Batch 6241/8000, Loss: 0.10357481241226196, Accuracy: 1.0\n",
            "  Batch 6242/8000, Loss: 0.32085251808166504, Accuracy: 1.0\n",
            "  Batch 6243/8000, Loss: 0.4029502272605896, Accuracy: 1.0\n",
            "  Batch 6244/8000, Loss: 0.6762582063674927, Accuracy: 1.0\n",
            "  Batch 6245/8000, Loss: 1.4496026039123535, Accuracy: 0.0\n",
            "  Batch 6246/8000, Loss: 0.10358994454145432, Accuracy: 1.0\n",
            "  Batch 6247/8000, Loss: 0.10593566298484802, Accuracy: 1.0\n",
            "  Batch 6248/8000, Loss: 0.37060511112213135, Accuracy: 1.0\n",
            "  Batch 6249/8000, Loss: 0.29119017720222473, Accuracy: 1.0\n",
            "  Batch 6250/8000, Loss: 0.30982506275177, Accuracy: 1.0\n",
            "  Batch 6251/8000, Loss: 0.5575510263442993, Accuracy: 1.0\n",
            "  Batch 6252/8000, Loss: 1.7489628791809082, Accuracy: 0.0\n",
            "  Batch 6253/8000, Loss: 0.6869940757751465, Accuracy: 1.0\n",
            "  Batch 6254/8000, Loss: 0.28497475385665894, Accuracy: 1.0\n",
            "  Batch 6255/8000, Loss: 0.10511836409568787, Accuracy: 1.0\n",
            "  Batch 6256/8000, Loss: 0.34998849034309387, Accuracy: 1.0\n",
            "  Batch 6257/8000, Loss: 0.5308111310005188, Accuracy: 1.0\n",
            "  Batch 6258/8000, Loss: 0.10787710547447205, Accuracy: 1.0\n",
            "  Batch 6259/8000, Loss: 1.8760648965835571, Accuracy: 0.0\n",
            "  Batch 6260/8000, Loss: 1.2350362539291382, Accuracy: 0.0\n",
            "  Batch 6261/8000, Loss: 0.10760904103517532, Accuracy: 1.0\n",
            "  Batch 6262/8000, Loss: 1.1936603784561157, Accuracy: 0.0\n",
            "  Batch 6263/8000, Loss: 0.10350823402404785, Accuracy: 1.0\n",
            "  Batch 6264/8000, Loss: 1.9345290660858154, Accuracy: 0.0\n",
            "  Batch 6265/8000, Loss: 0.26399993896484375, Accuracy: 1.0\n",
            "  Batch 6266/8000, Loss: 0.30043792724609375, Accuracy: 1.0\n",
            "  Batch 6267/8000, Loss: 0.3503536581993103, Accuracy: 1.0\n",
            "  Batch 6268/8000, Loss: 0.10757479071617126, Accuracy: 1.0\n",
            "  Batch 6269/8000, Loss: 0.7694277763366699, Accuracy: 1.0\n",
            "  Batch 6270/8000, Loss: 0.39502424001693726, Accuracy: 1.0\n",
            "  Batch 6271/8000, Loss: 0.21317635476589203, Accuracy: 1.0\n",
            "  Batch 6272/8000, Loss: 1.7870577573776245, Accuracy: 0.0\n",
            "  Batch 6273/8000, Loss: 0.2935604751110077, Accuracy: 1.0\n",
            "  Batch 6274/8000, Loss: 0.10827264189720154, Accuracy: 1.0\n",
            "  Batch 6275/8000, Loss: 0.10427147150039673, Accuracy: 1.0\n",
            "  Batch 6276/8000, Loss: 0.27805590629577637, Accuracy: 1.0\n",
            "  Batch 6277/8000, Loss: 0.3037993311882019, Accuracy: 1.0\n",
            "  Batch 6278/8000, Loss: 0.4377482831478119, Accuracy: 1.0\n",
            "  Batch 6279/8000, Loss: 0.3408946096897125, Accuracy: 1.0\n",
            "  Batch 6280/8000, Loss: 1.2195197343826294, Accuracy: 0.0\n",
            "  Batch 6281/8000, Loss: 0.3952198922634125, Accuracy: 1.0\n",
            "  Batch 6282/8000, Loss: 0.28695571422576904, Accuracy: 1.0\n",
            "  Batch 6283/8000, Loss: 0.6968318819999695, Accuracy: 1.0\n",
            "  Batch 6284/8000, Loss: 0.39464789628982544, Accuracy: 1.0\n",
            "  Batch 6285/8000, Loss: 0.5617209076881409, Accuracy: 1.0\n",
            "  Batch 6286/8000, Loss: 0.3975338637828827, Accuracy: 1.0\n",
            "  Batch 6287/8000, Loss: 0.8237282037734985, Accuracy: 0.0\n",
            "  Batch 6288/8000, Loss: 1.351308822631836, Accuracy: 0.0\n",
            "  Batch 6289/8000, Loss: 0.2856781780719757, Accuracy: 1.0\n",
            "  Batch 6290/8000, Loss: 0.2189573496580124, Accuracy: 1.0\n",
            "  Batch 6291/8000, Loss: 0.36946457624435425, Accuracy: 1.0\n",
            "  Batch 6292/8000, Loss: 0.2908061742782593, Accuracy: 1.0\n",
            "  Batch 6293/8000, Loss: 0.10333307087421417, Accuracy: 1.0\n",
            "  Batch 6294/8000, Loss: 0.9772166609764099, Accuracy: 0.0\n",
            "  Batch 6295/8000, Loss: 0.647054135799408, Accuracy: 1.0\n",
            "  Batch 6296/8000, Loss: 0.28816211223602295, Accuracy: 1.0\n",
            "  Batch 6297/8000, Loss: 0.10408163070678711, Accuracy: 1.0\n",
            "  Batch 6298/8000, Loss: 0.27158764004707336, Accuracy: 1.0\n",
            "  Batch 6299/8000, Loss: 0.10327456891536713, Accuracy: 1.0\n",
            "  Batch 6300/8000, Loss: 0.10356080532073975, Accuracy: 1.0\n",
            "  Batch 6301/8000, Loss: 0.10395250469446182, Accuracy: 1.0\n",
            "  Batch 6302/8000, Loss: 0.30356714129447937, Accuracy: 1.0\n",
            "  Batch 6303/8000, Loss: 1.088405728340149, Accuracy: 0.0\n",
            "  Batch 6304/8000, Loss: 1.5672744512557983, Accuracy: 0.0\n",
            "  Batch 6305/8000, Loss: 0.5681013464927673, Accuracy: 1.0\n",
            "  Batch 6306/8000, Loss: 0.2684032618999481, Accuracy: 1.0\n",
            "  Batch 6307/8000, Loss: 0.2761751413345337, Accuracy: 1.0\n",
            "  Batch 6308/8000, Loss: 1.8169598579406738, Accuracy: 0.0\n",
            "  Batch 6309/8000, Loss: 0.34358400106430054, Accuracy: 1.0\n",
            "  Batch 6310/8000, Loss: 1.54698646068573, Accuracy: 0.0\n",
            "  Batch 6311/8000, Loss: 0.10342764854431152, Accuracy: 1.0\n",
            "  Batch 6312/8000, Loss: 0.9628885984420776, Accuracy: 0.0\n",
            "  Batch 6313/8000, Loss: 0.9770480990409851, Accuracy: 0.0\n",
            "  Batch 6314/8000, Loss: 0.35111203789711, Accuracy: 1.0\n",
            "  Batch 6315/8000, Loss: 1.5783922672271729, Accuracy: 0.0\n",
            "  Batch 6316/8000, Loss: 0.11565208435058594, Accuracy: 1.0\n",
            "  Batch 6317/8000, Loss: 0.3354818820953369, Accuracy: 1.0\n",
            "  Batch 6318/8000, Loss: 0.3324887752532959, Accuracy: 1.0\n",
            "  Batch 6319/8000, Loss: 0.1032780259847641, Accuracy: 1.0\n",
            "  Batch 6320/8000, Loss: 0.7968345284461975, Accuracy: 0.0\n",
            "  Batch 6321/8000, Loss: 0.3514088988304138, Accuracy: 1.0\n",
            "  Batch 6322/8000, Loss: 0.33685964345932007, Accuracy: 1.0\n",
            "  Batch 6323/8000, Loss: 0.6155616641044617, Accuracy: 1.0\n",
            "  Batch 6324/8000, Loss: 0.10341423004865646, Accuracy: 1.0\n",
            "  Batch 6325/8000, Loss: 0.3494798541069031, Accuracy: 1.0\n",
            "  Batch 6326/8000, Loss: 0.35856398940086365, Accuracy: 1.0\n",
            "  Batch 6327/8000, Loss: 0.10422969609498978, Accuracy: 1.0\n",
            "  Batch 6328/8000, Loss: 0.23383967578411102, Accuracy: 1.0\n",
            "  Batch 6329/8000, Loss: 0.30292052030563354, Accuracy: 1.0\n",
            "  Batch 6330/8000, Loss: 0.37096107006073, Accuracy: 1.0\n",
            "  Batch 6331/8000, Loss: 1.6137347221374512, Accuracy: 0.0\n",
            "  Batch 6332/8000, Loss: 0.10381530225276947, Accuracy: 1.0\n",
            "  Batch 6333/8000, Loss: 1.0125923156738281, Accuracy: 0.0\n",
            "  Batch 6334/8000, Loss: 0.1033301055431366, Accuracy: 1.0\n",
            "  Batch 6335/8000, Loss: 0.4374486207962036, Accuracy: 1.0\n",
            "  Batch 6336/8000, Loss: 0.3250962495803833, Accuracy: 1.0\n",
            "  Batch 6337/8000, Loss: 0.35229045152664185, Accuracy: 1.0\n",
            "  Batch 6338/8000, Loss: 0.10318119823932648, Accuracy: 1.0\n",
            "  Batch 6339/8000, Loss: 0.13564348220825195, Accuracy: 1.0\n",
            "  Batch 6340/8000, Loss: 0.44253116846084595, Accuracy: 1.0\n",
            "  Batch 6341/8000, Loss: 0.10309016704559326, Accuracy: 1.0\n",
            "  Batch 6342/8000, Loss: 0.7237908840179443, Accuracy: 1.0\n",
            "  Batch 6343/8000, Loss: 1.006390929222107, Accuracy: 0.0\n",
            "  Batch 6344/8000, Loss: 0.10326230525970459, Accuracy: 1.0\n",
            "  Batch 6345/8000, Loss: 1.8637150526046753, Accuracy: 0.0\n",
            "  Batch 6346/8000, Loss: 1.704850673675537, Accuracy: 0.0\n",
            "  Batch 6347/8000, Loss: 0.4000358283519745, Accuracy: 1.0\n",
            "  Batch 6348/8000, Loss: 0.10319294035434723, Accuracy: 1.0\n",
            "  Batch 6349/8000, Loss: 0.3493213355541229, Accuracy: 1.0\n",
            "  Batch 6350/8000, Loss: 0.10313618183135986, Accuracy: 1.0\n",
            "  Batch 6351/8000, Loss: 2.8116190433502197, Accuracy: 0.0\n",
            "  Batch 6352/8000, Loss: 1.3374559879302979, Accuracy: 0.0\n",
            "  Batch 6353/8000, Loss: 0.8223176002502441, Accuracy: 0.0\n",
            "  Batch 6354/8000, Loss: 0.9775887131690979, Accuracy: 0.0\n",
            "  Batch 6355/8000, Loss: 1.0191569328308105, Accuracy: 0.0\n",
            "  Batch 6356/8000, Loss: 0.10398976504802704, Accuracy: 1.0\n",
            "  Batch 6357/8000, Loss: 0.6156718730926514, Accuracy: 1.0\n",
            "  Batch 6358/8000, Loss: 1.7095798254013062, Accuracy: 0.0\n",
            "  Batch 6359/8000, Loss: 0.10291998833417892, Accuracy: 1.0\n",
            "  Batch 6360/8000, Loss: 0.642621636390686, Accuracy: 1.0\n",
            "  Batch 6361/8000, Loss: 1.4608256816864014, Accuracy: 0.0\n",
            "  Batch 6362/8000, Loss: 0.10430830717086792, Accuracy: 1.0\n",
            "  Batch 6363/8000, Loss: 0.6824020743370056, Accuracy: 1.0\n",
            "  Batch 6364/8000, Loss: 1.2886884212493896, Accuracy: 0.0\n",
            "  Batch 6365/8000, Loss: 0.10285662114620209, Accuracy: 1.0\n",
            "  Batch 6366/8000, Loss: 0.10484562814235687, Accuracy: 1.0\n",
            "  Batch 6367/8000, Loss: 0.709600031375885, Accuracy: 1.0\n",
            "  Batch 6368/8000, Loss: 1.5094791650772095, Accuracy: 0.0\n",
            "  Batch 6369/8000, Loss: 0.4848850965499878, Accuracy: 1.0\n",
            "  Batch 6370/8000, Loss: 0.7368724942207336, Accuracy: 1.0\n",
            "  Batch 6371/8000, Loss: 0.44549161195755005, Accuracy: 1.0\n",
            "  Batch 6372/8000, Loss: 0.6789849996566772, Accuracy: 1.0\n",
            "  Batch 6373/8000, Loss: 0.411252498626709, Accuracy: 1.0\n",
            "  Batch 6374/8000, Loss: 0.11459575593471527, Accuracy: 1.0\n",
            "  Batch 6375/8000, Loss: 0.9311418533325195, Accuracy: 0.0\n",
            "  Batch 6376/8000, Loss: 0.38243845105171204, Accuracy: 1.0\n",
            "  Batch 6377/8000, Loss: 0.1880529820919037, Accuracy: 1.0\n",
            "  Batch 6378/8000, Loss: 0.6671239137649536, Accuracy: 1.0\n",
            "  Batch 6379/8000, Loss: 0.10282372683286667, Accuracy: 1.0\n",
            "  Batch 6380/8000, Loss: 1.1231969594955444, Accuracy: 0.0\n",
            "  Batch 6381/8000, Loss: 0.11467719078063965, Accuracy: 1.0\n",
            "  Batch 6382/8000, Loss: 0.42542746663093567, Accuracy: 1.0\n",
            "  Batch 6383/8000, Loss: 0.4275069832801819, Accuracy: 1.0\n",
            "  Batch 6384/8000, Loss: 0.9371466636657715, Accuracy: 0.0\n",
            "  Batch 6385/8000, Loss: 0.3856546878814697, Accuracy: 1.0\n",
            "  Batch 6386/8000, Loss: 0.8893709778785706, Accuracy: 0.0\n",
            "  Batch 6387/8000, Loss: 0.6027681231498718, Accuracy: 1.0\n",
            "  Batch 6388/8000, Loss: 0.10276500135660172, Accuracy: 1.0\n",
            "  Batch 6389/8000, Loss: 0.421171635389328, Accuracy: 1.0\n",
            "  Batch 6390/8000, Loss: 0.43405628204345703, Accuracy: 1.0\n",
            "  Batch 6391/8000, Loss: 0.7167308926582336, Accuracy: 1.0\n",
            "  Batch 6392/8000, Loss: 0.37378618121147156, Accuracy: 1.0\n",
            "  Batch 6393/8000, Loss: 0.10489779710769653, Accuracy: 1.0\n",
            "  Batch 6394/8000, Loss: 0.7163659334182739, Accuracy: 1.0\n",
            "  Batch 6395/8000, Loss: 0.7440744042396545, Accuracy: 1.0\n",
            "  Batch 6396/8000, Loss: 0.8560785055160522, Accuracy: 0.0\n",
            "  Batch 6397/8000, Loss: 0.4151679277420044, Accuracy: 1.0\n",
            "  Batch 6398/8000, Loss: 0.5360423922538757, Accuracy: 1.0\n",
            "  Batch 6399/8000, Loss: 0.10284208506345749, Accuracy: 1.0\n",
            "  Batch 6400/8000, Loss: 0.4960702359676361, Accuracy: 1.0\n",
            "  Batch 6401/8000, Loss: 0.4838513135910034, Accuracy: 1.0\n",
            "  Batch 6402/8000, Loss: 0.10355326533317566, Accuracy: 1.0\n",
            "  Batch 6403/8000, Loss: 0.33565473556518555, Accuracy: 1.0\n",
            "  Batch 6404/8000, Loss: 0.3786194622516632, Accuracy: 1.0\n",
            "  Batch 6405/8000, Loss: 0.9272950887680054, Accuracy: 0.0\n",
            "  Batch 6406/8000, Loss: 0.37968599796295166, Accuracy: 1.0\n",
            "  Batch 6407/8000, Loss: 0.10348659753799438, Accuracy: 1.0\n",
            "  Batch 6408/8000, Loss: 0.3761095106601715, Accuracy: 1.0\n",
            "  Batch 6409/8000, Loss: 0.38495928049087524, Accuracy: 1.0\n",
            "  Batch 6410/8000, Loss: 0.36671239137649536, Accuracy: 1.0\n",
            "  Batch 6411/8000, Loss: 0.37075185775756836, Accuracy: 1.0\n",
            "  Batch 6412/8000, Loss: 0.12694379687309265, Accuracy: 1.0\n",
            "  Batch 6413/8000, Loss: 0.36911436915397644, Accuracy: 1.0\n",
            "  Batch 6414/8000, Loss: 0.3847652077674866, Accuracy: 1.0\n",
            "  Batch 6415/8000, Loss: 0.6635546684265137, Accuracy: 1.0\n",
            "  Batch 6416/8000, Loss: 0.10269269347190857, Accuracy: 1.0\n",
            "  Batch 6417/8000, Loss: 1.058825969696045, Accuracy: 0.0\n",
            "  Batch 6418/8000, Loss: 0.44232848286628723, Accuracy: 1.0\n",
            "  Batch 6419/8000, Loss: 0.10429659485816956, Accuracy: 1.0\n",
            "  Batch 6420/8000, Loss: 0.7374323010444641, Accuracy: 1.0\n",
            "  Batch 6421/8000, Loss: 0.9312795996665955, Accuracy: 0.0\n",
            "  Batch 6422/8000, Loss: 0.39020654559135437, Accuracy: 1.0\n",
            "  Batch 6423/8000, Loss: 0.5297516584396362, Accuracy: 1.0\n",
            "  Batch 6424/8000, Loss: 0.5862923264503479, Accuracy: 1.0\n",
            "  Batch 6425/8000, Loss: 0.1056508719921112, Accuracy: 1.0\n",
            "  Batch 6426/8000, Loss: 0.3955160975456238, Accuracy: 1.0\n",
            "  Batch 6427/8000, Loss: 0.8369554281234741, Accuracy: 0.0\n",
            "  Batch 6428/8000, Loss: 0.3882003128528595, Accuracy: 1.0\n",
            "  Batch 6429/8000, Loss: 0.302029013633728, Accuracy: 1.0\n",
            "  Batch 6430/8000, Loss: 0.30768775939941406, Accuracy: 1.0\n",
            "  Batch 6431/8000, Loss: 0.36175161600112915, Accuracy: 1.0\n",
            "  Batch 6432/8000, Loss: 0.2874477505683899, Accuracy: 1.0\n",
            "  Batch 6433/8000, Loss: 0.10264725983142853, Accuracy: 1.0\n",
            "  Batch 6434/8000, Loss: 0.1053132563829422, Accuracy: 1.0\n",
            "  Batch 6435/8000, Loss: 0.41705501079559326, Accuracy: 1.0\n",
            "  Batch 6436/8000, Loss: 0.10421478748321533, Accuracy: 1.0\n",
            "  Batch 6437/8000, Loss: 0.3046397566795349, Accuracy: 1.0\n",
            "  Batch 6438/8000, Loss: 1.2204158306121826, Accuracy: 0.0\n",
            "  Batch 6439/8000, Loss: 0.26618874073028564, Accuracy: 1.0\n",
            "  Batch 6440/8000, Loss: 1.6854232549667358, Accuracy: 0.0\n",
            "  Batch 6441/8000, Loss: 0.468613862991333, Accuracy: 1.0\n",
            "  Batch 6442/8000, Loss: 0.10263599455356598, Accuracy: 1.0\n",
            "  Batch 6443/8000, Loss: 2.0670595169067383, Accuracy: 0.0\n",
            "  Batch 6444/8000, Loss: 0.10307502746582031, Accuracy: 1.0\n",
            "  Batch 6445/8000, Loss: 1.8666174411773682, Accuracy: 0.0\n",
            "  Batch 6446/8000, Loss: 0.10261021554470062, Accuracy: 1.0\n",
            "  Batch 6447/8000, Loss: 0.9048129320144653, Accuracy: 0.0\n",
            "  Batch 6448/8000, Loss: 0.10268433392047882, Accuracy: 1.0\n",
            "  Batch 6449/8000, Loss: 0.1025535985827446, Accuracy: 1.0\n",
            "  Batch 6450/8000, Loss: 0.33204054832458496, Accuracy: 1.0\n",
            "  Batch 6451/8000, Loss: 0.7964715957641602, Accuracy: 0.0\n",
            "  Batch 6452/8000, Loss: 0.11140075325965881, Accuracy: 1.0\n",
            "  Batch 6453/8000, Loss: 1.7285109758377075, Accuracy: 0.0\n",
            "  Batch 6454/8000, Loss: 0.10252156853675842, Accuracy: 1.0\n",
            "  Batch 6455/8000, Loss: 0.3923102021217346, Accuracy: 1.0\n",
            "  Batch 6456/8000, Loss: 0.9380850791931152, Accuracy: 0.0\n",
            "  Batch 6457/8000, Loss: 0.1044708639383316, Accuracy: 1.0\n",
            "  Batch 6458/8000, Loss: 0.10569398105144501, Accuracy: 1.0\n",
            "  Batch 6459/8000, Loss: 1.351531744003296, Accuracy: 0.0\n",
            "  Batch 6460/8000, Loss: 0.39485982060432434, Accuracy: 1.0\n",
            "  Batch 6461/8000, Loss: 0.10297943651676178, Accuracy: 1.0\n",
            "  Batch 6526/8000, Loss: 0.7747476696968079, Accuracy: 1.0\n",
            "  Batch 6527/8000, Loss: 0.26836636662483215, Accuracy: 1.0\n",
            "  Batch 6528/8000, Loss: 0.34277215600013733, Accuracy: 1.0\n",
            "  Batch 6529/8000, Loss: 0.5226056575775146, Accuracy: 1.0\n",
            "  Batch 6530/8000, Loss: 0.10215681791305542, Accuracy: 1.0\n",
            "  Batch 6531/8000, Loss: 0.3090425133705139, Accuracy: 1.0\n",
            "  Batch 6532/8000, Loss: 0.10217411816120148, Accuracy: 1.0\n",
            "  Batch 6533/8000, Loss: 0.10220314562320709, Accuracy: 1.0\n",
            "  Batch 6534/8000, Loss: 1.349367380142212, Accuracy: 0.0\n",
            "  Batch 6535/8000, Loss: 0.26404130458831787, Accuracy: 1.0\n",
            "  Batch 6536/8000, Loss: 0.2589992880821228, Accuracy: 1.0\n",
            "  Batch 6537/8000, Loss: 0.10217335820198059, Accuracy: 1.0\n",
            "  Batch 6538/8000, Loss: 1.5598764419555664, Accuracy: 0.0\n",
            "  Batch 6539/8000, Loss: 0.3144668936729431, Accuracy: 1.0\n",
            "  Batch 6540/8000, Loss: 0.27576056122779846, Accuracy: 1.0\n",
            "  Batch 6541/8000, Loss: 0.984449565410614, Accuracy: 0.0\n",
            "  Batch 6542/8000, Loss: 0.2616077661514282, Accuracy: 1.0\n",
            "  Batch 6543/8000, Loss: 0.7597626447677612, Accuracy: 1.0\n",
            "  Batch 6544/8000, Loss: 0.11912322044372559, Accuracy: 1.0\n",
            "  Batch 6545/8000, Loss: 0.7189373970031738, Accuracy: 1.0\n",
            "  Batch 6546/8000, Loss: 0.3230077028274536, Accuracy: 1.0\n",
            "  Batch 6547/8000, Loss: 0.45417532324790955, Accuracy: 1.0\n",
            "  Batch 6548/8000, Loss: 0.9033176302909851, Accuracy: 0.0\n",
            "  Batch 6549/8000, Loss: 0.10734105110168457, Accuracy: 1.0\n",
            "  Batch 6550/8000, Loss: 0.25594475865364075, Accuracy: 1.0\n",
            "  Batch 6551/8000, Loss: 0.667056679725647, Accuracy: 1.0\n",
            "  Batch 6552/8000, Loss: 0.2679206132888794, Accuracy: 1.0\n",
            "  Batch 6553/8000, Loss: 0.2932700216770172, Accuracy: 1.0\n",
            "  Batch 6554/8000, Loss: 1.4146461486816406, Accuracy: 0.0\n",
            "  Batch 6555/8000, Loss: 0.30910882353782654, Accuracy: 1.0\n",
            "  Batch 6556/8000, Loss: 0.3632620573043823, Accuracy: 1.0\n",
            "  Batch 6557/8000, Loss: 0.2837178409099579, Accuracy: 1.0\n",
            "  Batch 6558/8000, Loss: 0.8988495469093323, Accuracy: 0.0\n",
            "  Batch 6559/8000, Loss: 0.3271470367908478, Accuracy: 1.0\n",
            "  Batch 6560/8000, Loss: 0.24826882779598236, Accuracy: 1.0\n",
            "  Batch 6561/8000, Loss: 0.6106134653091431, Accuracy: 1.0\n",
            "  Batch 6562/8000, Loss: 0.10325893759727478, Accuracy: 1.0\n",
            "  Batch 6563/8000, Loss: 0.10200968384742737, Accuracy: 1.0\n",
            "  Batch 6564/8000, Loss: 1.9177732467651367, Accuracy: 0.0\n",
            "  Batch 6565/8000, Loss: 0.3438807725906372, Accuracy: 1.0\n",
            "  Batch 6566/8000, Loss: 0.6379473209381104, Accuracy: 1.0\n",
            "  Batch 6567/8000, Loss: 0.8680503964424133, Accuracy: 0.0\n",
            "  Batch 6568/8000, Loss: 1.3436981439590454, Accuracy: 0.0\n",
            "  Batch 6569/8000, Loss: 0.3252672851085663, Accuracy: 1.0\n",
            "  Batch 6570/8000, Loss: 0.10453154146671295, Accuracy: 1.0\n",
            "  Batch 6571/8000, Loss: 0.2710287868976593, Accuracy: 1.0\n",
            "  Batch 6572/8000, Loss: 0.8166351318359375, Accuracy: 0.0\n",
            "  Batch 6573/8000, Loss: 0.8744193911552429, Accuracy: 0.0\n",
            "  Batch 6574/8000, Loss: 0.30385521054267883, Accuracy: 1.0\n",
            "  Batch 6575/8000, Loss: 0.27097153663635254, Accuracy: 1.0\n",
            "  Batch 6576/8000, Loss: 0.6304806470870972, Accuracy: 1.0\n",
            "  Batch 6577/8000, Loss: 0.3011608123779297, Accuracy: 1.0\n",
            "  Batch 6578/8000, Loss: 0.6282104849815369, Accuracy: 1.0\n",
            "  Batch 6579/8000, Loss: 1.8691338300704956, Accuracy: 0.0\n",
            "  Batch 6580/8000, Loss: 0.3279447555541992, Accuracy: 1.0\n",
            "  Batch 6581/8000, Loss: 0.380472868680954, Accuracy: 1.0\n",
            "  Batch 6582/8000, Loss: 1.5451452732086182, Accuracy: 0.0\n",
            "  Batch 6583/8000, Loss: 0.10901056975126266, Accuracy: 1.0\n",
            "  Batch 6584/8000, Loss: 0.29097506403923035, Accuracy: 1.0\n",
            "  Batch 6585/8000, Loss: 0.6648797392845154, Accuracy: 1.0\n",
            "  Batch 6586/8000, Loss: 0.6784390807151794, Accuracy: 1.0\n",
            "  Batch 6587/8000, Loss: 0.4102226793766022, Accuracy: 1.0\n",
            "  Batch 6588/8000, Loss: 1.4830889701843262, Accuracy: 0.0\n",
            "  Batch 6589/8000, Loss: 1.6875911951065063, Accuracy: 0.0\n",
            "  Batch 6590/8000, Loss: 0.13337109982967377, Accuracy: 1.0\n",
            "  Batch 6591/8000, Loss: 0.3759011924266815, Accuracy: 1.0\n",
            "  Batch 6592/8000, Loss: 0.7421160340309143, Accuracy: 1.0\n",
            "  Batch 6593/8000, Loss: 0.10205379128456116, Accuracy: 1.0\n",
            "  Batch 6594/8000, Loss: 0.6687682271003723, Accuracy: 1.0\n",
            "  Batch 6595/8000, Loss: 0.9563610553741455, Accuracy: 0.0\n",
            "  Batch 6596/8000, Loss: 0.5009294748306274, Accuracy: 1.0\n",
            "  Batch 6597/8000, Loss: 0.10270124673843384, Accuracy: 1.0\n",
            "  Batch 6598/8000, Loss: 0.1860072761774063, Accuracy: 1.0\n",
            "  Batch 6599/8000, Loss: 0.3413946330547333, Accuracy: 1.0\n",
            "  Batch 6600/8000, Loss: 0.677352786064148, Accuracy: 1.0\n",
            "  Batch 6601/8000, Loss: 0.10257390141487122, Accuracy: 1.0\n",
            "  Batch 6602/8000, Loss: 0.42635372281074524, Accuracy: 1.0\n",
            "  Batch 6603/8000, Loss: 0.37059664726257324, Accuracy: 1.0\n",
            "  Batch 6604/8000, Loss: 0.6887176036834717, Accuracy: 1.0\n",
            "  Batch 6605/8000, Loss: 1.2252209186553955, Accuracy: 0.0\n",
            "  Batch 6606/8000, Loss: 0.10196774452924728, Accuracy: 1.0\n",
            "  Batch 6607/8000, Loss: 0.10185979306697845, Accuracy: 1.0\n",
            "  Batch 6608/8000, Loss: 0.8389056921005249, Accuracy: 0.0\n",
            "  Batch 6609/8000, Loss: 0.8609817624092102, Accuracy: 0.0\n",
            "  Batch 6610/8000, Loss: 1.0583558082580566, Accuracy: 0.0\n",
            "  Batch 6611/8000, Loss: 1.5929588079452515, Accuracy: 0.0\n",
            "  Batch 6612/8000, Loss: 0.10184837877750397, Accuracy: 1.0\n",
            "  Batch 6613/8000, Loss: 0.10196742415428162, Accuracy: 1.0\n",
            "  Batch 6614/8000, Loss: 1.0150883197784424, Accuracy: 0.0\n",
            "  Batch 6615/8000, Loss: 1.423417091369629, Accuracy: 0.0\n",
            "  Batch 6616/8000, Loss: 0.10180763155221939, Accuracy: 1.0\n",
            "  Batch 6617/8000, Loss: 0.40616026520729065, Accuracy: 1.0\n",
            "  Batch 6618/8000, Loss: 1.289634108543396, Accuracy: 0.0\n",
            "  Batch 6619/8000, Loss: 0.4933578372001648, Accuracy: 1.0\n",
            "  Batch 6620/8000, Loss: 0.7188999056816101, Accuracy: 1.0\n",
            "  Batch 6621/8000, Loss: 0.7423583269119263, Accuracy: 1.0\n",
            "  Batch 6622/8000, Loss: 0.7559190392494202, Accuracy: 1.0\n",
            "  Batch 6623/8000, Loss: 0.6031026840209961, Accuracy: 1.0\n",
            "  Batch 6624/8000, Loss: 0.19100044667720795, Accuracy: 1.0\n",
            "  Batch 6625/8000, Loss: 0.7718084454536438, Accuracy: 1.0\n",
            "  Batch 6626/8000, Loss: 0.4229097068309784, Accuracy: 1.0\n",
            "  Batch 6627/8000, Loss: 0.4850446283817291, Accuracy: 1.0\n",
            "  Batch 6628/8000, Loss: 0.10179848223924637, Accuracy: 1.0\n",
            "  Batch 6629/8000, Loss: 0.10173308849334717, Accuracy: 1.0\n",
            "  Batch 6630/8000, Loss: 0.5582433342933655, Accuracy: 1.0\n",
            "  Batch 6631/8000, Loss: 0.4642777740955353, Accuracy: 1.0\n",
            "  Batch 6632/8000, Loss: 0.10194940865039825, Accuracy: 1.0\n",
            "  Batch 6633/8000, Loss: 1.2118027210235596, Accuracy: 0.0\n",
            "  Batch 6634/8000, Loss: 0.41200128197669983, Accuracy: 1.0\n",
            "  Batch 6635/8000, Loss: 0.6256009340286255, Accuracy: 1.0\n",
            "  Batch 6636/8000, Loss: 0.10180637240409851, Accuracy: 1.0\n",
            "  Batch 6637/8000, Loss: 0.41972267627716064, Accuracy: 1.0\n",
            "  Batch 6638/8000, Loss: 0.10173910111188889, Accuracy: 1.0\n",
            "  Batch 6639/8000, Loss: 0.10568317025899887, Accuracy: 1.0\n",
            "  Batch 6640/8000, Loss: 0.8272813558578491, Accuracy: 0.0\n",
            "  Batch 6641/8000, Loss: 1.0761950016021729, Accuracy: 0.0\n",
            "  Batch 6642/8000, Loss: 1.456502079963684, Accuracy: 0.0\n",
            "  Batch 6643/8000, Loss: 0.6031184196472168, Accuracy: 1.0\n",
            "  Batch 6644/8000, Loss: 0.7684926390647888, Accuracy: 1.0\n",
            "  Batch 6645/8000, Loss: 0.10166873782873154, Accuracy: 1.0\n",
            "  Batch 6646/8000, Loss: 1.3315516710281372, Accuracy: 0.0\n",
            "  Batch 6647/8000, Loss: 1.4046335220336914, Accuracy: 0.0\n",
            "  Batch 6648/8000, Loss: 0.48175737261772156, Accuracy: 1.0\n",
            "  Batch 6649/8000, Loss: 0.5145590901374817, Accuracy: 1.0\n",
            "  Batch 6650/8000, Loss: 0.5028706192970276, Accuracy: 1.0\n",
            "  Batch 6651/8000, Loss: 0.6087555885314941, Accuracy: 1.0\n",
            "  Batch 6652/8000, Loss: 0.5170661807060242, Accuracy: 1.0\n",
            "  Batch 6653/8000, Loss: 0.10171949863433838, Accuracy: 1.0\n",
            "  Batch 6654/8000, Loss: 1.3183879852294922, Accuracy: 0.0\n",
            "  Batch 6655/8000, Loss: 0.46839115023612976, Accuracy: 1.0\n",
            "  Batch 6656/8000, Loss: 0.7140055894851685, Accuracy: 1.0\n",
            "  Batch 6657/8000, Loss: 1.2077049016952515, Accuracy: 0.0\n",
            "  Batch 6658/8000, Loss: 0.10159191489219666, Accuracy: 1.0\n",
            "  Batch 6659/8000, Loss: 1.4081865549087524, Accuracy: 0.0\n",
            "  Batch 6660/8000, Loss: 0.504999577999115, Accuracy: 1.0\n",
            "  Batch 6661/8000, Loss: 1.138318657875061, Accuracy: 0.0\n",
            "  Batch 6662/8000, Loss: 1.062319040298462, Accuracy: 0.0\n",
            "  Batch 6663/8000, Loss: 0.49510902166366577, Accuracy: 1.0\n",
            "  Batch 6664/8000, Loss: 0.4959048628807068, Accuracy: 1.0\n",
            "  Batch 6665/8000, Loss: 0.1016387790441513, Accuracy: 1.0\n",
            "  Batch 6666/8000, Loss: 0.10804017633199692, Accuracy: 1.0\n",
            "  Batch 6667/8000, Loss: 0.10177567601203918, Accuracy: 1.0\n",
            "  Batch 6668/8000, Loss: 0.7361308336257935, Accuracy: 1.0\n",
            "  Batch 6669/8000, Loss: 0.4827920198440552, Accuracy: 1.0\n",
            "  Batch 6670/8000, Loss: 0.7127531170845032, Accuracy: 1.0\n",
            "  Batch 6671/8000, Loss: 0.4709926247596741, Accuracy: 1.0\n",
            "  Batch 6672/8000, Loss: 0.44392895698547363, Accuracy: 1.0\n",
            "  Batch 6673/8000, Loss: 1.342044472694397, Accuracy: 0.0\n",
            "  Batch 6674/8000, Loss: 1.0670852661132812, Accuracy: 0.0\n",
            "  Batch 6675/8000, Loss: 1.1575385332107544, Accuracy: 0.0\n",
            "  Batch 6676/8000, Loss: 0.11276064068078995, Accuracy: 1.0\n",
            "  Batch 6677/8000, Loss: 0.4675755500793457, Accuracy: 1.0\n",
            "  Batch 6678/8000, Loss: 0.6833170652389526, Accuracy: 1.0\n",
            "  Batch 6679/8000, Loss: 0.1014459878206253, Accuracy: 1.0\n",
            "  Batch 6680/8000, Loss: 0.10609342157840729, Accuracy: 1.0\n",
            "  Batch 6681/8000, Loss: 0.9415934681892395, Accuracy: 0.0\n",
            "  Batch 6682/8000, Loss: 0.9685670137405396, Accuracy: 0.0\n",
            "  Batch 6683/8000, Loss: 0.48650822043418884, Accuracy: 1.0\n",
            "  Batch 6684/8000, Loss: 0.8126530051231384, Accuracy: 0.0\n",
            "  Batch 6685/8000, Loss: 0.3931889235973358, Accuracy: 1.0\n",
            "  Batch 6686/8000, Loss: 1.5585730075836182, Accuracy: 0.0\n",
            "  Batch 6687/8000, Loss: 7.411611080169678, Accuracy: 0.0\n",
            "  Batch 6688/8000, Loss: 0.10140438377857208, Accuracy: 1.0\n",
            "  Batch 6689/8000, Loss: 0.45053598284721375, Accuracy: 1.0\n",
            "  Batch 6690/8000, Loss: 1.4822214841842651, Accuracy: 0.0\n",
            "  Batch 6691/8000, Loss: 0.8165555596351624, Accuracy: 0.0\n",
            "  Batch 6692/8000, Loss: 0.783988893032074, Accuracy: 1.0\n",
            "  Batch 6693/8000, Loss: 0.4048129618167877, Accuracy: 1.0\n",
            "  Batch 6694/8000, Loss: 0.37568849325180054, Accuracy: 1.0\n",
            "  Batch 6695/8000, Loss: 0.10154225677251816, Accuracy: 1.0\n",
            "  Batch 6696/8000, Loss: 0.10975991189479828, Accuracy: 1.0\n",
            "  Batch 6697/8000, Loss: 0.1323174685239792, Accuracy: 1.0\n",
            "  Batch 6698/8000, Loss: 0.5945392847061157, Accuracy: 1.0\n",
            "  Batch 6699/8000, Loss: 0.6080626249313354, Accuracy: 1.0\n",
            "  Batch 6700/8000, Loss: 0.5564767718315125, Accuracy: 1.0\n",
            "  Batch 6701/8000, Loss: 0.42037034034729004, Accuracy: 1.0\n",
            "  Batch 6702/8000, Loss: 0.4056391716003418, Accuracy: 1.0\n",
            "  Batch 6703/8000, Loss: 0.38165193796157837, Accuracy: 1.0\n",
            "  Batch 6704/8000, Loss: 0.7889165282249451, Accuracy: 1.0\n",
            "  Batch 6705/8000, Loss: 0.10152100026607513, Accuracy: 1.0\n",
            "  Batch 6706/8000, Loss: 0.7880620360374451, Accuracy: 1.0\n",
            "  Batch 6707/8000, Loss: 0.9766973853111267, Accuracy: 0.0\n",
            "  Batch 6708/8000, Loss: 0.7739726901054382, Accuracy: 1.0\n",
            "  Batch 6709/8000, Loss: 0.36948972940444946, Accuracy: 1.0\n",
            "  Batch 6710/8000, Loss: 0.4477962255477905, Accuracy: 1.0\n",
            "  Batch 6711/8000, Loss: 1.169710636138916, Accuracy: 0.0\n",
            "  Batch 6712/8000, Loss: 0.6830536723136902, Accuracy: 1.0\n",
            "  Batch 6713/8000, Loss: 0.8085113763809204, Accuracy: 0.0\n",
            "  Batch 6714/8000, Loss: 0.7109454870223999, Accuracy: 1.0\n",
            "  Batch 6715/8000, Loss: 0.3793887197971344, Accuracy: 1.0\n",
            "  Batch 6716/8000, Loss: 1.4593440294265747, Accuracy: 0.0\n",
            "  Batch 6717/8000, Loss: 0.35795918107032776, Accuracy: 1.0\n",
            "  Batch 6718/8000, Loss: 0.3617207407951355, Accuracy: 1.0\n",
            "  Batch 6719/8000, Loss: 0.1088983565568924, Accuracy: 1.0\n",
            "  Batch 6720/8000, Loss: 0.7524308562278748, Accuracy: 1.0\n",
            "  Batch 6721/8000, Loss: 0.5275412797927856, Accuracy: 1.0\n",
            "  Batch 6722/8000, Loss: 1.5486863851547241, Accuracy: 0.0\n",
            "  Batch 6723/8000, Loss: 0.3698004484176636, Accuracy: 1.0\n",
            "  Batch 6724/8000, Loss: 0.5164215564727783, Accuracy: 1.0\n",
            "  Batch 6725/8000, Loss: 0.3515976071357727, Accuracy: 1.0\n",
            "  Batch 6726/8000, Loss: 0.5314617156982422, Accuracy: 1.0\n",
            "  Batch 6727/8000, Loss: 1.4485464096069336, Accuracy: 0.0\n",
            "  Batch 6728/8000, Loss: 0.1364443153142929, Accuracy: 1.0\n",
            "  Batch 6729/8000, Loss: 0.11539936065673828, Accuracy: 1.0\n",
            "  Batch 6730/8000, Loss: 1.3700788021087646, Accuracy: 0.0\n",
            "  Batch 6731/8000, Loss: 0.1344538927078247, Accuracy: 1.0\n",
            "  Batch 6732/8000, Loss: 0.3660028278827667, Accuracy: 1.0\n",
            "  Batch 6733/8000, Loss: 0.5114692449569702, Accuracy: 1.0\n",
            "  Batch 6734/8000, Loss: 0.5025416016578674, Accuracy: 1.0\n",
            "  Batch 6735/8000, Loss: 0.4755721986293793, Accuracy: 1.0\n",
            "  Batch 6736/8000, Loss: 0.7087612748146057, Accuracy: 1.0\n",
            "  Batch 6737/8000, Loss: 0.8235012888908386, Accuracy: 0.0\n",
            "  Batch 6738/8000, Loss: 0.3584471046924591, Accuracy: 1.0\n",
            "  Batch 6739/8000, Loss: 0.47099214792251587, Accuracy: 1.0\n",
            "  Batch 6740/8000, Loss: 1.3319945335388184, Accuracy: 0.0\n",
            "  Batch 6741/8000, Loss: 1.3770976066589355, Accuracy: 0.0\n",
            "  Batch 6742/8000, Loss: 0.7936491966247559, Accuracy: 1.0\n",
            "  Batch 6743/8000, Loss: 0.7674294114112854, Accuracy: 1.0\n",
            "  Batch 6744/8000, Loss: 0.11345186829566956, Accuracy: 1.0\n",
            "  Batch 6745/8000, Loss: 0.3565211594104767, Accuracy: 1.0\n",
            "  Batch 6746/8000, Loss: 1.6211224794387817, Accuracy: 0.0\n",
            "  Batch 6747/8000, Loss: 0.38848790526390076, Accuracy: 1.0\n",
            "  Batch 6748/8000, Loss: 0.11228524148464203, Accuracy: 1.0\n",
            "  Batch 6749/8000, Loss: 0.39124375581741333, Accuracy: 1.0\n",
            "  Batch 6750/8000, Loss: 0.10404137521982193, Accuracy: 1.0\n",
            "  Batch 6751/8000, Loss: 1.6142081022262573, Accuracy: 0.0\n",
            "  Batch 6752/8000, Loss: 1.6124364137649536, Accuracy: 0.0\n",
            "  Batch 6753/8000, Loss: 0.11922681331634521, Accuracy: 1.0\n",
            "  Batch 6754/8000, Loss: 1.2974001169204712, Accuracy: 0.0\n",
            "  Batch 6755/8000, Loss: 1.2123104333877563, Accuracy: 0.0\n",
            "  Batch 6756/8000, Loss: 0.9400802850723267, Accuracy: 0.0\n",
            "  Batch 6757/8000, Loss: 0.13322269916534424, Accuracy: 1.0\n",
            "  Batch 6758/8000, Loss: 0.1139451190829277, Accuracy: 1.0\n",
            "  Batch 6759/8000, Loss: 1.2606450319290161, Accuracy: 0.0\n",
            "  Batch 6760/8000, Loss: 0.6692451238632202, Accuracy: 1.0\n",
            "  Batch 6761/8000, Loss: 1.3061344623565674, Accuracy: 0.0\n",
            "  Batch 6762/8000, Loss: 0.5504140853881836, Accuracy: 1.0\n",
            "  Batch 6763/8000, Loss: 0.4458969831466675, Accuracy: 1.0\n",
            "  Batch 6764/8000, Loss: 1.026774287223816, Accuracy: 0.0\n",
            "  Batch 6765/8000, Loss: 0.10126261413097382, Accuracy: 1.0\n",
            "  Batch 6766/8000, Loss: 0.4225752353668213, Accuracy: 1.0\n",
            "  Batch 6767/8000, Loss: 0.4260065257549286, Accuracy: 1.0\n",
            "  Batch 6768/8000, Loss: 0.11335868388414383, Accuracy: 1.0\n",
            "  Batch 6769/8000, Loss: 0.4119570851325989, Accuracy: 1.0\n",
            "  Batch 6770/8000, Loss: 0.7282825708389282, Accuracy: 1.0\n",
            "  Batch 6771/8000, Loss: 1.2900816202163696, Accuracy: 0.0\n",
            "  Batch 6772/8000, Loss: 0.12328095734119415, Accuracy: 1.0\n",
            "  Batch 6773/8000, Loss: 0.4576762020587921, Accuracy: 1.0\n",
            "  Batch 6774/8000, Loss: 0.6075833439826965, Accuracy: 1.0\n",
            "  Batch 6775/8000, Loss: 0.40639007091522217, Accuracy: 1.0\n",
            "  Batch 6776/8000, Loss: 0.1288634091615677, Accuracy: 1.0\n",
            "  Batch 6777/8000, Loss: 0.6998887062072754, Accuracy: 1.0\n",
            "  Batch 6778/8000, Loss: 0.4566322863101959, Accuracy: 1.0\n",
            "  Batch 6779/8000, Loss: 0.6422756314277649, Accuracy: 1.0\n",
            "  Batch 6780/8000, Loss: 0.8137844800949097, Accuracy: 0.0\n",
            "  Batch 6781/8000, Loss: 0.8275325298309326, Accuracy: 0.0\n",
            "  Batch 6782/8000, Loss: 0.7685682773590088, Accuracy: 1.0\n",
            "  Batch 6783/8000, Loss: 0.9843771457672119, Accuracy: 0.0\n",
            "  Batch 6784/8000, Loss: 0.5309805274009705, Accuracy: 1.0\n",
            "  Batch 6785/8000, Loss: 1.1637182235717773, Accuracy: 0.0\n",
            "  Batch 6786/8000, Loss: 0.44495952129364014, Accuracy: 1.0\n",
            "  Batch 6787/8000, Loss: 0.6125498414039612, Accuracy: 1.0\n",
            "  Batch 6788/8000, Loss: 0.41074514389038086, Accuracy: 1.0\n",
            "  Batch 6789/8000, Loss: 0.4395361542701721, Accuracy: 1.0\n",
            "  Batch 6790/8000, Loss: 1.1589152812957764, Accuracy: 0.0\n",
            "  Batch 6791/8000, Loss: 1.0179955959320068, Accuracy: 0.0\n",
            "  Batch 6792/8000, Loss: 0.4866797924041748, Accuracy: 1.0\n",
            "  Batch 6793/8000, Loss: 0.5810661315917969, Accuracy: 1.0\n",
            "  Batch 6794/8000, Loss: 0.5875771045684814, Accuracy: 1.0\n",
            "  Batch 6795/8000, Loss: 0.49146032333374023, Accuracy: 1.0\n",
            "  Batch 6796/8000, Loss: 0.41497236490249634, Accuracy: 1.0\n",
            "  Batch 6797/8000, Loss: 0.9932011961936951, Accuracy: 0.0\n",
            "  Batch 6798/8000, Loss: 0.4341006577014923, Accuracy: 1.0\n",
            "  Batch 6799/8000, Loss: 1.3634401559829712, Accuracy: 0.0\n",
            "  Batch 6800/8000, Loss: 1.376476764678955, Accuracy: 0.0\n",
            "  Batch 6801/8000, Loss: 0.10078589618206024, Accuracy: 1.0\n",
            "  Batch 6802/8000, Loss: 0.7253470420837402, Accuracy: 1.0\n",
            "  Batch 6803/8000, Loss: 0.10249896347522736, Accuracy: 1.0\n",
            "  Batch 6804/8000, Loss: 0.628940999507904, Accuracy: 1.0\n",
            "  Batch 6805/8000, Loss: 0.5830708742141724, Accuracy: 1.0\n",
            "  Batch 6806/8000, Loss: 0.10087994486093521, Accuracy: 1.0\n",
            "  Batch 6807/8000, Loss: 0.4046628773212433, Accuracy: 1.0\n",
            "  Batch 6808/8000, Loss: 0.46052053570747375, Accuracy: 1.0\n",
            "  Batch 6809/8000, Loss: 0.43047672510147095, Accuracy: 1.0\n",
            "  Batch 6810/8000, Loss: 0.87556391954422, Accuracy: 0.0\n",
            "  Batch 6811/8000, Loss: 0.8233988881111145, Accuracy: 0.0\n",
            "  Batch 6812/8000, Loss: 0.10081096738576889, Accuracy: 1.0\n",
            "  Batch 6813/8000, Loss: 0.5973463654518127, Accuracy: 1.0\n",
            "  Batch 6814/8000, Loss: 1.2240009307861328, Accuracy: 0.0\n",
            "  Batch 6815/8000, Loss: 0.10076824575662613, Accuracy: 1.0\n",
            "  Batch 6816/8000, Loss: 0.2530626654624939, Accuracy: 1.0\n",
            "  Batch 6817/8000, Loss: 0.10801069438457489, Accuracy: 1.0\n",
            "  Batch 6818/8000, Loss: 0.8306820392608643, Accuracy: 0.0\n",
            "  Batch 6819/8000, Loss: 0.4448032081127167, Accuracy: 1.0\n",
            "  Batch 6820/8000, Loss: 0.10907474160194397, Accuracy: 1.0\n",
            "  Batch 6821/8000, Loss: 0.1025649756193161, Accuracy: 1.0\n",
            "  Batch 6822/8000, Loss: 0.15524104237556458, Accuracy: 1.0\n",
            "  Batch 6823/8000, Loss: 0.11468259990215302, Accuracy: 1.0\n",
            "  Batch 6824/8000, Loss: 0.10676724463701248, Accuracy: 1.0\n",
            "  Batch 6825/8000, Loss: 0.11102691292762756, Accuracy: 1.0\n",
            "  Batch 6826/8000, Loss: 0.729936420917511, Accuracy: 1.0\n",
            "  Batch 6827/8000, Loss: 0.11226033419370651, Accuracy: 1.0\n",
            "  Batch 6828/8000, Loss: 0.6071594953536987, Accuracy: 1.0\n",
            "  Batch 6829/8000, Loss: 0.6014930009841919, Accuracy: 1.0\n",
            "  Batch 6830/8000, Loss: 1.388978362083435, Accuracy: 0.0\n",
            "  Batch 6831/8000, Loss: 0.7167837023735046, Accuracy: 1.0\n",
            "  Batch 6832/8000, Loss: 0.4252587556838989, Accuracy: 1.0\n",
            "  Batch 6833/8000, Loss: 1.7001794576644897, Accuracy: 0.0\n",
            "  Batch 6834/8000, Loss: 0.45518767833709717, Accuracy: 1.0\n",
            "  Batch 6835/8000, Loss: 0.2289067953824997, Accuracy: 1.0\n",
            "  Batch 6836/8000, Loss: 0.7409108281135559, Accuracy: 1.0\n",
            "  Batch 6837/8000, Loss: 0.4123286306858063, Accuracy: 1.0\n",
            "  Batch 6838/8000, Loss: 1.3107919692993164, Accuracy: 0.0\n",
            "  Batch 6839/8000, Loss: 0.12052866816520691, Accuracy: 1.0\n",
            "  Batch 6840/8000, Loss: 0.6354053020477295, Accuracy: 1.0\n",
            "  Batch 6841/8000, Loss: 0.4707931876182556, Accuracy: 1.0\n",
            "  Batch 6842/8000, Loss: 0.3824405372142792, Accuracy: 1.0\n",
            "  Batch 6843/8000, Loss: 0.4985560178756714, Accuracy: 1.0\n",
            "  Batch 6844/8000, Loss: 0.7221152186393738, Accuracy: 1.0\n",
            "  Batch 6845/8000, Loss: 1.0106761455535889, Accuracy: 0.0\n",
            "  Batch 6846/8000, Loss: 0.2178686261177063, Accuracy: 1.0\n",
            "  Batch 6847/8000, Loss: 0.5013260841369629, Accuracy: 1.0\n",
            "  Batch 6848/8000, Loss: 0.10066833347082138, Accuracy: 1.0\n",
            "  Batch 6849/8000, Loss: 0.9296920299530029, Accuracy: 0.0\n",
            "  Batch 6850/8000, Loss: 0.8648511171340942, Accuracy: 0.0\n",
            "  Batch 6851/8000, Loss: 0.3627440631389618, Accuracy: 1.0\n",
            "  Batch 6852/8000, Loss: 0.3747814893722534, Accuracy: 1.0\n",
            "  Batch 6853/8000, Loss: 0.14972785115242004, Accuracy: 1.0\n",
            "  Batch 6854/8000, Loss: 0.10054698586463928, Accuracy: 1.0\n",
            "  Batch 6855/8000, Loss: 0.8880094885826111, Accuracy: 0.0\n",
            "  Batch 6856/8000, Loss: 0.7711948752403259, Accuracy: 1.0\n",
            "  Batch 6857/8000, Loss: 0.47366631031036377, Accuracy: 1.0\n",
            "  Batch 6858/8000, Loss: 0.42138177156448364, Accuracy: 1.0\n",
            "  Batch 6859/8000, Loss: 0.10143641382455826, Accuracy: 1.0\n",
            "  Batch 6860/8000, Loss: 0.1077948659658432, Accuracy: 1.0\n",
            "  Batch 6861/8000, Loss: 0.4473545253276825, Accuracy: 1.0\n",
            "  Batch 6862/8000, Loss: 0.10843809694051743, Accuracy: 1.0\n",
            "  Batch 6863/8000, Loss: 0.48138198256492615, Accuracy: 1.0\n",
            "  Batch 6864/8000, Loss: 0.10769133269786835, Accuracy: 1.0\n",
            "  Batch 6865/8000, Loss: 0.36303961277008057, Accuracy: 1.0\n",
            "  Batch 6866/8000, Loss: 0.1222161054611206, Accuracy: 1.0\n",
            "  Batch 6867/8000, Loss: 0.7960022687911987, Accuracy: 0.0\n",
            "  Batch 6868/8000, Loss: 0.3800082504749298, Accuracy: 1.0\n",
            "  Batch 6869/8000, Loss: 0.46047091484069824, Accuracy: 1.0\n",
            "  Batch 6870/8000, Loss: 0.5358858704566956, Accuracy: 1.0\n",
            "  Batch 6871/8000, Loss: 0.39396628737449646, Accuracy: 1.0\n",
            "  Batch 6872/8000, Loss: 0.481519877910614, Accuracy: 1.0\n",
            "  Batch 6873/8000, Loss: 1.628135323524475, Accuracy: 0.0\n",
            "  Batch 6874/8000, Loss: 0.6395915746688843, Accuracy: 1.0\n",
            "  Batch 6875/8000, Loss: 0.6682126522064209, Accuracy: 1.0\n",
            "  Batch 6876/8000, Loss: 0.6049131155014038, Accuracy: 1.0\n",
            "  Batch 6877/8000, Loss: 0.3785557150840759, Accuracy: 1.0\n",
            "  Batch 6878/8000, Loss: 0.7791102528572083, Accuracy: 1.0\n",
            "  Batch 6879/8000, Loss: 0.42330077290534973, Accuracy: 1.0\n",
            "  Batch 6880/8000, Loss: 0.7306562066078186, Accuracy: 1.0\n",
            "  Batch 6881/8000, Loss: 0.10064868628978729, Accuracy: 1.0\n",
            "  Batch 6882/8000, Loss: 0.7851117253303528, Accuracy: 1.0\n",
            "  Batch 6883/8000, Loss: 0.6897991895675659, Accuracy: 1.0\n",
            "  Batch 6884/8000, Loss: 0.5087313652038574, Accuracy: 1.0\n",
            "  Batch 6885/8000, Loss: 0.3136289119720459, Accuracy: 1.0\n",
            "  Batch 6886/8000, Loss: 0.4529676139354706, Accuracy: 1.0\n",
            "  Batch 6887/8000, Loss: 1.3216488361358643, Accuracy: 0.0\n",
            "  Batch 6888/8000, Loss: 0.10670675337314606, Accuracy: 1.0\n",
            "  Batch 6889/8000, Loss: 0.2911156713962555, Accuracy: 1.0\n",
            "  Batch 6890/8000, Loss: 0.10493583977222443, Accuracy: 1.0\n",
            "  Batch 6891/8000, Loss: 0.35326308012008667, Accuracy: 1.0\n",
            "  Batch 6892/8000, Loss: 0.10071080178022385, Accuracy: 1.0\n",
            "  Batch 6893/8000, Loss: 0.10563303530216217, Accuracy: 1.0\n",
            "  Batch 6894/8000, Loss: 0.38978272676467896, Accuracy: 1.0\n",
            "  Batch 6895/8000, Loss: 0.30897057056427, Accuracy: 1.0\n",
            "  Batch 6896/8000, Loss: 0.6417783498764038, Accuracy: 1.0\n",
            "  Batch 6897/8000, Loss: 0.2984888553619385, Accuracy: 1.0\n",
            "  Batch 6898/8000, Loss: 0.10932433605194092, Accuracy: 1.0\n",
            "  Batch 6899/8000, Loss: 0.10318607091903687, Accuracy: 1.0\n",
            "  Batch 6900/8000, Loss: 0.284970760345459, Accuracy: 1.0\n",
            "  Batch 6901/8000, Loss: 0.29173678159713745, Accuracy: 1.0\n",
            "  Batch 6902/8000, Loss: 0.2788429260253906, Accuracy: 1.0\n",
            "  Batch 6903/8000, Loss: 1.8338056802749634, Accuracy: 0.0\n",
            "  Batch 6904/8000, Loss: 0.1011568233370781, Accuracy: 1.0\n",
            "  Batch 6905/8000, Loss: 0.10472604632377625, Accuracy: 1.0\n",
            "  Batch 6906/8000, Loss: 0.28423798084259033, Accuracy: 1.0\n",
            "  Batch 6907/8000, Loss: 0.27457910776138306, Accuracy: 1.0\n",
            "  Batch 6908/8000, Loss: 0.509167492389679, Accuracy: 1.0\n",
            "  Batch 6909/8000, Loss: 0.10046487301588058, Accuracy: 1.0\n",
            "  Batch 6910/8000, Loss: 1.9897054433822632, Accuracy: 0.0\n",
            "  Batch 6911/8000, Loss: 0.3816498816013336, Accuracy: 1.0\n",
            "  Batch 6912/8000, Loss: 0.25226181745529175, Accuracy: 1.0\n",
            "  Batch 6913/8000, Loss: 0.2625977396965027, Accuracy: 1.0\n",
            "  Batch 6914/8000, Loss: 0.42016613483428955, Accuracy: 1.0\n",
            "  Batch 6915/8000, Loss: 1.6792787313461304, Accuracy: 0.0\n",
            "  Batch 6916/8000, Loss: 0.10422834753990173, Accuracy: 1.0\n",
            "  Batch 6917/8000, Loss: 0.26100680232048035, Accuracy: 1.0\n",
            "  Batch 6918/8000, Loss: 0.29479455947875977, Accuracy: 1.0\n",
            "  Batch 6919/8000, Loss: 0.3827873170375824, Accuracy: 1.0\n",
            "  Batch 6920/8000, Loss: 0.27727627754211426, Accuracy: 1.0\n",
            "  Batch 6921/8000, Loss: 0.10030287504196167, Accuracy: 1.0\n",
            "  Batch 6922/8000, Loss: 0.31897011399269104, Accuracy: 1.0\n",
            "  Batch 6923/8000, Loss: 2.1725428104400635, Accuracy: 0.0\n",
            "  Batch 6924/8000, Loss: 0.304119348526001, Accuracy: 1.0\n",
            "  Batch 6925/8000, Loss: 1.7190098762512207, Accuracy: 0.0\n",
            "  Batch 6926/8000, Loss: 0.3690534830093384, Accuracy: 1.0\n",
            "  Batch 6927/8000, Loss: 2.1007447242736816, Accuracy: 0.0\n",
            "  Batch 6928/8000, Loss: 1.3411788940429688, Accuracy: 0.0\n",
            "  Batch 6929/8000, Loss: 0.362086683511734, Accuracy: 1.0\n",
            "  Batch 6930/8000, Loss: 0.1722990870475769, Accuracy: 1.0\n",
            "  Batch 6931/8000, Loss: 0.3017062842845917, Accuracy: 1.0\n",
            "  Batch 6932/8000, Loss: 0.11367443203926086, Accuracy: 1.0\n",
            "  Batch 6933/8000, Loss: 0.5825535655021667, Accuracy: 1.0\n",
            "  Batch 6934/8000, Loss: 0.2966189980506897, Accuracy: 1.0\n",
            "  Batch 6935/8000, Loss: 0.10060575604438782, Accuracy: 1.0\n",
            "  Batch 6936/8000, Loss: 0.29695624113082886, Accuracy: 1.0\n",
            "  Batch 6937/8000, Loss: 1.030474066734314, Accuracy: 0.0\n",
            "  Batch 6938/8000, Loss: 0.326860636472702, Accuracy: 1.0\n",
            "  Batch 6939/8000, Loss: 0.2903893291950226, Accuracy: 1.0\n",
            "  Batch 6940/8000, Loss: 0.29887187480926514, Accuracy: 1.0\n",
            "  Batch 6941/8000, Loss: 0.290214866399765, Accuracy: 1.0\n",
            "  Batch 6942/8000, Loss: 0.10026586055755615, Accuracy: 1.0\n",
            "  Batch 6943/8000, Loss: 0.7757893800735474, Accuracy: 1.0\n",
            "  Batch 6944/8000, Loss: 1.444741129875183, Accuracy: 0.0\n",
            "  Batch 6945/8000, Loss: 0.3068840801715851, Accuracy: 1.0\n",
            "  Batch 6946/8000, Loss: 0.10021105408668518, Accuracy: 1.0\n",
            "  Batch 6947/8000, Loss: 0.10062369704246521, Accuracy: 1.0\n",
            "  Batch 6948/8000, Loss: 0.2948437035083771, Accuracy: 1.0\n",
            "  Batch 6949/8000, Loss: 0.31426042318344116, Accuracy: 1.0\n",
            "  Batch 6950/8000, Loss: 0.3967478275299072, Accuracy: 1.0\n",
            "  Batch 6951/8000, Loss: 0.2750250995159149, Accuracy: 1.0\n",
            "  Batch 6952/8000, Loss: 0.2771618366241455, Accuracy: 1.0\n",
            "  Batch 6953/8000, Loss: 0.26337945461273193, Accuracy: 1.0\n",
            "  Batch 6954/8000, Loss: 0.26107800006866455, Accuracy: 1.0\n",
            "  Batch 6955/8000, Loss: 0.10065039992332458, Accuracy: 1.0\n",
            "  Batch 6956/8000, Loss: 1.2446885108947754, Accuracy: 0.0\n",
            "  Batch 6957/8000, Loss: 0.25467443466186523, Accuracy: 1.0\n",
            "  Batch 6958/8000, Loss: 0.29111695289611816, Accuracy: 1.0\n",
            "  Batch 6959/8000, Loss: 1.9694079160690308, Accuracy: 0.0\n",
            "  Batch 6960/8000, Loss: 0.2897658050060272, Accuracy: 1.0\n",
            "  Batch 6961/8000, Loss: 0.3655729591846466, Accuracy: 1.0\n",
            "  Batch 6962/8000, Loss: 0.12181901931762695, Accuracy: 1.0\n",
            "  Batch 6963/8000, Loss: 0.27204030752182007, Accuracy: 1.0\n",
            "  Batch 6964/8000, Loss: 0.3479118049144745, Accuracy: 1.0\n",
            "  Batch 6965/8000, Loss: 1.2038443088531494, Accuracy: 0.0\n",
            "  Batch 6966/8000, Loss: 0.2973388731479645, Accuracy: 1.0\n",
            "  Batch 6967/8000, Loss: 0.9661346673965454, Accuracy: 0.0\n",
            "  Batch 6968/8000, Loss: 0.18148602545261383, Accuracy: 1.0\n",
            "  Batch 6969/8000, Loss: 0.12418383359909058, Accuracy: 1.0\n",
            "  Batch 6970/8000, Loss: 0.13854475319385529, Accuracy: 1.0\n",
            "  Batch 6971/8000, Loss: 0.27191781997680664, Accuracy: 1.0\n",
            "  Batch 6972/8000, Loss: 0.2531616687774658, Accuracy: 1.0\n",
            "  Batch 6973/8000, Loss: 0.28768056631088257, Accuracy: 1.0\n",
            "  Batch 6974/8000, Loss: 1.6421892642974854, Accuracy: 0.0\n",
            "  Batch 6975/8000, Loss: 0.12027327716350555, Accuracy: 1.0\n",
            "  Batch 6976/8000, Loss: 0.27725186944007874, Accuracy: 1.0\n",
            "  Batch 6977/8000, Loss: 0.6208228468894958, Accuracy: 1.0\n",
            "  Batch 6978/8000, Loss: 0.2675444483757019, Accuracy: 1.0\n",
            "  Batch 6979/8000, Loss: 0.2632681131362915, Accuracy: 1.0\n",
            "  Batch 6980/8000, Loss: 0.3127985894680023, Accuracy: 1.0\n",
            "  Batch 6981/8000, Loss: 0.31923234462738037, Accuracy: 1.0\n",
            "  Batch 6982/8000, Loss: 0.2627635896205902, Accuracy: 1.0\n",
            "  Batch 6983/8000, Loss: 0.2949455678462982, Accuracy: 1.0\n",
            "  Batch 6984/8000, Loss: 0.10900133848190308, Accuracy: 1.0\n",
            "  Batch 6985/8000, Loss: 0.2569003403186798, Accuracy: 1.0\n",
            "  Batch 6986/8000, Loss: 0.10608362406492233, Accuracy: 1.0\n",
            "  Batch 6987/8000, Loss: 0.3060813248157501, Accuracy: 1.0\n",
            "  Batch 6988/8000, Loss: 1.6184680461883545, Accuracy: 0.0\n",
            "  Batch 6989/8000, Loss: 0.7362139225006104, Accuracy: 1.0\n",
            "  Batch 6990/8000, Loss: 1.8418989181518555, Accuracy: 0.0\n",
            "  Batch 6991/8000, Loss: 0.2741284966468811, Accuracy: 1.0\n",
            "  Batch 6992/8000, Loss: 0.9544371366500854, Accuracy: 0.0\n",
            "  Batch 6993/8000, Loss: 0.10016094148159027, Accuracy: 1.0\n",
            "  Batch 6994/8000, Loss: 1.2390767335891724, Accuracy: 0.0\n",
            "  Batch 6995/8000, Loss: 0.3104962408542633, Accuracy: 1.0\n",
            "  Batch 6996/8000, Loss: 1.8855215311050415, Accuracy: 0.0\n",
            "  Batch 6997/8000, Loss: 0.30004367232322693, Accuracy: 1.0\n",
            "  Batch 6998/8000, Loss: 0.2914365231990814, Accuracy: 1.0\n",
            "  Batch 6999/8000, Loss: 0.10337599366903305, Accuracy: 1.0\n",
            "  Batch 7000/8000, Loss: 0.31746557354927063, Accuracy: 1.0\n",
            "  Batch 7001/8000, Loss: 2.226017951965332, Accuracy: 0.0\n",
            "  Batch 7002/8000, Loss: 0.41281378269195557, Accuracy: 1.0\n",
            "  Batch 7003/8000, Loss: 0.3686641752719879, Accuracy: 1.0\n",
            "  Batch 7004/8000, Loss: 0.7270625829696655, Accuracy: 1.0\n",
            "  Batch 7005/8000, Loss: 0.2087588608264923, Accuracy: 1.0\n",
            "  Batch 7006/8000, Loss: 0.16060581803321838, Accuracy: 1.0\n",
            "  Batch 7007/8000, Loss: 0.2932504117488861, Accuracy: 1.0\n",
            "  Batch 7008/8000, Loss: 1.715867519378662, Accuracy: 0.0\n",
            "  Batch 7009/8000, Loss: 1.5670944452285767, Accuracy: 0.0\n",
            "  Batch 7010/8000, Loss: 0.2736949026584625, Accuracy: 1.0\n",
            "  Batch 7011/8000, Loss: 1.9447565078735352, Accuracy: 0.0\n",
            "  Batch 7012/8000, Loss: 0.2749016582965851, Accuracy: 1.0\n",
            "  Batch 7013/8000, Loss: 0.30899399518966675, Accuracy: 1.0\n",
            "  Batch 7014/8000, Loss: 0.09993486851453781, Accuracy: 1.0\n",
            "  Batch 7015/8000, Loss: 0.30547258257865906, Accuracy: 1.0\n",
            "  Batch 7016/8000, Loss: 0.11097076535224915, Accuracy: 1.0\n",
            "  Batch 7017/8000, Loss: 0.9583248496055603, Accuracy: 0.0\n",
            "  Batch 7018/8000, Loss: 0.5267139077186584, Accuracy: 1.0\n",
            "  Batch 7019/8000, Loss: 0.8225358128547668, Accuracy: 0.0\n",
            "  Batch 7020/8000, Loss: 0.7505372166633606, Accuracy: 1.0\n",
            "  Batch 7021/8000, Loss: 1.393166422843933, Accuracy: 0.0\n",
            "  Batch 7022/8000, Loss: 0.3390924036502838, Accuracy: 1.0\n",
            "  Batch 7023/8000, Loss: 0.3795790374279022, Accuracy: 1.0\n",
            "  Batch 7024/8000, Loss: 0.3448027968406677, Accuracy: 1.0\n",
            "  Batch 7025/8000, Loss: 0.4261738657951355, Accuracy: 1.0\n",
            "  Batch 7026/8000, Loss: 0.7650282382965088, Accuracy: 1.0\n",
            "  Batch 7027/8000, Loss: 0.6699519753456116, Accuracy: 1.0\n",
            "  Batch 7028/8000, Loss: 1.102553367614746, Accuracy: 0.0\n",
            "  Batch 7029/8000, Loss: 0.2865830659866333, Accuracy: 1.0\n",
            "  Batch 7030/8000, Loss: 0.3172144293785095, Accuracy: 1.0\n",
            "  Batch 7031/8000, Loss: 0.3795973062515259, Accuracy: 1.0\n",
            "  Batch 7032/8000, Loss: 1.7505390644073486, Accuracy: 0.0\n",
            "  Batch 7033/8000, Loss: 0.3253314793109894, Accuracy: 1.0\n",
            "  Batch 7034/8000, Loss: 0.27725571393966675, Accuracy: 1.0\n",
            "  Batch 7035/8000, Loss: 0.740231990814209, Accuracy: 1.0\n",
            "  Batch 7036/8000, Loss: 0.6991550326347351, Accuracy: 1.0\n",
            "  Batch 7037/8000, Loss: 0.4780455231666565, Accuracy: 1.0\n",
            "  Batch 7038/8000, Loss: 0.10037063807249069, Accuracy: 1.0\n",
            "  Batch 7039/8000, Loss: 0.11324935406446457, Accuracy: 1.0\n",
            "  Batch 7040/8000, Loss: 0.1390368789434433, Accuracy: 1.0\n",
            "  Batch 7041/8000, Loss: 1.9265227317810059, Accuracy: 0.0\n",
            "  Batch 7042/8000, Loss: 0.37256672978401184, Accuracy: 1.0\n",
            "  Batch 7043/8000, Loss: 0.133060485124588, Accuracy: 1.0\n",
            "  Batch 7044/8000, Loss: 0.1477227360010147, Accuracy: 1.0\n",
            "  Batch 7045/8000, Loss: 0.3641606867313385, Accuracy: 1.0\n",
            "  Batch 7046/8000, Loss: 0.11961489915847778, Accuracy: 1.0\n",
            "  Batch 7047/8000, Loss: 0.3290316164493561, Accuracy: 1.0\n",
            "  Batch 7048/8000, Loss: 0.3347896933555603, Accuracy: 1.0\n",
            "  Batch 7049/8000, Loss: 0.9770641326904297, Accuracy: 0.0\n",
            "  Batch 7050/8000, Loss: 0.25395992398262024, Accuracy: 1.0\n",
            "  Batch 7051/8000, Loss: 0.709898054599762, Accuracy: 1.0\n",
            "  Batch 7052/8000, Loss: 0.48179563879966736, Accuracy: 1.0\n",
            "  Batch 7053/8000, Loss: 0.3028844892978668, Accuracy: 1.0\n",
            "  Batch 7054/8000, Loss: 0.34404221177101135, Accuracy: 1.0\n",
            "  Batch 7055/8000, Loss: 0.7321616411209106, Accuracy: 1.0\n",
            "  Batch 7056/8000, Loss: 0.3419899642467499, Accuracy: 1.0\n",
            "  Batch 7057/8000, Loss: 0.10617922246456146, Accuracy: 1.0\n",
            "  Batch 7058/8000, Loss: 0.2893638610839844, Accuracy: 1.0\n",
            "  Batch 7059/8000, Loss: 1.8428832292556763, Accuracy: 0.0\n",
            "  Batch 7060/8000, Loss: 0.12692730128765106, Accuracy: 1.0\n",
            "  Batch 7061/8000, Loss: 0.4591606855392456, Accuracy: 1.0\n",
            "  Batch 7062/8000, Loss: 0.31267908215522766, Accuracy: 1.0\n",
            "  Batch 7063/8000, Loss: 0.20831887423992157, Accuracy: 1.0\n",
            "  Batch 7064/8000, Loss: 0.232870951294899, Accuracy: 1.0\n",
            "  Batch 7065/8000, Loss: 0.3624480366706848, Accuracy: 1.0\n",
            "  Batch 7066/8000, Loss: 0.33931925892829895, Accuracy: 1.0\n",
            "  Batch 7067/8000, Loss: 1.2101314067840576, Accuracy: 0.0\n",
            "  Batch 7068/8000, Loss: 0.7713544964790344, Accuracy: 1.0\n",
            "  Batch 7069/8000, Loss: 0.3425031304359436, Accuracy: 1.0\n",
            "  Batch 7070/8000, Loss: 0.2689819037914276, Accuracy: 1.0\n",
            "  Batch 7071/8000, Loss: 2.1101906299591064, Accuracy: 0.0\n",
            "  Batch 7072/8000, Loss: 0.1223776787519455, Accuracy: 1.0\n",
            "  Batch 7073/8000, Loss: 0.2608555853366852, Accuracy: 1.0\n",
            "  Batch 7074/8000, Loss: 0.3421228229999542, Accuracy: 1.0\n",
            "  Batch 7075/8000, Loss: 0.5720378160476685, Accuracy: 1.0\n",
            "  Batch 7076/8000, Loss: 0.2627323269844055, Accuracy: 1.0\n",
            "  Batch 7077/8000, Loss: 0.25425055623054504, Accuracy: 1.0\n",
            "  Batch 7078/8000, Loss: 0.6074740290641785, Accuracy: 1.0\n",
            "  Batch 7079/8000, Loss: 1.5784971714019775, Accuracy: 0.0\n",
            "  Batch 7080/8000, Loss: 0.10689163208007812, Accuracy: 1.0\n",
            "  Batch 7081/8000, Loss: 1.018276572227478, Accuracy: 0.0\n",
            "  Batch 7082/8000, Loss: 2.1401517391204834, Accuracy: 0.0\n",
            "  Batch 7083/8000, Loss: 0.3191447854042053, Accuracy: 1.0\n",
            "  Batch 7084/8000, Loss: 0.11329524219036102, Accuracy: 1.0\n",
            "  Batch 7085/8000, Loss: 1.6904232501983643, Accuracy: 0.0\n",
            "  Batch 7086/8000, Loss: 0.2812272012233734, Accuracy: 1.0\n",
            "  Batch 7087/8000, Loss: 0.35259145498275757, Accuracy: 1.0\n",
            "  Batch 7088/8000, Loss: 1.7034426927566528, Accuracy: 0.0\n",
            "  Batch 7089/8000, Loss: 0.8336395025253296, Accuracy: 0.0\n",
            "  Batch 7090/8000, Loss: 0.40311920642852783, Accuracy: 1.0\n",
            "  Batch 7091/8000, Loss: 1.5594007968902588, Accuracy: 0.0\n",
            "  Batch 7092/8000, Loss: 0.10940837860107422, Accuracy: 1.0\n",
            "  Batch 7093/8000, Loss: 1.6276235580444336, Accuracy: 0.0\n",
            "  Batch 7094/8000, Loss: 0.10237237066030502, Accuracy: 1.0\n",
            "  Batch 7095/8000, Loss: 1.573724389076233, Accuracy: 0.0\n",
            "  Batch 7096/8000, Loss: 0.7113590836524963, Accuracy: 1.0\n",
            "  Batch 7097/8000, Loss: 0.10181030631065369, Accuracy: 1.0\n",
            "  Batch 7098/8000, Loss: 1.1466829776763916, Accuracy: 0.0\n",
            "  Batch 7099/8000, Loss: 0.3692278265953064, Accuracy: 1.0\n",
            "  Batch 7100/8000, Loss: 0.10058256983757019, Accuracy: 1.0\n",
            "  Batch 7101/8000, Loss: 0.3854180574417114, Accuracy: 1.0\n",
            "  Batch 7102/8000, Loss: 0.10200564563274384, Accuracy: 1.0\n",
            "  Batch 7103/8000, Loss: 1.5730582475662231, Accuracy: 0.0\n",
            "  Batch 7104/8000, Loss: 0.4702749252319336, Accuracy: 1.0\n",
            "  Batch 7105/8000, Loss: 0.6902289986610413, Accuracy: 1.0\n",
            "  Batch 7106/8000, Loss: 0.3742225170135498, Accuracy: 1.0\n",
            "  Batch 7107/8000, Loss: 0.47876209020614624, Accuracy: 1.0\n",
            "  Batch 7108/8000, Loss: 0.3969564437866211, Accuracy: 1.0\n",
            "  Batch 7109/8000, Loss: 0.6525246500968933, Accuracy: 1.0\n",
            "  Batch 7110/8000, Loss: 1.4715322256088257, Accuracy: 0.0\n",
            "  Batch 7111/8000, Loss: 0.46833306550979614, Accuracy: 1.0\n",
            "  Batch 7112/8000, Loss: 0.10519473999738693, Accuracy: 1.0\n",
            "  Batch 7113/8000, Loss: 1.234452724456787, Accuracy: 0.0\n",
            "  Batch 7114/8000, Loss: 0.10621895641088486, Accuracy: 1.0\n",
            "  Batch 7115/8000, Loss: 0.973758339881897, Accuracy: 0.0\n",
            "  Batch 7116/8000, Loss: 1.393813133239746, Accuracy: 0.0\n",
            "  Batch 7117/8000, Loss: 2.9460322856903076, Accuracy: 0.0\n",
            "  Batch 7118/8000, Loss: 1.126786231994629, Accuracy: 0.0\n",
            "  Batch 7119/8000, Loss: 0.4990755021572113, Accuracy: 1.0\n",
            "  Batch 7120/8000, Loss: 0.6613050699234009, Accuracy: 1.0\n",
            "  Batch 7121/8000, Loss: 0.1041402518749237, Accuracy: 1.0\n",
            "  Batch 7122/8000, Loss: 0.1042076051235199, Accuracy: 1.0\n",
            "  Batch 7123/8000, Loss: 1.4468549489974976, Accuracy: 0.0\n",
            "  Batch 7124/8000, Loss: 0.9243518710136414, Accuracy: 0.0\n",
            "  Batch 7125/8000, Loss: 1.1318658590316772, Accuracy: 0.0\n",
            "  Batch 7126/8000, Loss: 0.4175485074520111, Accuracy: 1.0\n",
            "  Batch 7127/8000, Loss: 0.669823408126831, Accuracy: 1.0\n",
            "  Batch 7128/8000, Loss: 0.10270515084266663, Accuracy: 1.0\n",
            "  Batch 7129/8000, Loss: 0.1069502979516983, Accuracy: 1.0\n",
            "  Batch 7130/8000, Loss: 0.49804234504699707, Accuracy: 1.0\n",
            "  Batch 7131/8000, Loss: 0.10121609270572662, Accuracy: 1.0\n",
            "  Batch 7132/8000, Loss: 0.9162869453430176, Accuracy: 0.0\n",
            "  Batch 7133/8000, Loss: 0.15577328205108643, Accuracy: 1.0\n",
            "  Batch 7134/8000, Loss: 0.10405278950929642, Accuracy: 1.0\n",
            "  Batch 7135/8000, Loss: 0.10673902928829193, Accuracy: 1.0\n",
            "  Batch 7136/8000, Loss: 0.5531806945800781, Accuracy: 1.0\n",
            "  Batch 7137/8000, Loss: 0.09938070178031921, Accuracy: 1.0\n",
            "  Batch 7138/8000, Loss: 0.8012603521347046, Accuracy: 0.0\n",
            "  Batch 7139/8000, Loss: 0.3280068039894104, Accuracy: 1.0\n",
            "  Batch 7140/8000, Loss: 0.8735852241516113, Accuracy: 0.0\n",
            "  Batch 7141/8000, Loss: 0.4711153507232666, Accuracy: 1.0\n",
            "  Batch 7142/8000, Loss: 0.104965440928936, Accuracy: 1.0\n",
            "  Batch 7143/8000, Loss: 0.7248744964599609, Accuracy: 1.0\n",
            "  Batch 7144/8000, Loss: 0.669783890247345, Accuracy: 1.0\n",
            "  Batch 7145/8000, Loss: 0.1104603111743927, Accuracy: 1.0\n",
            "  Batch 7146/8000, Loss: 0.8348961472511292, Accuracy: 0.0\n",
            "  Batch 7147/8000, Loss: 0.444160521030426, Accuracy: 1.0\n",
            "  Batch 7148/8000, Loss: 0.8985241055488586, Accuracy: 0.0\n",
            "  Batch 7149/8000, Loss: 0.09981587529182434, Accuracy: 1.0\n",
            "  Batch 7150/8000, Loss: 0.4298418462276459, Accuracy: 1.0\n",
            "  Batch 7151/8000, Loss: 0.09924375265836716, Accuracy: 1.0\n",
            "  Batch 7152/8000, Loss: 0.22241432964801788, Accuracy: 1.0\n",
            "  Batch 7153/8000, Loss: 0.44812095165252686, Accuracy: 1.0\n",
            "  Batch 7154/8000, Loss: 1.2582148313522339, Accuracy: 0.0\n",
            "  Batch 7155/8000, Loss: 1.2478491067886353, Accuracy: 0.0\n",
            "  Batch 7156/8000, Loss: 0.09955273568630219, Accuracy: 1.0\n",
            "  Batch 7157/8000, Loss: 0.9416234493255615, Accuracy: 0.0\n",
            "  Batch 7158/8000, Loss: 0.834892749786377, Accuracy: 0.0\n",
            "  Batch 7159/8000, Loss: 0.4511258006095886, Accuracy: 1.0\n",
            "  Batch 7160/8000, Loss: 0.48223698139190674, Accuracy: 1.0\n",
            "  Batch 7161/8000, Loss: 0.11006700992584229, Accuracy: 1.0\n",
            "  Batch 7162/8000, Loss: 0.3442307710647583, Accuracy: 1.0\n",
            "  Batch 7163/8000, Loss: 0.1072688102722168, Accuracy: 1.0\n",
            "  Batch 7164/8000, Loss: 0.9099566340446472, Accuracy: 0.0\n",
            "  Batch 7165/8000, Loss: 0.10396302491426468, Accuracy: 1.0\n",
            "  Batch 7166/8000, Loss: 0.1068335473537445, Accuracy: 1.0\n",
            "  Batch 7167/8000, Loss: 0.8049262762069702, Accuracy: 0.0\n",
            "  Batch 7168/8000, Loss: 0.10411140322685242, Accuracy: 1.0\n",
            "  Batch 7169/8000, Loss: 0.10743981599807739, Accuracy: 1.0\n",
            "  Batch 7170/8000, Loss: 0.160908043384552, Accuracy: 1.0\n",
            "  Batch 7171/8000, Loss: 0.481461763381958, Accuracy: 1.0\n",
            "  Batch 7172/8000, Loss: 0.5602260231971741, Accuracy: 1.0\n",
            "  Batch 7173/8000, Loss: 0.9136945605278015, Accuracy: 0.0\n",
            "  Batch 7174/8000, Loss: 0.7263625860214233, Accuracy: 1.0\n",
            "  Batch 7175/8000, Loss: 0.881348192691803, Accuracy: 0.0\n",
            "  Batch 7176/8000, Loss: 1.1254559755325317, Accuracy: 0.0\n",
            "  Batch 7177/8000, Loss: 1.402241826057434, Accuracy: 0.0\n",
            "  Batch 7178/8000, Loss: 0.44713959097862244, Accuracy: 1.0\n",
            "  Batch 7179/8000, Loss: 1.382460355758667, Accuracy: 0.0\n",
            "  Batch 7180/8000, Loss: 0.5304336547851562, Accuracy: 1.0\n",
            "  Batch 7181/8000, Loss: 0.4269050657749176, Accuracy: 1.0\n",
            "  Batch 7182/8000, Loss: 0.44850075244903564, Accuracy: 1.0\n",
            "  Batch 7183/8000, Loss: 0.4534616768360138, Accuracy: 1.0\n",
            "  Batch 7184/8000, Loss: 0.4470157027244568, Accuracy: 1.0\n",
            "  Batch 7185/8000, Loss: 0.09964539110660553, Accuracy: 1.0\n",
            "  Batch 7186/8000, Loss: 0.10765272378921509, Accuracy: 1.0\n",
            "  Batch 7187/8000, Loss: 0.23982742428779602, Accuracy: 1.0\n",
            "  Batch 7188/8000, Loss: 0.42956316471099854, Accuracy: 1.0\n",
            "  Batch 7189/8000, Loss: 0.8805369138717651, Accuracy: 0.0\n",
            "  Batch 7190/8000, Loss: 0.10326986014842987, Accuracy: 1.0\n",
            "  Batch 7191/8000, Loss: 0.4085444211959839, Accuracy: 1.0\n",
            "  Batch 7192/8000, Loss: 0.5370491743087769, Accuracy: 1.0\n",
            "  Batch 7193/8000, Loss: 0.14006486535072327, Accuracy: 1.0\n",
            "  Batch 7194/8000, Loss: 0.5073685646057129, Accuracy: 1.0\n",
            "  Batch 7195/8000, Loss: 0.1406789869070053, Accuracy: 1.0\n",
            "  Batch 7196/8000, Loss: 0.5130413770675659, Accuracy: 1.0\n",
            "  Batch 7197/8000, Loss: 1.3128066062927246, Accuracy: 0.0\n",
            "  Batch 7198/8000, Loss: 1.1694071292877197, Accuracy: 0.0\n",
            "  Batch 7199/8000, Loss: 0.09919744729995728, Accuracy: 1.0\n",
            "  Batch 7200/8000, Loss: 0.104145348072052, Accuracy: 1.0\n",
            "  Batch 7201/8000, Loss: 0.8745477199554443, Accuracy: 0.0\n",
            "  Batch 7202/8000, Loss: 0.09943933039903641, Accuracy: 1.0\n",
            "  Batch 7203/8000, Loss: 0.4059937298297882, Accuracy: 1.0\n",
            "  Batch 7204/8000, Loss: 0.6177594661712646, Accuracy: 1.0\n",
            "  Batch 7205/8000, Loss: 0.3706308901309967, Accuracy: 1.0\n",
            "  Batch 7206/8000, Loss: 1.205428123474121, Accuracy: 0.0\n",
            "  Batch 7207/8000, Loss: 0.6707978844642639, Accuracy: 1.0\n",
            "  Batch 7208/8000, Loss: 1.4417390823364258, Accuracy: 0.0\n",
            "  Batch 7209/8000, Loss: 0.4117110073566437, Accuracy: 1.0\n",
            "  Batch 7210/8000, Loss: 0.11183664202690125, Accuracy: 1.0\n",
            "  Batch 7211/8000, Loss: 0.10178127139806747, Accuracy: 1.0\n",
            "  Batch 7212/8000, Loss: 0.5606606006622314, Accuracy: 1.0\n",
            "  Batch 7213/8000, Loss: 1.3707215785980225, Accuracy: 0.0\n",
            "  Batch 7214/8000, Loss: 1.3956000804901123, Accuracy: 0.0\n",
            "  Batch 7215/8000, Loss: 0.7800165414810181, Accuracy: 1.0\n",
            "  Batch 7216/8000, Loss: 1.413381576538086, Accuracy: 0.0\n",
            "  Batch 7217/8000, Loss: 0.44855886697769165, Accuracy: 1.0\n",
            "  Batch 7218/8000, Loss: 0.10197345167398453, Accuracy: 1.0\n",
            "  Batch 7219/8000, Loss: 0.12179572135210037, Accuracy: 1.0\n",
            "  Batch 7220/8000, Loss: 0.8408447504043579, Accuracy: 0.0\n",
            "  Batch 7221/8000, Loss: 0.09894651174545288, Accuracy: 1.0\n",
            "  Batch 7222/8000, Loss: 0.10225178301334381, Accuracy: 1.0\n",
            "  Batch 7223/8000, Loss: 0.6215243935585022, Accuracy: 1.0\n",
            "  Batch 7224/8000, Loss: 1.2936424016952515, Accuracy: 0.0\n",
            "  Batch 7225/8000, Loss: 0.6952041983604431, Accuracy: 1.0\n",
            "  Batch 7226/8000, Loss: 1.199329137802124, Accuracy: 0.0\n",
            "  Batch 7227/8000, Loss: 0.4809672236442566, Accuracy: 1.0\n",
            "  Batch 7228/8000, Loss: 0.8721315264701843, Accuracy: 0.0\n",
            "  Batch 7229/8000, Loss: 1.2286152839660645, Accuracy: 0.0\n",
            "  Batch 7230/8000, Loss: 0.48472800850868225, Accuracy: 1.0\n",
            "  Batch 7231/8000, Loss: 0.1189882755279541, Accuracy: 1.0\n",
            "  Batch 7232/8000, Loss: 1.1679977178573608, Accuracy: 0.0\n",
            "  Batch 7233/8000, Loss: 1.1310253143310547, Accuracy: 0.0\n",
            "  Batch 7234/8000, Loss: 0.5265036225318909, Accuracy: 1.0\n",
            "  Batch 7235/8000, Loss: 0.0999046191573143, Accuracy: 1.0\n",
            "  Batch 7236/8000, Loss: 1.0779577493667603, Accuracy: 0.0\n",
            "  Batch 7237/8000, Loss: 0.6366872787475586, Accuracy: 1.0\n",
            "  Batch 7238/8000, Loss: 0.5726994872093201, Accuracy: 1.0\n",
            "  Batch 7239/8000, Loss: 0.32308369874954224, Accuracy: 1.0\n",
            "  Batch 7240/8000, Loss: 0.5302067995071411, Accuracy: 1.0\n",
            "  Batch 7241/8000, Loss: 0.5646328330039978, Accuracy: 1.0\n",
            "  Batch 7242/8000, Loss: 0.5149157643318176, Accuracy: 1.0\n",
            "  Batch 7243/8000, Loss: 0.09980562329292297, Accuracy: 1.0\n",
            "  Batch 7244/8000, Loss: 1.1692336797714233, Accuracy: 0.0\n",
            "  Batch 7245/8000, Loss: 0.4972561001777649, Accuracy: 1.0\n",
            "  Batch 7246/8000, Loss: 1.0453698635101318, Accuracy: 0.0\n",
            "  Batch 7247/8000, Loss: 0.4978712201118469, Accuracy: 1.0\n",
            "  Batch 7248/8000, Loss: 0.0991305261850357, Accuracy: 1.0\n",
            "  Batch 7249/8000, Loss: 0.09983983635902405, Accuracy: 1.0\n",
            "  Batch 7250/8000, Loss: 0.508015513420105, Accuracy: 1.0\n",
            "  Batch 7251/8000, Loss: 1.0191116333007812, Accuracy: 0.0\n",
            "  Batch 7252/8000, Loss: 0.09969759732484818, Accuracy: 1.0\n",
            "  Batch 7253/8000, Loss: 0.9756003022193909, Accuracy: 0.0\n",
            "  Batch 7254/8000, Loss: 0.0992562472820282, Accuracy: 1.0\n",
            "  Batch 7255/8000, Loss: 0.7320951819419861, Accuracy: 1.0\n",
            "  Batch 7256/8000, Loss: 1.198878526687622, Accuracy: 0.0\n",
            "  Batch 7257/8000, Loss: 0.2678908705711365, Accuracy: 1.0\n",
            "  Batch 7258/8000, Loss: 0.09876349568367004, Accuracy: 1.0\n",
            "  Batch 7259/8000, Loss: 0.5140098333358765, Accuracy: 1.0\n",
            "  Batch 7260/8000, Loss: 0.7371507883071899, Accuracy: 1.0\n",
            "  Batch 7261/8000, Loss: 0.6366157531738281, Accuracy: 1.0\n",
            "  Batch 7262/8000, Loss: 0.09907788038253784, Accuracy: 1.0\n",
            "  Batch 7263/8000, Loss: 0.4844855070114136, Accuracy: 1.0\n",
            "  Batch 7264/8000, Loss: 0.09889709204435349, Accuracy: 1.0\n",
            "  Batch 7265/8000, Loss: 1.158690333366394, Accuracy: 0.0\n",
            "  Batch 7266/8000, Loss: 0.5565808415412903, Accuracy: 1.0\n",
            "  Batch 7267/8000, Loss: 0.7262990474700928, Accuracy: 1.0\n",
            "  Batch 7268/8000, Loss: 0.80485999584198, Accuracy: 0.0\n",
            "  Batch 7269/8000, Loss: 0.551246702671051, Accuracy: 1.0\n",
            "  Batch 7270/8000, Loss: 0.5373387932777405, Accuracy: 1.0\n",
            "  Batch 7271/8000, Loss: 0.5413908958435059, Accuracy: 1.0\n",
            "  Batch 7272/8000, Loss: 0.6199973821640015, Accuracy: 1.0\n",
            "  Batch 7273/8000, Loss: 1.1831783056259155, Accuracy: 0.0\n",
            "  Batch 7274/8000, Loss: 0.6146572232246399, Accuracy: 1.0\n",
            "  Batch 7275/8000, Loss: 0.11816909909248352, Accuracy: 1.0\n",
            "  Batch 7276/8000, Loss: 0.09869697690010071, Accuracy: 1.0\n",
            "  Batch 7277/8000, Loss: 1.2085368633270264, Accuracy: 0.0\n",
            "  Batch 7278/8000, Loss: 0.7579101920127869, Accuracy: 1.0\n",
            "  Batch 7279/8000, Loss: 0.6359559297561646, Accuracy: 1.0\n",
            "  Batch 7280/8000, Loss: 0.6741248369216919, Accuracy: 1.0\n",
            "  Batch 7281/8000, Loss: 0.10483309626579285, Accuracy: 1.0\n",
            "  Batch 7282/8000, Loss: 0.09882953763008118, Accuracy: 1.0\n",
            "  Batch 7283/8000, Loss: 0.10688972473144531, Accuracy: 1.0\n",
            "  Batch 7284/8000, Loss: 0.09908787161111832, Accuracy: 1.0\n",
            "  Batch 7285/8000, Loss: 1.1751919984817505, Accuracy: 0.0\n",
            "  Batch 7286/8000, Loss: 0.09887509047985077, Accuracy: 1.0\n",
            "  Batch 7287/8000, Loss: 0.09886828809976578, Accuracy: 1.0\n",
            "  Batch 7288/8000, Loss: 0.10006146132946014, Accuracy: 1.0\n",
            "  Batch 7289/8000, Loss: 0.09863460808992386, Accuracy: 1.0\n",
            "  Batch 7290/8000, Loss: 0.9976986646652222, Accuracy: 0.0\n",
            "  Batch 7291/8000, Loss: 0.09876852482557297, Accuracy: 1.0\n",
            "  Batch 7292/8000, Loss: 0.29291385412216187, Accuracy: 1.0\n",
            "  Batch 7293/8000, Loss: 0.10140040516853333, Accuracy: 1.0\n",
            "  Batch 7294/8000, Loss: 0.09900495409965515, Accuracy: 1.0\n",
            "  Batch 7295/8000, Loss: 0.48680293560028076, Accuracy: 1.0\n",
            "  Batch 7296/8000, Loss: 0.09944403916597366, Accuracy: 1.0\n",
            "  Batch 7297/8000, Loss: 0.2519363760948181, Accuracy: 1.0\n",
            "  Batch 7298/8000, Loss: 2.3141019344329834, Accuracy: 0.0\n",
            "  Batch 7299/8000, Loss: 0.9684299230575562, Accuracy: 0.0\n",
            "  Batch 7300/8000, Loss: 0.6325663924217224, Accuracy: 1.0\n",
            "  Batch 7301/8000, Loss: 1.155659794807434, Accuracy: 0.0\n",
            "  Batch 7302/8000, Loss: 0.5480747818946838, Accuracy: 1.0\n",
            "  Batch 7303/8000, Loss: 0.20266295969486237, Accuracy: 1.0\n",
            "  Batch 7304/8000, Loss: 0.5004210472106934, Accuracy: 1.0\n",
            "  Batch 7305/8000, Loss: 0.5221667289733887, Accuracy: 1.0\n",
            "  Batch 7306/8000, Loss: 1.2113032341003418, Accuracy: 0.0\n",
            "  Batch 7307/8000, Loss: 0.10107776522636414, Accuracy: 1.0\n",
            "  Batch 7308/8000, Loss: 0.10106098651885986, Accuracy: 1.0\n",
            "  Batch 7309/8000, Loss: 0.10158354043960571, Accuracy: 1.0\n",
            "  Batch 7310/8000, Loss: 0.10677799582481384, Accuracy: 1.0\n",
            "  Batch 7311/8000, Loss: 1.119054913520813, Accuracy: 0.0\n",
            "  Batch 7312/8000, Loss: 0.10454101860523224, Accuracy: 1.0\n",
            "  Batch 7313/8000, Loss: 0.0988444983959198, Accuracy: 1.0\n",
            "  Batch 7314/8000, Loss: 0.5560499429702759, Accuracy: 1.0\n",
            "  Batch 7315/8000, Loss: 0.8767280578613281, Accuracy: 0.0\n",
            "  Batch 7316/8000, Loss: 0.4906492531299591, Accuracy: 1.0\n",
            "  Batch 7317/8000, Loss: 0.4831541180610657, Accuracy: 1.0\n",
            "  Batch 7318/8000, Loss: 0.9872534871101379, Accuracy: 0.0\n",
            "  Batch 7319/8000, Loss: 0.10100135207176208, Accuracy: 1.0\n",
            "  Batch 7320/8000, Loss: 0.09871665388345718, Accuracy: 1.0\n",
            "  Batch 7321/8000, Loss: 0.493449866771698, Accuracy: 1.0\n",
            "  Batch 7322/8000, Loss: 1.0915604829788208, Accuracy: 0.0\n",
            "  Batch 7323/8000, Loss: 1.2376186847686768, Accuracy: 0.0\n",
            "  Batch 7324/8000, Loss: 0.4897426962852478, Accuracy: 1.0\n",
            "  Batch 7325/8000, Loss: 0.5321671366691589, Accuracy: 1.0\n",
            "  Batch 7326/8000, Loss: 2.771101713180542, Accuracy: 0.0\n",
            "  Batch 7327/8000, Loss: 0.10005925595760345, Accuracy: 1.0\n",
            "  Batch 7328/8000, Loss: 1.208266258239746, Accuracy: 0.0\n",
            "  Batch 7329/8000, Loss: 0.10331273078918457, Accuracy: 1.0\n",
            "  Batch 7330/8000, Loss: 0.9639977812767029, Accuracy: 0.0\n",
            "  Batch 7331/8000, Loss: 0.49085432291030884, Accuracy: 1.0\n",
            "  Batch 7332/8000, Loss: 1.1393649578094482, Accuracy: 0.0\n",
            "  Batch 7333/8000, Loss: 0.900449275970459, Accuracy: 0.0\n",
            "  Batch 7334/8000, Loss: 0.4917445182800293, Accuracy: 1.0\n",
            "  Batch 7335/8000, Loss: 0.1377173364162445, Accuracy: 1.0\n",
            "  Batch 7336/8000, Loss: 0.1045268326997757, Accuracy: 1.0\n",
            "  Batch 7337/8000, Loss: 0.5089922547340393, Accuracy: 1.0\n",
            "  Batch 7338/8000, Loss: 0.12277630716562271, Accuracy: 1.0\n",
            "  Batch 7339/8000, Loss: 0.9071222543716431, Accuracy: 0.0\n",
            "  Batch 7340/8000, Loss: 0.5105613470077515, Accuracy: 1.0\n",
            "  Batch 7341/8000, Loss: 0.613838255405426, Accuracy: 1.0\n",
            "  Batch 7342/8000, Loss: 0.566312313079834, Accuracy: 1.0\n",
            "  Batch 7343/8000, Loss: 0.5046967267990112, Accuracy: 1.0\n",
            "  Batch 7344/8000, Loss: 1.1876485347747803, Accuracy: 0.0\n",
            "  Batch 7345/8000, Loss: 0.0986015647649765, Accuracy: 1.0\n",
            "  Batch 7346/8000, Loss: 0.10647381842136383, Accuracy: 1.0\n",
            "  Batch 7347/8000, Loss: 0.09874764084815979, Accuracy: 1.0\n",
            "  Batch 7348/8000, Loss: 0.09994202852249146, Accuracy: 1.0\n",
            "  Batch 7349/8000, Loss: 0.11556292325258255, Accuracy: 1.0\n",
            "  Batch 7350/8000, Loss: 0.1103888526558876, Accuracy: 1.0\n",
            "  Batch 7351/8000, Loss: 1.0739903450012207, Accuracy: 0.0\n",
            "  Batch 7352/8000, Loss: 0.5011321306228638, Accuracy: 1.0\n",
            "  Batch 7353/8000, Loss: 0.49100688099861145, Accuracy: 1.0\n",
            "  Batch 7354/8000, Loss: 1.222121000289917, Accuracy: 0.0\n",
            "  Batch 7355/8000, Loss: 0.7128248810768127, Accuracy: 1.0\n",
            "  Batch 7356/8000, Loss: 0.7461453080177307, Accuracy: 1.0\n",
            "  Batch 7357/8000, Loss: 0.11155778914690018, Accuracy: 1.0\n",
            "  Batch 7358/8000, Loss: 0.7620331645011902, Accuracy: 1.0\n",
            "  Batch 7359/8000, Loss: 1.0007753372192383, Accuracy: 0.0\n",
            "  Batch 7360/8000, Loss: 0.0985092967748642, Accuracy: 1.0\n",
            "  Batch 7361/8000, Loss: 1.2269545793533325, Accuracy: 0.0\n",
            "  Batch 7362/8000, Loss: 0.5745216608047485, Accuracy: 1.0\n",
            "  Batch 7363/8000, Loss: 0.5772976875305176, Accuracy: 1.0\n",
            "  Batch 7364/8000, Loss: 1.1453248262405396, Accuracy: 0.0\n",
            "  Batch 7365/8000, Loss: 0.5701415538787842, Accuracy: 1.0\n",
            "  Batch 7366/8000, Loss: 0.6941452622413635, Accuracy: 1.0\n",
            "  Batch 7367/8000, Loss: 0.7581698298454285, Accuracy: 1.0\n",
            "  Batch 7368/8000, Loss: 0.5008367896080017, Accuracy: 1.0\n",
            "  Batch 7369/8000, Loss: 0.7192313075065613, Accuracy: 1.0\n",
            "  Batch 7370/8000, Loss: 0.5235285758972168, Accuracy: 1.0\n",
            "  Batch 7371/8000, Loss: 0.5483365058898926, Accuracy: 1.0\n",
            "  Batch 7372/8000, Loss: 0.5491978526115417, Accuracy: 1.0\n",
            "  Batch 7373/8000, Loss: 0.11720317602157593, Accuracy: 1.0\n",
            "  Batch 7374/8000, Loss: 0.5525835156440735, Accuracy: 1.0\n",
            "  Batch 7375/8000, Loss: 0.11591324210166931, Accuracy: 1.0\n",
            "  Batch 7376/8000, Loss: 0.9618971943855286, Accuracy: 0.0\n",
            "  Batch 7377/8000, Loss: 0.09835100919008255, Accuracy: 1.0\n",
            "  Batch 7378/8000, Loss: 0.1048850268125534, Accuracy: 1.0\n",
            "  Batch 7379/8000, Loss: 0.47902727127075195, Accuracy: 1.0\n",
            "  Batch 7380/8000, Loss: 1.128604769706726, Accuracy: 0.0\n",
            "  Batch 7381/8000, Loss: 0.7307244539260864, Accuracy: 1.0\n",
            "  Batch 7382/8000, Loss: 0.09832298755645752, Accuracy: 1.0\n",
            "  Batch 7383/8000, Loss: 0.09872555732727051, Accuracy: 1.0\n",
            "  Batch 7384/8000, Loss: 0.5336245894432068, Accuracy: 1.0\n",
            "  Batch 7385/8000, Loss: 0.5383076071739197, Accuracy: 1.0\n",
            "  Batch 7386/8000, Loss: 0.4697432518005371, Accuracy: 1.0\n",
            "  Batch 7387/8000, Loss: 0.540605366230011, Accuracy: 1.0\n",
            "  Batch 7388/8000, Loss: 0.48472920060157776, Accuracy: 1.0\n",
            "  Batch 7389/8000, Loss: 0.11287616938352585, Accuracy: 1.0\n",
            "  Batch 7390/8000, Loss: 1.306606650352478, Accuracy: 0.0\n",
            "  Batch 7391/8000, Loss: 0.5041316747665405, Accuracy: 1.0\n",
            "  Batch 7392/8000, Loss: 0.4623991847038269, Accuracy: 1.0\n",
            "  Batch 7393/8000, Loss: 1.1442310810089111, Accuracy: 0.0\n",
            "  Batch 7394/8000, Loss: 0.4296613335609436, Accuracy: 1.0\n",
            "  Batch 7395/8000, Loss: 0.0985286682844162, Accuracy: 1.0\n",
            "  Batch 7396/8000, Loss: 0.5364514589309692, Accuracy: 1.0\n",
            "  Batch 7397/8000, Loss: 0.483105331659317, Accuracy: 1.0\n",
            "  Batch 7398/8000, Loss: 0.7083774209022522, Accuracy: 1.0\n",
            "  Batch 7399/8000, Loss: 0.7709531188011169, Accuracy: 1.0\n",
            "  Batch 7400/8000, Loss: 0.47596412897109985, Accuracy: 1.0\n",
            "  Batch 7401/8000, Loss: 0.46807199716567993, Accuracy: 1.0\n",
            "  Batch 7402/8000, Loss: 0.5133540630340576, Accuracy: 1.0\n",
            "  Batch 7403/8000, Loss: 0.45496639609336853, Accuracy: 1.0\n",
            "  Batch 7404/8000, Loss: 0.4356497526168823, Accuracy: 1.0\n",
            "  Batch 7405/8000, Loss: 0.4501500725746155, Accuracy: 1.0\n",
            "  Batch 7406/8000, Loss: 0.47970426082611084, Accuracy: 1.0\n",
            "  Batch 7407/8000, Loss: 0.11554679274559021, Accuracy: 1.0\n",
            "  Batch 7408/8000, Loss: 0.4455750584602356, Accuracy: 1.0\n",
            "  Batch 7409/8000, Loss: 0.4390428364276886, Accuracy: 1.0\n",
            "  Batch 7410/8000, Loss: 1.3141506910324097, Accuracy: 0.0\n",
            "  Batch 7411/8000, Loss: 0.1331224888563156, Accuracy: 1.0\n",
            "  Batch 7412/8000, Loss: 0.10638464987277985, Accuracy: 1.0\n",
            "  Batch 7413/8000, Loss: 0.6303218007087708, Accuracy: 1.0\n",
            "  Batch 7414/8000, Loss: 1.283370852470398, Accuracy: 0.0\n",
            "  Batch 7415/8000, Loss: 0.11420153081417084, Accuracy: 1.0\n",
            "  Batch 7416/8000, Loss: 0.10358144342899323, Accuracy: 1.0\n",
            "  Batch 7417/8000, Loss: 0.5405919551849365, Accuracy: 1.0\n",
            "  Batch 7418/8000, Loss: 0.6759390830993652, Accuracy: 1.0\n",
            "  Batch 7419/8000, Loss: 0.10634470731019974, Accuracy: 1.0\n",
            "  Batch 7420/8000, Loss: 0.4182155132293701, Accuracy: 1.0\n",
            "  Batch 7421/8000, Loss: 0.7050198912620544, Accuracy: 1.0\n",
            "  Batch 7422/8000, Loss: 0.9897578954696655, Accuracy: 0.0\n",
            "  Batch 7423/8000, Loss: 1.1379557847976685, Accuracy: 0.0\n",
            "  Batch 7424/8000, Loss: 1.0827230215072632, Accuracy: 0.0\n",
            "  Batch 7425/8000, Loss: 1.404287338256836, Accuracy: 0.0\n",
            "  Batch 7426/8000, Loss: 0.5513926148414612, Accuracy: 1.0\n",
            "  Batch 7427/8000, Loss: 0.6394744515419006, Accuracy: 1.0\n",
            "  Batch 7428/8000, Loss: 0.46110472083091736, Accuracy: 1.0\n",
            "  Batch 7429/8000, Loss: 1.025600790977478, Accuracy: 0.0\n",
            "  Batch 7430/8000, Loss: 0.3985130190849304, Accuracy: 1.0\n",
            "  Batch 7431/8000, Loss: 0.9820775985717773, Accuracy: 0.0\n",
            "  Batch 7432/8000, Loss: 0.5975402593612671, Accuracy: 1.0\n",
            "  Batch 7433/8000, Loss: 1.3520089387893677, Accuracy: 0.0\n",
            "  Batch 7434/8000, Loss: 0.6303079724311829, Accuracy: 1.0\n",
            "  Batch 7435/8000, Loss: 0.399290531873703, Accuracy: 1.0\n",
            "  Batch 7436/8000, Loss: 0.6397258639335632, Accuracy: 1.0\n",
            "  Batch 7437/8000, Loss: 0.45885106921195984, Accuracy: 1.0\n",
            "  Batch 7438/8000, Loss: 0.6870902180671692, Accuracy: 1.0\n",
            "  Batch 7439/8000, Loss: 0.5069422125816345, Accuracy: 1.0\n",
            "  Batch 7440/8000, Loss: 0.10146866738796234, Accuracy: 1.0\n",
            "  Batch 7441/8000, Loss: 0.09846135973930359, Accuracy: 1.0\n",
            "  Batch 7442/8000, Loss: 0.09805291891098022, Accuracy: 1.0\n",
            "  Batch 7443/8000, Loss: 0.10660812258720398, Accuracy: 1.0\n",
            "  Batch 7444/8000, Loss: 0.6251730918884277, Accuracy: 1.0\n",
            "  Batch 7445/8000, Loss: 0.09840013086795807, Accuracy: 1.0\n",
            "  Batch 7446/8000, Loss: 0.44091540575027466, Accuracy: 1.0\n",
            "  Batch 7447/8000, Loss: 1.2833693027496338, Accuracy: 0.0\n",
            "  Batch 7448/8000, Loss: 1.3216556310653687, Accuracy: 0.0\n",
            "  Batch 7449/8000, Loss: 0.45447012782096863, Accuracy: 1.0\n",
            "  Batch 7450/8000, Loss: 0.4384859502315521, Accuracy: 1.0\n",
            "  Batch 7451/8000, Loss: 0.625694751739502, Accuracy: 1.0\n",
            "  Batch 7452/8000, Loss: 0.5038588047027588, Accuracy: 1.0\n",
            "  Batch 7453/8000, Loss: 0.10384891927242279, Accuracy: 1.0\n",
            "  Batch 7454/8000, Loss: 0.1016530692577362, Accuracy: 1.0\n",
            "  Batch 7455/8000, Loss: 0.44821834564208984, Accuracy: 1.0\n",
            "  Batch 7456/8000, Loss: 0.09797005355358124, Accuracy: 1.0\n",
            "  Batch 7457/8000, Loss: 1.3104103803634644, Accuracy: 0.0\n",
            "  Batch 7458/8000, Loss: 0.4336862862110138, Accuracy: 1.0\n",
            "  Batch 7459/8000, Loss: 0.5772851705551147, Accuracy: 1.0\n",
            "  Batch 7460/8000, Loss: 1.0159015655517578, Accuracy: 0.0\n",
            "  Batch 7461/8000, Loss: 0.09793351590633392, Accuracy: 1.0\n",
            "  Batch 7462/8000, Loss: 0.49641066789627075, Accuracy: 1.0\n",
            "  Batch 7463/8000, Loss: 1.0424989461898804, Accuracy: 0.0\n",
            "  Batch 7464/8000, Loss: 0.09798727929592133, Accuracy: 1.0\n",
            "  Batch 7465/8000, Loss: 1.4352658987045288, Accuracy: 0.0\n",
            "  Batch 7466/8000, Loss: 0.45676520466804504, Accuracy: 1.0\n",
            "  Batch 7467/8000, Loss: 0.47455310821533203, Accuracy: 1.0\n",
            "  Batch 7468/8000, Loss: 0.09947369992733002, Accuracy: 1.0\n",
            "  Batch 7469/8000, Loss: 0.45465779304504395, Accuracy: 1.0\n",
            "  Batch 7470/8000, Loss: 0.6333556175231934, Accuracy: 1.0\n",
            "  Batch 7471/8000, Loss: 0.09787605702877045, Accuracy: 1.0\n",
            "  Batch 7472/8000, Loss: 0.4110093116760254, Accuracy: 1.0\n",
            "  Batch 7473/8000, Loss: 0.10172475874423981, Accuracy: 1.0\n",
            "  Batch 7474/8000, Loss: 0.5999401211738586, Accuracy: 1.0\n",
            "  Batch 7475/8000, Loss: 1.237542986869812, Accuracy: 0.0\n",
            "  Batch 7476/8000, Loss: 0.6106811761856079, Accuracy: 1.0\n",
            "  Batch 7477/8000, Loss: 0.11619002372026443, Accuracy: 1.0\n",
            "  Batch 7478/8000, Loss: 0.4933476746082306, Accuracy: 1.0\n",
            "  Batch 7479/8000, Loss: 0.4268746078014374, Accuracy: 1.0\n",
            "  Batch 7480/8000, Loss: 0.4228712022304535, Accuracy: 1.0\n",
            "  Batch 7481/8000, Loss: 0.4243071973323822, Accuracy: 1.0\n",
            "  Batch 7482/8000, Loss: 0.11448602378368378, Accuracy: 1.0\n",
            "  Batch 7483/8000, Loss: 0.48401719331741333, Accuracy: 1.0\n",
            "  Batch 7484/8000, Loss: 0.4085548520088196, Accuracy: 1.0\n",
            "  Batch 7485/8000, Loss: 1.0371040105819702, Accuracy: 0.0\n",
            "  Batch 7486/8000, Loss: 0.3613250255584717, Accuracy: 1.0\n",
            "  Batch 7487/8000, Loss: 0.4813782274723053, Accuracy: 1.0\n",
            "  Batch 7488/8000, Loss: 0.0985330194234848, Accuracy: 1.0\n",
            "  Batch 7489/8000, Loss: 0.36015838384628296, Accuracy: 1.0\n",
            "  Batch 7490/8000, Loss: 0.43351271748542786, Accuracy: 1.0\n",
            "  Batch 7491/8000, Loss: 0.10272756218910217, Accuracy: 1.0\n",
            "  Batch 7492/8000, Loss: 0.378368616104126, Accuracy: 1.0\n",
            "  Batch 7493/8000, Loss: 0.9549511671066284, Accuracy: 0.0\n",
            "  Batch 7494/8000, Loss: 0.3900449573993683, Accuracy: 1.0\n",
            "  Batch 7495/8000, Loss: 0.10052575170993805, Accuracy: 1.0\n",
            "  Batch 7496/8000, Loss: 0.09795522689819336, Accuracy: 1.0\n",
            "  Batch 7497/8000, Loss: 0.3981282413005829, Accuracy: 1.0\n",
            "  Batch 7498/8000, Loss: 0.3549037277698517, Accuracy: 1.0\n",
            "  Batch 7499/8000, Loss: 0.3359823226928711, Accuracy: 1.0\n",
            "  Batch 7500/8000, Loss: 0.36969873309135437, Accuracy: 1.0\n",
            "  Batch 7501/8000, Loss: 0.34242361783981323, Accuracy: 1.0\n",
            "  Batch 7502/8000, Loss: 0.8782007098197937, Accuracy: 0.0\n",
            "  Batch 7503/8000, Loss: 0.34904688596725464, Accuracy: 1.0\n",
            "  Batch 7504/8000, Loss: 0.09826014190912247, Accuracy: 1.0\n",
            "  Batch 7505/8000, Loss: 0.10198820382356644, Accuracy: 1.0\n",
            "  Batch 7506/8000, Loss: 0.3616698086261749, Accuracy: 1.0\n",
            "  Batch 7507/8000, Loss: 0.40709391236305237, Accuracy: 1.0\n",
            "  Batch 7508/8000, Loss: 0.41382715106010437, Accuracy: 1.0\n",
            "  Batch 7509/8000, Loss: 1.492098331451416, Accuracy: 0.0\n",
            "  Batch 7510/8000, Loss: 0.1850089132785797, Accuracy: 1.0\n",
            "  Batch 7511/8000, Loss: 0.33826974034309387, Accuracy: 1.0\n",
            "  Batch 7512/8000, Loss: 0.100290447473526, Accuracy: 1.0\n",
            "  Batch 7513/8000, Loss: 1.5995975732803345, Accuracy: 0.0\n",
            "  Batch 7514/8000, Loss: 0.5019832253456116, Accuracy: 1.0\n",
            "  Batch 7515/8000, Loss: 0.10075937211513519, Accuracy: 1.0\n",
            "  Batch 7516/8000, Loss: 0.6328579783439636, Accuracy: 1.0\n",
            "  Batch 7517/8000, Loss: 0.6135625839233398, Accuracy: 1.0\n",
            "  Batch 7518/8000, Loss: 0.6046856641769409, Accuracy: 1.0\n",
            "  Batch 7519/8000, Loss: 0.39794033765792847, Accuracy: 1.0\n",
            "  Batch 7520/8000, Loss: 1.3483325242996216, Accuracy: 0.0\n",
            "  Batch 7521/8000, Loss: 1.6299958229064941, Accuracy: 0.0\n",
            "  Batch 7522/8000, Loss: 0.3407001793384552, Accuracy: 1.0\n",
            "  Batch 7523/8000, Loss: 0.09913066774606705, Accuracy: 1.0\n",
            "  Batch 7524/8000, Loss: 2.040057420730591, Accuracy: 0.0\n",
            "  Batch 7525/8000, Loss: 0.5125119686126709, Accuracy: 1.0\n",
            "  Batch 7526/8000, Loss: 1.5976907014846802, Accuracy: 0.0\n",
            "  Batch 7527/8000, Loss: 0.3969246745109558, Accuracy: 1.0\n",
            "  Batch 7528/8000, Loss: 0.4383717179298401, Accuracy: 1.0\n",
            "  Batch 7529/8000, Loss: 0.10043521225452423, Accuracy: 1.0\n",
            "  Batch 7530/8000, Loss: 0.35746267437934875, Accuracy: 1.0\n",
            "  Batch 7531/8000, Loss: 0.38514435291290283, Accuracy: 1.0\n",
            "  Batch 7532/8000, Loss: 0.40570273995399475, Accuracy: 1.0\n",
            "  Batch 7533/8000, Loss: 0.5797327756881714, Accuracy: 1.0\n",
            "  Batch 7534/8000, Loss: 0.42423707246780396, Accuracy: 1.0\n",
            "  Batch 7535/8000, Loss: 0.11191149055957794, Accuracy: 1.0\n",
            "  Batch 7536/8000, Loss: 0.9365366101264954, Accuracy: 0.0\n",
            "  Batch 7537/8000, Loss: 0.3804157078266144, Accuracy: 1.0\n",
            "  Batch 7538/8000, Loss: 0.10587761551141739, Accuracy: 1.0\n",
            "  Batch 7539/8000, Loss: 0.3387790322303772, Accuracy: 1.0\n",
            "  Batch 7540/8000, Loss: 1.4238715171813965, Accuracy: 0.0\n",
            "  Batch 7541/8000, Loss: 0.5228478312492371, Accuracy: 1.0\n",
            "  Batch 7542/8000, Loss: 0.37879592180252075, Accuracy: 1.0\n",
            "  Batch 7543/8000, Loss: 0.11062582582235336, Accuracy: 1.0\n",
            "  Batch 7544/8000, Loss: 1.6350692510604858, Accuracy: 0.0\n",
            "  Batch 7545/8000, Loss: 0.3242250084877014, Accuracy: 1.0\n",
            "  Batch 7546/8000, Loss: 0.5790548324584961, Accuracy: 1.0\n",
            "  Batch 7547/8000, Loss: 1.5207220315933228, Accuracy: 0.0\n",
            "  Batch 7548/8000, Loss: 0.1070767343044281, Accuracy: 1.0\n",
            "  Batch 7549/8000, Loss: 0.18633800745010376, Accuracy: 1.0\n",
            "  Batch 7550/8000, Loss: 0.11349636316299438, Accuracy: 1.0\n",
            "  Batch 7551/8000, Loss: 0.1062900722026825, Accuracy: 1.0\n",
            "  Batch 7552/8000, Loss: 1.1919025182724, Accuracy: 0.0\n",
            "  Batch 7553/8000, Loss: 1.0654113292694092, Accuracy: 0.0\n",
            "  Batch 7554/8000, Loss: 0.4049496054649353, Accuracy: 1.0\n",
            "  Batch 7555/8000, Loss: 0.3491740822792053, Accuracy: 1.0\n",
            "  Batch 7556/8000, Loss: 0.10304148495197296, Accuracy: 1.0\n",
            "  Batch 7557/8000, Loss: 0.37227141857147217, Accuracy: 1.0\n",
            "  Batch 7558/8000, Loss: 0.434710294008255, Accuracy: 1.0\n",
            "  Batch 7559/8000, Loss: 0.09757719188928604, Accuracy: 1.0\n",
            "  Batch 7560/8000, Loss: 1.47571861743927, Accuracy: 0.0\n",
            "  Batch 7561/8000, Loss: 0.1002795398235321, Accuracy: 1.0\n",
            "  Batch 7562/8000, Loss: 0.45987996459007263, Accuracy: 1.0\n",
            "  Batch 7563/8000, Loss: 0.5894005298614502, Accuracy: 1.0\n",
            "  Batch 7564/8000, Loss: 0.41855287551879883, Accuracy: 1.0\n",
            "  Batch 7565/8000, Loss: 0.48813438415527344, Accuracy: 1.0\n",
            "  Batch 7566/8000, Loss: 1.1406913995742798, Accuracy: 0.0\n",
            "  Batch 7567/8000, Loss: 0.37003806233406067, Accuracy: 1.0\n",
            "  Batch 7568/8000, Loss: 0.10183023661375046, Accuracy: 1.0\n",
            "  Batch 7569/8000, Loss: 0.10950175672769547, Accuracy: 1.0\n",
            "  Batch 7570/8000, Loss: 0.0979098230600357, Accuracy: 1.0\n",
            "  Batch 7571/8000, Loss: 0.7351589202880859, Accuracy: 1.0\n",
            "  Batch 7572/8000, Loss: 0.3420187532901764, Accuracy: 1.0\n",
            "  Batch 7573/8000, Loss: 0.3437177538871765, Accuracy: 1.0\n",
            "  Batch 7574/8000, Loss: 0.3586341440677643, Accuracy: 1.0\n",
            "  Batch 7575/8000, Loss: 1.6122015714645386, Accuracy: 0.0\n",
            "  Batch 7576/8000, Loss: 1.4883842468261719, Accuracy: 0.0\n",
            "  Batch 7577/8000, Loss: 0.38236790895462036, Accuracy: 1.0\n",
            "  Batch 7578/8000, Loss: 1.731971263885498, Accuracy: 0.0\n",
            "  Batch 7579/8000, Loss: 0.6803151965141296, Accuracy: 1.0\n",
            "  Batch 7580/8000, Loss: 1.0578261613845825, Accuracy: 0.0\n",
            "  Batch 7581/8000, Loss: 0.10342760384082794, Accuracy: 1.0\n",
            "  Batch 7582/8000, Loss: 1.0270434617996216, Accuracy: 0.0\n",
            "  Batch 7583/8000, Loss: 0.10328182578086853, Accuracy: 1.0\n",
            "  Batch 7584/8000, Loss: 0.2545780837535858, Accuracy: 1.0\n",
            "  Batch 7585/8000, Loss: 0.5569548606872559, Accuracy: 1.0\n",
            "  Batch 7586/8000, Loss: 0.10560977458953857, Accuracy: 1.0\n",
            "  Batch 7587/8000, Loss: 0.3792487382888794, Accuracy: 1.0\n",
            "  Batch 7588/8000, Loss: 0.17072270810604095, Accuracy: 1.0\n",
            "  Batch 7589/8000, Loss: 0.3744654655456543, Accuracy: 1.0\n",
            "  Batch 7590/8000, Loss: 1.5350650548934937, Accuracy: 0.0\n",
            "  Batch 7591/8000, Loss: 0.3803609013557434, Accuracy: 1.0\n",
            "  Batch 7592/8000, Loss: 1.0886791944503784, Accuracy: 0.0\n",
            "  Batch 7593/8000, Loss: 0.4705043137073517, Accuracy: 1.0\n",
            "  Batch 7594/8000, Loss: 0.12962700426578522, Accuracy: 1.0\n",
            "  Batch 7595/8000, Loss: 0.4211430847644806, Accuracy: 1.0\n",
            "  Batch 7596/8000, Loss: 0.42356076836586, Accuracy: 1.0\n",
            "  Batch 7597/8000, Loss: 0.5954399108886719, Accuracy: 1.0\n",
            "  Batch 7598/8000, Loss: 0.4820159077644348, Accuracy: 1.0\n",
            "  Batch 7599/8000, Loss: 0.37969088554382324, Accuracy: 1.0\n",
            "  Batch 7600/8000, Loss: 1.0909810066223145, Accuracy: 0.0\n",
            "  Batch 7601/8000, Loss: 0.8693574070930481, Accuracy: 0.0\n",
            "  Batch 7602/8000, Loss: 0.4295443594455719, Accuracy: 1.0\n",
            "  Batch 7603/8000, Loss: 0.3511243462562561, Accuracy: 1.0\n",
            "  Batch 7604/8000, Loss: 0.3517146110534668, Accuracy: 1.0\n",
            "  Batch 7605/8000, Loss: 0.2352096289396286, Accuracy: 1.0\n",
            "  Batch 7606/8000, Loss: 0.5541152954101562, Accuracy: 1.0\n",
            "  Batch 7607/8000, Loss: 0.10575810074806213, Accuracy: 1.0\n",
            "  Batch 7608/8000, Loss: 0.35486915707588196, Accuracy: 1.0\n",
            "  Batch 7609/8000, Loss: 0.36182901263237, Accuracy: 1.0\n",
            "  Batch 7610/8000, Loss: 1.5992248058319092, Accuracy: 0.0\n",
            "  Batch 7611/8000, Loss: 1.5683306455612183, Accuracy: 0.0\n",
            "  Batch 7612/8000, Loss: 0.41951167583465576, Accuracy: 1.0\n",
            "  Batch 7613/8000, Loss: 1.6484363079071045, Accuracy: 0.0\n",
            "  Batch 7614/8000, Loss: 0.11280352622270584, Accuracy: 1.0\n",
            "  Batch 7615/8000, Loss: 0.3595485985279083, Accuracy: 1.0\n",
            "  Batch 7616/8000, Loss: 0.3366462290287018, Accuracy: 1.0\n",
            "  Batch 7617/8000, Loss: 0.1026887446641922, Accuracy: 1.0\n",
            "  Batch 7618/8000, Loss: 0.09736203402280807, Accuracy: 1.0\n",
            "  Batch 7619/8000, Loss: 0.48109543323516846, Accuracy: 1.0\n",
            "  Batch 7620/8000, Loss: 1.5313493013381958, Accuracy: 0.0\n",
            "  Batch 7621/8000, Loss: 0.3860560655593872, Accuracy: 1.0\n",
            "  Batch 7622/8000, Loss: 0.1056230366230011, Accuracy: 1.0\n",
            "  Batch 7623/8000, Loss: 0.5530918836593628, Accuracy: 1.0\n",
            "  Batch 7624/8000, Loss: 0.5062391757965088, Accuracy: 1.0\n",
            "  Batch 7625/8000, Loss: 1.3233661651611328, Accuracy: 0.0\n",
            "  Batch 7626/8000, Loss: 1.306426763534546, Accuracy: 0.0\n",
            "  Batch 7627/8000, Loss: 0.38207948207855225, Accuracy: 1.0\n",
            "  Batch 7628/8000, Loss: 0.3651072680950165, Accuracy: 1.0\n",
            "  Batch 7629/8000, Loss: 0.8896790742874146, Accuracy: 0.0\n",
            "  Batch 7630/8000, Loss: 0.7075814604759216, Accuracy: 1.0\n",
            "  Batch 7631/8000, Loss: 0.10086803883314133, Accuracy: 1.0\n",
            "  Batch 7632/8000, Loss: 0.10420189797878265, Accuracy: 1.0\n",
            "  Batch 7633/8000, Loss: 0.45536890625953674, Accuracy: 1.0\n",
            "  Batch 7634/8000, Loss: 0.5010679364204407, Accuracy: 1.0\n",
            "  Batch 7635/8000, Loss: 0.3827313780784607, Accuracy: 1.0\n",
            "  Batch 7636/8000, Loss: 0.960971474647522, Accuracy: 0.0\n",
            "  Batch 7637/8000, Loss: 0.09727281332015991, Accuracy: 1.0\n",
            "  Batch 7638/8000, Loss: 0.3698296546936035, Accuracy: 1.0\n",
            "  Batch 7639/8000, Loss: 0.4114014208316803, Accuracy: 1.0\n",
            "  Batch 7640/8000, Loss: 0.4233900010585785, Accuracy: 1.0\n",
            "  Batch 7641/8000, Loss: 0.4045441150665283, Accuracy: 1.0\n",
            "  Batch 7642/8000, Loss: 0.1262579709291458, Accuracy: 1.0\n",
            "  Batch 7643/8000, Loss: 0.35863739252090454, Accuracy: 1.0\n",
            "  Batch 7644/8000, Loss: 1.5459527969360352, Accuracy: 0.0\n",
            "  Batch 7645/8000, Loss: 0.4552684724330902, Accuracy: 1.0\n",
            "  Batch 7646/8000, Loss: 0.6327586770057678, Accuracy: 1.0\n",
            "  Batch 7647/8000, Loss: 0.9993453025817871, Accuracy: 0.0\n",
            "  Batch 7648/8000, Loss: 0.445171982049942, Accuracy: 1.0\n",
            "  Batch 7649/8000, Loss: 0.9781145453453064, Accuracy: 0.0\n",
            "  Batch 7650/8000, Loss: 0.37695395946502686, Accuracy: 1.0\n",
            "  Batch 7651/8000, Loss: 0.455674946308136, Accuracy: 1.0\n",
            "  Batch 7652/8000, Loss: 0.15648037195205688, Accuracy: 1.0\n",
            "  Batch 7653/8000, Loss: 0.4472024440765381, Accuracy: 1.0\n",
            "  Batch 7654/8000, Loss: 0.3484480082988739, Accuracy: 1.0\n",
            "  Batch 7655/8000, Loss: 0.8792385458946228, Accuracy: 0.0\n",
            "  Batch 7656/8000, Loss: 0.11019603908061981, Accuracy: 1.0\n",
            "  Batch 7657/8000, Loss: 0.4043949842453003, Accuracy: 1.0\n",
            "  Batch 7658/8000, Loss: 0.3752816915512085, Accuracy: 1.0\n",
            "  Batch 7659/8000, Loss: 1.4902557134628296, Accuracy: 0.0\n",
            "  Batch 7660/8000, Loss: 0.09979784488677979, Accuracy: 1.0\n",
            "  Batch 7661/8000, Loss: 0.6191220283508301, Accuracy: 1.0\n",
            "  Batch 7662/8000, Loss: 0.10463069379329681, Accuracy: 1.0\n",
            "  Batch 7663/8000, Loss: 1.064316749572754, Accuracy: 0.0\n",
            "  Batch 7664/8000, Loss: 0.3375093340873718, Accuracy: 1.0\n",
            "  Batch 7665/8000, Loss: 0.09965397417545319, Accuracy: 1.0\n",
            "  Batch 7666/8000, Loss: 0.5779259204864502, Accuracy: 1.0\n",
            "  Batch 7667/8000, Loss: 0.4631515145301819, Accuracy: 1.0\n",
            "  Batch 7668/8000, Loss: 1.0093728303909302, Accuracy: 0.0\n",
            "  Batch 7669/8000, Loss: 1.0173821449279785, Accuracy: 0.0\n",
            "  Batch 7670/8000, Loss: 0.3752122223377228, Accuracy: 1.0\n",
            "  Batch 7671/8000, Loss: 1.2172316312789917, Accuracy: 0.0\n",
            "  Batch 7672/8000, Loss: 0.7392114996910095, Accuracy: 1.0\n",
            "  Batch 7673/8000, Loss: 0.3635510802268982, Accuracy: 1.0\n",
            "  Batch 7674/8000, Loss: 0.2853236198425293, Accuracy: 1.0\n",
            "  Batch 7675/8000, Loss: 1.018367052078247, Accuracy: 0.0\n",
            "  Batch 7676/8000, Loss: 0.3729002773761749, Accuracy: 1.0\n",
            "  Batch 7677/8000, Loss: 0.10883847624063492, Accuracy: 1.0\n",
            "  Batch 7678/8000, Loss: 0.09710904955863953, Accuracy: 1.0\n",
            "  Batch 7679/8000, Loss: 0.0976678729057312, Accuracy: 1.0\n",
            "  Batch 7680/8000, Loss: 0.4763314723968506, Accuracy: 1.0\n",
            "  Batch 7681/8000, Loss: 0.09976385533809662, Accuracy: 1.0\n",
            "  Batch 7682/8000, Loss: 0.3593313992023468, Accuracy: 1.0\n",
            "  Batch 7683/8000, Loss: 0.10078857839107513, Accuracy: 1.0\n",
            "  Batch 7684/8000, Loss: 1.5580462217330933, Accuracy: 0.0\n",
            "  Batch 7685/8000, Loss: 0.09708739817142487, Accuracy: 1.0\n",
            "  Batch 7686/8000, Loss: 0.429336816072464, Accuracy: 1.0\n",
            "  Batch 7687/8000, Loss: 0.673488199710846, Accuracy: 1.0\n",
            "  Batch 7688/8000, Loss: 0.6581782102584839, Accuracy: 1.0\n",
            "  Batch 7689/8000, Loss: 0.09897202998399734, Accuracy: 1.0\n",
            "  Batch 7690/8000, Loss: 0.7092870473861694, Accuracy: 1.0\n",
            "  Batch 7691/8000, Loss: 0.3414973020553589, Accuracy: 1.0\n",
            "  Batch 7692/8000, Loss: 0.38594502210617065, Accuracy: 1.0\n",
            "  Batch 7693/8000, Loss: 1.0866706371307373, Accuracy: 0.0\n",
            "  Batch 7694/8000, Loss: 0.36013513803482056, Accuracy: 1.0\n",
            "  Batch 7695/8000, Loss: 0.412193238735199, Accuracy: 1.0\n",
            "  Batch 7696/8000, Loss: 0.3976466655731201, Accuracy: 1.0\n",
            "  Batch 7697/8000, Loss: 0.6677016019821167, Accuracy: 1.0\n",
            "  Batch 7698/8000, Loss: 0.09702500700950623, Accuracy: 1.0\n",
            "  Batch 7699/8000, Loss: 1.3467568159103394, Accuracy: 0.0\n",
            "  Batch 7700/8000, Loss: 0.09701468050479889, Accuracy: 1.0\n",
            "  Batch 7701/8000, Loss: 0.6409111022949219, Accuracy: 1.0\n",
            "  Batch 7702/8000, Loss: 0.8724125623703003, Accuracy: 0.0\n",
            "  Batch 7703/8000, Loss: 0.2524198591709137, Accuracy: 1.0\n",
            "  Batch 7704/8000, Loss: 1.5555850267410278, Accuracy: 0.0\n",
            "  Batch 7705/8000, Loss: 0.5995316505432129, Accuracy: 1.0\n",
            "  Batch 7706/8000, Loss: 0.10141284763813019, Accuracy: 1.0\n",
            "  Batch 7707/8000, Loss: 0.09909142553806305, Accuracy: 1.0\n",
            "  Batch 7708/8000, Loss: 0.10154350847005844, Accuracy: 1.0\n",
            "  Batch 7709/8000, Loss: 0.45615407824516296, Accuracy: 1.0\n",
            "  Batch 7710/8000, Loss: 1.2234039306640625, Accuracy: 0.0\n",
            "  Batch 7711/8000, Loss: 0.37433162331581116, Accuracy: 1.0\n",
            "  Batch 7712/8000, Loss: 0.0986773669719696, Accuracy: 1.0\n",
            "  Batch 7713/8000, Loss: 1.549135446548462, Accuracy: 0.0\n",
            "  Batch 7714/8000, Loss: 0.4810407757759094, Accuracy: 1.0\n",
            "  Batch 7715/8000, Loss: 0.09830772131681442, Accuracy: 1.0\n",
            "  Batch 7716/8000, Loss: 1.2170923948287964, Accuracy: 0.0\n",
            "  Batch 7717/8000, Loss: 0.38745054602622986, Accuracy: 1.0\n",
            "  Batch 7718/8000, Loss: 0.4514732360839844, Accuracy: 1.0\n",
            "  Batch 7719/8000, Loss: 0.399565726518631, Accuracy: 1.0\n",
            "  Batch 7720/8000, Loss: 0.09942266345024109, Accuracy: 1.0\n",
            "  Batch 7721/8000, Loss: 0.9702739119529724, Accuracy: 0.0\n",
            "  Batch 7722/8000, Loss: 0.9134683012962341, Accuracy: 0.0\n",
            "  Batch 7723/8000, Loss: 0.09698463976383209, Accuracy: 1.0\n",
            "  Batch 7724/8000, Loss: 0.40116649866104126, Accuracy: 1.0\n",
            "  Batch 7725/8000, Loss: 0.09959016740322113, Accuracy: 1.0\n",
            "  Batch 7726/8000, Loss: 0.36276474595069885, Accuracy: 1.0\n",
            "  Batch 7727/8000, Loss: 1.4564769268035889, Accuracy: 0.0\n",
            "  Batch 7728/8000, Loss: 0.09939055144786835, Accuracy: 1.0\n",
            "  Batch 7729/8000, Loss: 0.30637091398239136, Accuracy: 1.0\n",
            "  Batch 7730/8000, Loss: 0.4057430028915405, Accuracy: 1.0\n",
            "  Batch 7731/8000, Loss: 1.430541753768921, Accuracy: 0.0\n",
            "  Batch 7732/8000, Loss: 1.4230632781982422, Accuracy: 0.0\n",
            "  Batch 7733/8000, Loss: 1.516032338142395, Accuracy: 0.0\n",
            "  Batch 7734/8000, Loss: 0.8012229204177856, Accuracy: 0.0\n",
            "  Batch 7735/8000, Loss: 1.4221601486206055, Accuracy: 0.0\n",
            "  Batch 7736/8000, Loss: 0.7646912932395935, Accuracy: 1.0\n",
            "  Batch 7737/8000, Loss: 0.8678003549575806, Accuracy: 0.0\n",
            "  Batch 7738/8000, Loss: 0.41779494285583496, Accuracy: 1.0\n",
            "  Batch 7739/8000, Loss: 0.4347575902938843, Accuracy: 1.0\n",
            "  Batch 7740/8000, Loss: 0.43819504976272583, Accuracy: 1.0\n",
            "  Batch 7741/8000, Loss: 0.44761180877685547, Accuracy: 1.0\n",
            "  Batch 7742/8000, Loss: 1.3194841146469116, Accuracy: 0.0\n",
            "  Batch 7743/8000, Loss: 0.9069797992706299, Accuracy: 0.0\n",
            "  Batch 7744/8000, Loss: 0.09921231120824814, Accuracy: 1.0\n",
            "  Batch 7745/8000, Loss: 0.7664140462875366, Accuracy: 1.0\n",
            "  Batch 7746/8000, Loss: 0.09681428968906403, Accuracy: 1.0\n",
            "  Batch 7747/8000, Loss: 0.6647548079490662, Accuracy: 1.0\n",
            "  Batch 7748/8000, Loss: 0.5728745460510254, Accuracy: 1.0\n",
            "  Batch 7749/8000, Loss: 0.6870477795600891, Accuracy: 1.0\n",
            "  Batch 7750/8000, Loss: 0.47842687368392944, Accuracy: 1.0\n",
            "  Batch 7751/8000, Loss: 0.43871667981147766, Accuracy: 1.0\n",
            "  Batch 7752/8000, Loss: 0.09858793020248413, Accuracy: 1.0\n",
            "  Batch 7753/8000, Loss: 0.5790554285049438, Accuracy: 1.0\n",
            "  Batch 7754/8000, Loss: 0.09863367676734924, Accuracy: 1.0\n",
            "  Batch 7755/8000, Loss: 0.6342630982398987, Accuracy: 1.0\n",
            "  Batch 7756/8000, Loss: 0.6664667129516602, Accuracy: 1.0\n",
            "  Batch 7757/8000, Loss: 1.13648521900177, Accuracy: 0.0\n",
            "  Batch 7758/8000, Loss: 0.4399617910385132, Accuracy: 1.0\n",
            "  Batch 7759/8000, Loss: 0.28847289085388184, Accuracy: 1.0\n",
            "  Batch 7760/8000, Loss: 1.1547374725341797, Accuracy: 0.0\n",
            "  Batch 7761/8000, Loss: 1.212349534034729, Accuracy: 0.0\n",
            "  Batch 7762/8000, Loss: 1.0714420080184937, Accuracy: 0.0\n",
            "  Batch 7763/8000, Loss: 0.098331019282341, Accuracy: 1.0\n",
            "  Batch 7764/8000, Loss: 0.09817728400230408, Accuracy: 1.0\n",
            "  Batch 7765/8000, Loss: 0.425139844417572, Accuracy: 1.0\n",
            "  Batch 7766/8000, Loss: 0.7581480145454407, Accuracy: 1.0\n",
            "  Batch 7767/8000, Loss: 0.5533077120780945, Accuracy: 1.0\n",
            "  Batch 7768/8000, Loss: 1.3225593566894531, Accuracy: 0.0\n",
            "  Batch 7769/8000, Loss: 0.43603774905204773, Accuracy: 1.0\n",
            "  Batch 7770/8000, Loss: 0.9862099289894104, Accuracy: 0.0\n",
            "  Batch 7771/8000, Loss: 0.10709944367408752, Accuracy: 1.0\n",
            "  Batch 7772/8000, Loss: 0.09816911816596985, Accuracy: 1.0\n",
            "  Batch 7773/8000, Loss: 0.10427713394165039, Accuracy: 1.0\n",
            "  Batch 7774/8000, Loss: 1.0752676725387573, Accuracy: 0.0\n",
            "  Batch 7775/8000, Loss: 0.5117834806442261, Accuracy: 1.0\n",
            "  Batch 7776/8000, Loss: 0.5711392760276794, Accuracy: 1.0\n",
            "  Batch 7777/8000, Loss: 1.1216334104537964, Accuracy: 0.0\n",
            "  Batch 7778/8000, Loss: 0.25481659173965454, Accuracy: 1.0\n",
            "  Batch 7779/8000, Loss: 0.623599648475647, Accuracy: 1.0\n",
            "  Batch 7780/8000, Loss: 0.45004361867904663, Accuracy: 1.0\n",
            "  Batch 7781/8000, Loss: 0.42022907733917236, Accuracy: 1.0\n",
            "  Batch 7782/8000, Loss: 0.4386883080005646, Accuracy: 1.0\n",
            "  Batch 7783/8000, Loss: 0.46416646242141724, Accuracy: 1.0\n",
            "  Batch 7784/8000, Loss: 0.5912505388259888, Accuracy: 1.0\n",
            "  Batch 7785/8000, Loss: 0.42069798707962036, Accuracy: 1.0\n",
            "  Batch 7786/8000, Loss: 0.4760746359825134, Accuracy: 1.0\n",
            "  Batch 7787/8000, Loss: 1.0648735761642456, Accuracy: 0.0\n",
            "  Batch 7788/8000, Loss: 0.16645212471485138, Accuracy: 1.0\n",
            "  Batch 7789/8000, Loss: 0.4475155770778656, Accuracy: 1.0\n",
            "  Batch 7790/8000, Loss: 0.5418981909751892, Accuracy: 1.0\n",
            "  Batch 7791/8000, Loss: 0.45911672711372375, Accuracy: 1.0\n",
            "  Batch 7792/8000, Loss: 0.42182913422584534, Accuracy: 1.0\n",
            "  Batch 7793/8000, Loss: 0.43854957818984985, Accuracy: 1.0\n",
            "  Batch 7794/8000, Loss: 1.1359652280807495, Accuracy: 0.0\n",
            "  Batch 7795/8000, Loss: 1.158941626548767, Accuracy: 0.0\n",
            "  Batch 7796/8000, Loss: 0.3907372057437897, Accuracy: 1.0\n",
            "  Batch 7797/8000, Loss: 0.41985267400741577, Accuracy: 1.0\n",
            "  Batch 7798/8000, Loss: 0.09783829003572464, Accuracy: 1.0\n",
            "  Batch 7799/8000, Loss: 0.52060866355896, Accuracy: 1.0\n",
            "  Batch 7800/8000, Loss: 0.5503892302513123, Accuracy: 1.0\n",
            "  Batch 7801/8000, Loss: 0.4296586811542511, Accuracy: 1.0\n",
            "  Batch 7802/8000, Loss: 0.3856343626976013, Accuracy: 1.0\n",
            "  Batch 7803/8000, Loss: 0.34975412487983704, Accuracy: 1.0\n",
            "  Batch 7804/8000, Loss: 0.6344347596168518, Accuracy: 1.0\n",
            "  Batch 7805/8000, Loss: 0.3830401599407196, Accuracy: 1.0\n",
            "  Batch 7806/8000, Loss: 0.6130151748657227, Accuracy: 1.0\n",
            "  Batch 7807/8000, Loss: 0.47412246465682983, Accuracy: 1.0\n",
            "  Batch 7808/8000, Loss: 0.7575201392173767, Accuracy: 1.0\n",
            "  Batch 7809/8000, Loss: 1.4433029890060425, Accuracy: 0.0\n",
            "  Batch 7810/8000, Loss: 0.4451517164707184, Accuracy: 1.0\n",
            "  Batch 7811/8000, Loss: 0.502858579158783, Accuracy: 1.0\n",
            "  Batch 7812/8000, Loss: 0.43610769510269165, Accuracy: 1.0\n",
            "  Batch 7813/8000, Loss: 1.5671920776367188, Accuracy: 0.0\n",
            "  Batch 7814/8000, Loss: 0.09731705486774445, Accuracy: 1.0\n",
            "  Batch 7815/8000, Loss: 0.09792807698249817, Accuracy: 1.0\n",
            "  Batch 7816/8000, Loss: 1.1944721937179565, Accuracy: 0.0\n",
            "  Batch 7817/8000, Loss: 0.529242217540741, Accuracy: 1.0\n",
            "  Batch 7818/8000, Loss: 0.3531520664691925, Accuracy: 1.0\n",
            "  Batch 7819/8000, Loss: 0.09661044180393219, Accuracy: 1.0\n",
            "  Batch 7820/8000, Loss: 0.475856214761734, Accuracy: 1.0\n",
            "  Batch 7821/8000, Loss: 0.20842143893241882, Accuracy: 1.0\n",
            "  Batch 7822/8000, Loss: 0.3798661530017853, Accuracy: 1.0\n",
            "  Batch 7823/8000, Loss: 1.589826226234436, Accuracy: 0.0\n",
            "  Batch 7824/8000, Loss: 0.09723816812038422, Accuracy: 1.0\n",
            "  Batch 7825/8000, Loss: 0.6202387809753418, Accuracy: 1.0\n",
            "  Batch 7826/8000, Loss: 0.5772231221199036, Accuracy: 1.0\n",
            "  Batch 7827/8000, Loss: 0.45666879415512085, Accuracy: 1.0\n",
            "  Batch 7828/8000, Loss: 0.0971602126955986, Accuracy: 1.0\n",
            "  Batch 7829/8000, Loss: 0.09653864800930023, Accuracy: 1.0\n",
            "  Batch 7830/8000, Loss: 0.4135839641094208, Accuracy: 1.0\n",
            "  Batch 7831/8000, Loss: 1.0280020236968994, Accuracy: 0.0\n",
            "  Batch 7832/8000, Loss: 0.3536781668663025, Accuracy: 1.0\n",
            "  Batch 7833/8000, Loss: 0.09654828906059265, Accuracy: 1.0\n",
            "  Batch 7834/8000, Loss: 0.09646452963352203, Accuracy: 1.0\n",
            "  Batch 7835/8000, Loss: 0.09652175009250641, Accuracy: 1.0\n",
            "  Batch 7836/8000, Loss: 0.315570205450058, Accuracy: 1.0\n",
            "  Batch 7837/8000, Loss: 0.09773034602403641, Accuracy: 1.0\n",
            "  Batch 7838/8000, Loss: 0.097219318151474, Accuracy: 1.0\n",
            "  Batch 7839/8000, Loss: 0.35366055369377136, Accuracy: 1.0\n",
            "  Batch 7840/8000, Loss: 0.4807813763618469, Accuracy: 1.0\n",
            "  Batch 7841/8000, Loss: 0.48489469289779663, Accuracy: 1.0\n",
            "  Batch 7842/8000, Loss: 0.09740827977657318, Accuracy: 1.0\n",
            "  Batch 7843/8000, Loss: 0.6006748080253601, Accuracy: 1.0\n",
            "  Batch 7844/8000, Loss: 0.6632718443870544, Accuracy: 1.0\n",
            "  Batch 7845/8000, Loss: 0.09748011827468872, Accuracy: 1.0\n",
            "  Batch 7846/8000, Loss: 0.10967637598514557, Accuracy: 1.0\n",
            "  Batch 7847/8000, Loss: 1.2866930961608887, Accuracy: 0.0\n",
            "  Batch 7848/8000, Loss: 0.32680201530456543, Accuracy: 1.0\n",
            "  Batch 7849/8000, Loss: 0.34636569023132324, Accuracy: 1.0\n",
            "  Batch 7850/8000, Loss: 0.5567820072174072, Accuracy: 1.0\n",
            "  Batch 7851/8000, Loss: 1.0915478467941284, Accuracy: 0.0\n",
            "  Batch 7852/8000, Loss: 0.09872119128704071, Accuracy: 1.0\n",
            "  Batch 7853/8000, Loss: 0.3853124976158142, Accuracy: 1.0\n",
            "  Batch 7854/8000, Loss: 0.09643064439296722, Accuracy: 1.0\n",
            "  Batch 7855/8000, Loss: 0.6949159502983093, Accuracy: 1.0\n",
            "  Batch 7856/8000, Loss: 0.09721272438764572, Accuracy: 1.0\n",
            "  Batch 7857/8000, Loss: 0.09638930857181549, Accuracy: 1.0\n",
            "  Batch 7858/8000, Loss: 0.09677892178297043, Accuracy: 1.0\n",
            "  Batch 7859/8000, Loss: 0.09724600613117218, Accuracy: 1.0\n",
            "  Batch 7860/8000, Loss: 0.10027910023927689, Accuracy: 1.0\n",
            "  Batch 7861/8000, Loss: 0.3175366520881653, Accuracy: 1.0\n",
            "  Batch 7862/8000, Loss: 0.0971013754606247, Accuracy: 1.0\n",
            "  Batch 7863/8000, Loss: 0.35404908657073975, Accuracy: 1.0\n",
            "  Batch 7864/8000, Loss: 0.3319540321826935, Accuracy: 1.0\n",
            "  Batch 7865/8000, Loss: 0.44319355487823486, Accuracy: 1.0\n",
            "  Batch 7866/8000, Loss: 0.55301833152771, Accuracy: 1.0\n",
            "  Batch 7867/8000, Loss: 0.5422760844230652, Accuracy: 1.0\n",
            "  Batch 7868/8000, Loss: 0.09711438417434692, Accuracy: 1.0\n",
            "  Batch 7869/8000, Loss: 1.666902780532837, Accuracy: 0.0\n",
            "  Batch 7870/8000, Loss: 0.43836095929145813, Accuracy: 1.0\n",
            "  Batch 7871/8000, Loss: 0.09834267199039459, Accuracy: 1.0\n",
            "  Batch 7872/8000, Loss: 0.9768912196159363, Accuracy: 0.0\n",
            "  Batch 7873/8000, Loss: 1.40904700756073, Accuracy: 0.0\n",
            "  Batch 7874/8000, Loss: 0.09756540507078171, Accuracy: 1.0\n",
            "  Batch 7875/8000, Loss: 0.5388173460960388, Accuracy: 1.0\n",
            "  Batch 7876/8000, Loss: 0.3351125121116638, Accuracy: 1.0\n",
            "  Batch 7877/8000, Loss: 0.28350386023521423, Accuracy: 1.0\n",
            "  Batch 7878/8000, Loss: 1.1551997661590576, Accuracy: 0.0\n",
            "  Batch 7879/8000, Loss: 0.565772294998169, Accuracy: 1.0\n",
            "  Batch 7880/8000, Loss: 0.4972001910209656, Accuracy: 1.0\n",
            "  Batch 7881/8000, Loss: 0.3136337697505951, Accuracy: 1.0\n",
            "  Batch 7882/8000, Loss: 0.5657839775085449, Accuracy: 1.0\n",
            "  Batch 7883/8000, Loss: 1.205349326133728, Accuracy: 0.0\n",
            "  Batch 7884/8000, Loss: 0.17683649063110352, Accuracy: 1.0\n",
            "  Batch 7885/8000, Loss: 0.3085560202598572, Accuracy: 1.0\n",
            "  Batch 7886/8000, Loss: 0.39105015993118286, Accuracy: 1.0\n",
            "  Batch 7887/8000, Loss: 0.09682586789131165, Accuracy: 1.0\n",
            "  Batch 7888/8000, Loss: 1.14696204662323, Accuracy: 0.0\n",
            "  Batch 7889/8000, Loss: 0.9710264801979065, Accuracy: 0.0\n",
            "  Batch 7890/8000, Loss: 1.0080821514129639, Accuracy: 0.0\n",
            "  Batch 7891/8000, Loss: 1.0702146291732788, Accuracy: 0.0\n",
            "  Batch 7892/8000, Loss: 1.649312138557434, Accuracy: 0.0\n",
            "  Batch 7893/8000, Loss: 0.2430509775876999, Accuracy: 1.0\n",
            "  Batch 7894/8000, Loss: 0.16877424716949463, Accuracy: 1.0\n",
            "  Batch 7985/8000, Loss: 0.32389724254608154, Accuracy: 1.0\n",
            "  Batch 7986/8000, Loss: 1.8538990020751953, Accuracy: 0.0\n",
            "  Batch 7987/8000, Loss: 0.09706903994083405, Accuracy: 1.0\n",
            "  Batch 7988/8000, Loss: 0.31449711322784424, Accuracy: 1.0\n",
            "  Batch 7989/8000, Loss: 0.9543221592903137, Accuracy: 0.0\n",
            "  Batch 7990/8000, Loss: 0.42159515619277954, Accuracy: 1.0\n",
            "  Batch 7991/8000, Loss: 0.10817467421293259, Accuracy: 1.0\n",
            "  Batch 7992/8000, Loss: 0.2836816906929016, Accuracy: 1.0\n",
            "  Batch 7993/8000, Loss: 0.7203117609024048, Accuracy: 1.0\n",
            "  Batch 7994/8000, Loss: 0.31848379969596863, Accuracy: 1.0\n",
            "  Batch 7995/8000, Loss: 0.1030479371547699, Accuracy: 1.0\n",
            "  Batch 7996/8000, Loss: 0.6114739775657654, Accuracy: 1.0\n",
            "  Batch 7997/8000, Loss: 0.3183729648590088, Accuracy: 1.0\n",
            "  Batch 7998/8000, Loss: 0.2890230715274811, Accuracy: 1.0\n",
            "  Batch 7999/8000, Loss: 0.09583216905593872, Accuracy: 1.0\n",
            "  Batch 8000/8000, Loss: 0.3002048134803772, Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "ANN model with five hidden layers, increasing number of neurons, relu activation for hidden layers,\n",
        "softmax activation for output layer, Binary Crossentropy as loss function, Adam function as optimizer,\n",
        "accuracy as metric,learning rate of 0.0001, batch normalization trained on 15 epochs sample-wise.\n",
        "'''\n",
        "\n",
        "model_11 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, kernel_regularizer=regularizers.l2(0.001), activation = 'relu'),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_11.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "  for i in range(len(X_train)):\n",
        "    X_batch = X_train[i:i+batch_size]\n",
        "    y_batch = y_train[i:i+batch_size]\n",
        "    loss, acc = model_11.train_on_batch(X_batch, y_batch)\n",
        "    print(f\"  Batch {i+1}/{len(X_train)}, Loss: {loss}, Accuracy: {acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIpDoKZkFnZr",
        "outputId": "dc1f90a7-0e39-4c11-ff69-78fca02b27cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 1s 6ms/step - loss: 0.5249 - accuracy: 0.7890\n",
            "Test Accuracy: 0.7889999747276306\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model_11.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL4_CROOHfTD",
        "outputId": "7912e129-097a-40be-e437-1297715d4c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m112.6/129.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ui6l-J93fd7Y",
        "outputId": "313ad614-6845-41c3-88cd-b3296b46b552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 00s]\n",
            "\n",
            "Best val_accuracy So Far: 0.49050000309944153\n",
            "Total elapsed time: 00h 04m 57s\n",
            "\n",
            "Search: Running Trial #4\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "288               |384               |units\n",
            "3                 |1                 |num_layers\n",
            "sigmoid           |tanh              |activation\n",
            "0.01              |0.01              |learning_rate\n",
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
            "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
            "    return model.fit(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/tmp/__autograph_generated_fileeb3qjy28.py\", line 15, in tf__train_function\n",
            "    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n",
            "        return step_function(self, iterator)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n",
            "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n",
            "        outputs = model.train_step(data)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n",
            "        loss = self.compute_loss(x, y, y_pred, sample_weight)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n",
            "        return self.compiled_loss(\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n",
            "        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n",
            "        losses = call_fn(y_true, y_pred)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n",
            "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n",
            "        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n",
            "    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n",
            "        return tf.nn.sigmoid_cross_entropy_with_logits(\n",
            "\n",
            "    ValueError: `logits` and `labels` must have the same shape, received ((32, 2) vs (32, 1)).\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/tmp/__autograph_generated_fileeb3qjy28.py\", line 15, in tf__train_function\n    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\nValueError: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((32, 2) vs (32, 1)).\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-60f533e98f32>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mbest_hps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36mon_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTrial\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mLOCKS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthread_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mneed_acquire\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mTHREADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36mend_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_consecutive_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/oracle.py\u001b[0m in \u001b[0;36m_check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconsecutive_failures\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_consecutive_failed_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                 raise RuntimeError(\n\u001b[0m\u001b[1;32m    546\u001b[0m                     \u001b[0;34m\"Number of consecutive failures exceeded the limit \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                     \u001b[0;34mf\"of {self.max_consecutive_failed_trials}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/tmp/__autograph_generated_fileeb3qjy28.py\", line 15, in tf__train_function\n    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\nValueError: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((32, 2) vs (32, 1)).\n\n"
          ]
        }
      ],
      "source": [
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "def build_model(hp):\n",
        "    input_layer = Input(shape=(X.shape[1],))\n",
        "\n",
        "    # Hyperparameters to tune\n",
        "    num_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "    num_layers = hp.Int('num_layers', 1, 3)\n",
        "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
        "    learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])\n",
        "\n",
        "    x = input_layer\n",
        "    for _ in range(num_layers):\n",
        "        x = Dense(num_units, activation=activation)(x)\n",
        "\n",
        "    output_layer = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=20,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='ann_hyperparameter_tuning'\n",
        ")\n",
        "\n",
        "tuner.search(X, y, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1lyhWfsftCp"
      },
      "outputs": [],
      "source": [
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(X, y, epochs=50, batch_size=32, validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UoHnTo7Gvs2t"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mJiqRuu-u7hl"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/05/25 13:05:32 WARNING mlflow.tensorflow: You are saving a TensorFlow Core model or Keras model without a signature. Inference with mlflow.pyfunc.spark_udf() will not work unless the model's pyfunc representation accepts pandas DataFrames as inference inputs.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\anany\\AppData\\Local\\Temp\\tmpp224bkmo\\model\\data\\model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\anany\\AppData\\Local\\Temp\\tmpp224bkmo\\model\\data\\model\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 6ms/step\n",
            "Accuracy: 0.7385\n",
            "Confusion Matrix:\n",
            "[[1003    2]\n",
            " [ 521  474]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.66      1.00      0.79      1005\n",
            "         1.0       1.00      0.48      0.64       995\n",
            "\n",
            "    accuracy                           0.74      2000\n",
            "   macro avg       0.83      0.74      0.72      2000\n",
            "weighted avg       0.83      0.74      0.72      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "model_12 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1024, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])\n",
        "\n",
        "model_12.compile(loss = 'binary_crossentropy',\n",
        "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "# history_12 = model_12.fit(X_train,\n",
        "#                     y_train,\n",
        "#                     epochs = 1)\n",
        "\n",
        "\n",
        "with mlflow.start_run():\n",
        "  history_12 = model_12.fit(X_train,\n",
        "                            y_train,\n",
        "                            epochs = 50,\n",
        "                            batch_size = 32,\n",
        "                            verbose = 0)\n",
        "  mlflow.tensorflow.log_model(model_12, 'model_12')\n",
        "  y_pred_prob = model_12.predict(X_test).flatten()\n",
        "  y_pred = np.where(y_pred_prob >= 0.5,1,-1)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  cr = classification_report(y_test, y_pred)\n",
        "\n",
        "  mlflow.log_metric(\"accuracy\", accuracy)\n",
        "  mlflow.log_text(str(cm), \"confusion_matrix.txt\")\n",
        "  mlflow.log_text(cr, \"classification_report.txt\")\n",
        "\n",
        "  print(f'Accuracy: {accuracy}')\n",
        "  print(f'Confusion Matrix:\\n{cm}')\n",
        "  print(f'Classification Report:\\n{cr}')\n",
        "\n",
        "# logged_model = '/content/mlruns/0/model'  # Replace <your_run_id> with the actual run ID\n",
        "# loaded_model = mlflow.tensorflow.load_model(logged_model)\n",
        "\n",
        "# # Make predictions\n",
        "# new_data = scaler.transform([[value1, value2, ...]])  # Replace with new data\n",
        "# predictions = (loaded_model.predict(new_data).flatten() > 0.5).astype(int)\n",
        "# print(f'Predictions: {predictions}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2000,)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_prob.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 1, ..., 0, 0, 0])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
              "       0])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred[1:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4684    1.0\n",
              "1731    1.0\n",
              "4742    1.0\n",
              "4521   -1.0\n",
              "6340   -1.0\n",
              "       ... \n",
              "4516    1.0\n",
              "1261    1.0\n",
              "3160   -1.0\n",
              "970    -1.0\n",
              "3912    1.0\n",
              "Name: Label, Length: 199, dtype: float64"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test[1:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
